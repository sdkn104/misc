from datetime import date, datetime  # noqa: F401

from typing import List, Dict  # noqa: F401

from openapi_server.models.base_model import Model
from openapi_server.models.chat_completion_functions import ChatCompletionFunctions
from openapi_server.models.chat_completion_request_message import ChatCompletionRequestMessage
from openapi_server.models.chat_completion_stream_options import ChatCompletionStreamOptions
from openapi_server.models.chat_completion_tool import ChatCompletionTool
from openapi_server.models.chat_completion_tool_choice_option import ChatCompletionToolChoiceOption
from openapi_server.models.create_chat_completion_request_all_of_audio import CreateChatCompletionRequestAllOfAudio
from openapi_server.models.create_chat_completion_request_all_of_function_call import CreateChatCompletionRequestAllOfFunctionCall
from openapi_server.models.create_chat_completion_request_all_of_response_format import CreateChatCompletionRequestAllOfResponseFormat
from openapi_server.models.model_ids_shared import ModelIdsShared
from openapi_server.models.prediction_content import PredictionContent
from openapi_server.models.reasoning_effort import ReasoningEffort
from openapi_server.models.service_tier import ServiceTier
from openapi_server.models.stop_configuration import StopConfiguration
from openapi_server.models.web_search import WebSearch
from openapi_server import util

from openapi_server.models.chat_completion_functions import ChatCompletionFunctions  # noqa: E501
from openapi_server.models.chat_completion_request_message import ChatCompletionRequestMessage  # noqa: E501
from openapi_server.models.chat_completion_stream_options import ChatCompletionStreamOptions  # noqa: E501
from openapi_server.models.chat_completion_tool import ChatCompletionTool  # noqa: E501
from openapi_server.models.chat_completion_tool_choice_option import ChatCompletionToolChoiceOption  # noqa: E501
from openapi_server.models.create_chat_completion_request_all_of_audio import CreateChatCompletionRequestAllOfAudio  # noqa: E501
from openapi_server.models.create_chat_completion_request_all_of_function_call import CreateChatCompletionRequestAllOfFunctionCall  # noqa: E501
from openapi_server.models.create_chat_completion_request_all_of_response_format import CreateChatCompletionRequestAllOfResponseFormat  # noqa: E501
from openapi_server.models.model_ids_shared import ModelIdsShared  # noqa: E501
from openapi_server.models.prediction_content import PredictionContent  # noqa: E501
from openapi_server.models.reasoning_effort import ReasoningEffort  # noqa: E501
from openapi_server.models.service_tier import ServiceTier  # noqa: E501
from openapi_server.models.stop_configuration import StopConfiguration  # noqa: E501
from openapi_server.models.web_search import WebSearch  # noqa: E501

class CreateChatCompletionRequest(Model):
    """NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).

    Do not edit the class manually.
    """

    def __init__(self, metadata=None, temperature=1, top_p=1, user=None, service_tier=ServiceTier.AUTO, messages=None, model=None, modalities=None, reasoning_effort=ReasoningEffort.MEDIUM, max_completion_tokens=None, frequency_penalty=0, presence_penalty=0, web_search_options=None, top_logprobs=None, response_format=None, audio=None, store=False, stream=False, stop=None, logit_bias=None, logprobs=False, max_tokens=None, n=1, prediction=None, seed=None, stream_options=None, tools=None, tool_choice=None, parallel_tool_calls=True, function_call=None, functions=None):  # noqa: E501
        """CreateChatCompletionRequest - a model defined in OpenAPI

        :param metadata: The metadata of this CreateChatCompletionRequest.  # noqa: E501
        :type metadata: Dict[str, str]
        :param temperature: The temperature of this CreateChatCompletionRequest.  # noqa: E501
        :type temperature: float
        :param top_p: The top_p of this CreateChatCompletionRequest.  # noqa: E501
        :type top_p: float
        :param user: The user of this CreateChatCompletionRequest.  # noqa: E501
        :type user: str
        :param service_tier: The service_tier of this CreateChatCompletionRequest.  # noqa: E501
        :type service_tier: ServiceTier
        :param messages: The messages of this CreateChatCompletionRequest.  # noqa: E501
        :type messages: List[ChatCompletionRequestMessage]
        :param model: The model of this CreateChatCompletionRequest.  # noqa: E501
        :type model: ModelIdsShared
        :param modalities: The modalities of this CreateChatCompletionRequest.  # noqa: E501
        :type modalities: List[str]
        :param reasoning_effort: The reasoning_effort of this CreateChatCompletionRequest.  # noqa: E501
        :type reasoning_effort: ReasoningEffort
        :param max_completion_tokens: The max_completion_tokens of this CreateChatCompletionRequest.  # noqa: E501
        :type max_completion_tokens: int
        :param frequency_penalty: The frequency_penalty of this CreateChatCompletionRequest.  # noqa: E501
        :type frequency_penalty: float
        :param presence_penalty: The presence_penalty of this CreateChatCompletionRequest.  # noqa: E501
        :type presence_penalty: float
        :param web_search_options: The web_search_options of this CreateChatCompletionRequest.  # noqa: E501
        :type web_search_options: WebSearch
        :param top_logprobs: The top_logprobs of this CreateChatCompletionRequest.  # noqa: E501
        :type top_logprobs: int
        :param response_format: The response_format of this CreateChatCompletionRequest.  # noqa: E501
        :type response_format: CreateChatCompletionRequestAllOfResponseFormat
        :param audio: The audio of this CreateChatCompletionRequest.  # noqa: E501
        :type audio: CreateChatCompletionRequestAllOfAudio
        :param store: The store of this CreateChatCompletionRequest.  # noqa: E501
        :type store: bool
        :param stream: The stream of this CreateChatCompletionRequest.  # noqa: E501
        :type stream: bool
        :param stop: The stop of this CreateChatCompletionRequest.  # noqa: E501
        :type stop: StopConfiguration
        :param logit_bias: The logit_bias of this CreateChatCompletionRequest.  # noqa: E501
        :type logit_bias: Dict[str, int]
        :param logprobs: The logprobs of this CreateChatCompletionRequest.  # noqa: E501
        :type logprobs: bool
        :param max_tokens: The max_tokens of this CreateChatCompletionRequest.  # noqa: E501
        :type max_tokens: int
        :param n: The n of this CreateChatCompletionRequest.  # noqa: E501
        :type n: int
        :param prediction: The prediction of this CreateChatCompletionRequest.  # noqa: E501
        :type prediction: PredictionContent
        :param seed: The seed of this CreateChatCompletionRequest.  # noqa: E501
        :type seed: int
        :param stream_options: The stream_options of this CreateChatCompletionRequest.  # noqa: E501
        :type stream_options: ChatCompletionStreamOptions
        :param tools: The tools of this CreateChatCompletionRequest.  # noqa: E501
        :type tools: List[ChatCompletionTool]
        :param tool_choice: The tool_choice of this CreateChatCompletionRequest.  # noqa: E501
        :type tool_choice: ChatCompletionToolChoiceOption
        :param parallel_tool_calls: The parallel_tool_calls of this CreateChatCompletionRequest.  # noqa: E501
        :type parallel_tool_calls: bool
        :param function_call: The function_call of this CreateChatCompletionRequest.  # noqa: E501
        :type function_call: CreateChatCompletionRequestAllOfFunctionCall
        :param functions: The functions of this CreateChatCompletionRequest.  # noqa: E501
        :type functions: List[ChatCompletionFunctions]
        """
        self.openapi_types = {
            'metadata': Dict[str, str],
            'temperature': float,
            'top_p': float,
            'user': str,
            'service_tier': ServiceTier,
            'messages': List[ChatCompletionRequestMessage],
            'model': ModelIdsShared,
            'modalities': List[str],
            'reasoning_effort': ReasoningEffort,
            'max_completion_tokens': int,
            'frequency_penalty': float,
            'presence_penalty': float,
            'web_search_options': WebSearch,
            'top_logprobs': int,
            'response_format': CreateChatCompletionRequestAllOfResponseFormat,
            'audio': CreateChatCompletionRequestAllOfAudio,
            'store': bool,
            'stream': bool,
            'stop': StopConfiguration,
            'logit_bias': Dict[str, int],
            'logprobs': bool,
            'max_tokens': int,
            'n': int,
            'prediction': PredictionContent,
            'seed': int,
            'stream_options': ChatCompletionStreamOptions,
            'tools': List[ChatCompletionTool],
            'tool_choice': ChatCompletionToolChoiceOption,
            'parallel_tool_calls': bool,
            'function_call': CreateChatCompletionRequestAllOfFunctionCall,
            'functions': List[ChatCompletionFunctions]
        }

        self.attribute_map = {
            'metadata': 'metadata',
            'temperature': 'temperature',
            'top_p': 'top_p',
            'user': 'user',
            'service_tier': 'service_tier',
            'messages': 'messages',
            'model': 'model',
            'modalities': 'modalities',
            'reasoning_effort': 'reasoning_effort',
            'max_completion_tokens': 'max_completion_tokens',
            'frequency_penalty': 'frequency_penalty',
            'presence_penalty': 'presence_penalty',
            'web_search_options': 'web_search_options',
            'top_logprobs': 'top_logprobs',
            'response_format': 'response_format',
            'audio': 'audio',
            'store': 'store',
            'stream': 'stream',
            'stop': 'stop',
            'logit_bias': 'logit_bias',
            'logprobs': 'logprobs',
            'max_tokens': 'max_tokens',
            'n': 'n',
            'prediction': 'prediction',
            'seed': 'seed',
            'stream_options': 'stream_options',
            'tools': 'tools',
            'tool_choice': 'tool_choice',
            'parallel_tool_calls': 'parallel_tool_calls',
            'function_call': 'function_call',
            'functions': 'functions'
        }

        self._metadata = metadata
        self._temperature = temperature
        self._top_p = top_p
        self._user = user
        self._service_tier = service_tier
        self._messages = messages
        self._model = model
        self._modalities = modalities
        self._reasoning_effort = reasoning_effort
        self._max_completion_tokens = max_completion_tokens
        self._frequency_penalty = frequency_penalty
        self._presence_penalty = presence_penalty
        self._web_search_options = web_search_options
        self._top_logprobs = top_logprobs
        self._response_format = response_format
        self._audio = audio
        self._store = store
        self._stream = stream
        self._stop = stop
        self._logit_bias = logit_bias
        self._logprobs = logprobs
        self._max_tokens = max_tokens
        self._n = n
        self._prediction = prediction
        self._seed = seed
        self._stream_options = stream_options
        self._tools = tools
        self._tool_choice = tool_choice
        self._parallel_tool_calls = parallel_tool_calls
        self._function_call = function_call
        self._functions = functions

    @classmethod
    def from_dict(cls, dikt) -> 'CreateChatCompletionRequest':
        """Returns the dict as a model

        :param dikt: A dict.
        :type: dict
        :return: The CreateChatCompletionRequest of this CreateChatCompletionRequest.  # noqa: E501
        :rtype: CreateChatCompletionRequest
        """
        return util.deserialize_model(dikt, cls)

    @property
    def metadata(self) -> Dict[str, str]:
        """Gets the metadata of this CreateChatCompletionRequest.

        Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.   Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters.   # noqa: E501

        :return: The metadata of this CreateChatCompletionRequest.
        :rtype: Dict[str, str]
        """
        return self._metadata

    @metadata.setter
    def metadata(self, metadata: Dict[str, str]):
        """Sets the metadata of this CreateChatCompletionRequest.

        Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.   Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters.   # noqa: E501

        :param metadata: The metadata of this CreateChatCompletionRequest.
        :type metadata: Dict[str, str]
        """

        self._metadata = metadata

    @property
    def temperature(self) -> float:
        """Gets the temperature of this CreateChatCompletionRequest.

        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.   # noqa: E501

        :return: The temperature of this CreateChatCompletionRequest.
        :rtype: float
        """
        return self._temperature

    @temperature.setter
    def temperature(self, temperature: float):
        """Sets the temperature of this CreateChatCompletionRequest.

        What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.   # noqa: E501

        :param temperature: The temperature of this CreateChatCompletionRequest.
        :type temperature: float
        """
        if temperature is not None and temperature > 2:  # noqa: E501
            raise ValueError("Invalid value for `temperature`, must be a value less than or equal to `2`")  # noqa: E501
        if temperature is not None and temperature < 0:  # noqa: E501
            raise ValueError("Invalid value for `temperature`, must be a value greater than or equal to `0`")  # noqa: E501

        self._temperature = temperature

    @property
    def top_p(self) -> float:
        """Gets the top_p of this CreateChatCompletionRequest.

        An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both.   # noqa: E501

        :return: The top_p of this CreateChatCompletionRequest.
        :rtype: float
        """
        return self._top_p

    @top_p.setter
    def top_p(self, top_p: float):
        """Sets the top_p of this CreateChatCompletionRequest.

        An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.  We generally recommend altering this or `temperature` but not both.   # noqa: E501

        :param top_p: The top_p of this CreateChatCompletionRequest.
        :type top_p: float
        """
        if top_p is not None and top_p > 1:  # noqa: E501
            raise ValueError("Invalid value for `top_p`, must be a value less than or equal to `1`")  # noqa: E501
        if top_p is not None and top_p < 0:  # noqa: E501
            raise ValueError("Invalid value for `top_p`, must be a value greater than or equal to `0`")  # noqa: E501

        self._top_p = top_p

    @property
    def user(self) -> str:
        """Gets the user of this CreateChatCompletionRequest.

        A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).   # noqa: E501

        :return: The user of this CreateChatCompletionRequest.
        :rtype: str
        """
        return self._user

    @user.setter
    def user(self, user: str):
        """Sets the user of this CreateChatCompletionRequest.

        A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).   # noqa: E501

        :param user: The user of this CreateChatCompletionRequest.
        :type user: str
        """

        self._user = user

    @property
    def service_tier(self) -> ServiceTier:
        """Gets the service_tier of this CreateChatCompletionRequest.


        :return: The service_tier of this CreateChatCompletionRequest.
        :rtype: ServiceTier
        """
        return self._service_tier

    @service_tier.setter
    def service_tier(self, service_tier: ServiceTier):
        """Sets the service_tier of this CreateChatCompletionRequest.


        :param service_tier: The service_tier of this CreateChatCompletionRequest.
        :type service_tier: ServiceTier
        """

        self._service_tier = service_tier

    @property
    def messages(self) -> List[ChatCompletionRequestMessage]:
        """Gets the messages of this CreateChatCompletionRequest.

        A list of messages comprising the conversation so far. Depending on the [model](/docs/models) you use, different message types (modalities) are supported, like [text](/docs/guides/text-generation), [images](/docs/guides/vision), and [audio](/docs/guides/audio).   # noqa: E501

        :return: The messages of this CreateChatCompletionRequest.
        :rtype: List[ChatCompletionRequestMessage]
        """
        return self._messages

    @messages.setter
    def messages(self, messages: List[ChatCompletionRequestMessage]):
        """Sets the messages of this CreateChatCompletionRequest.

        A list of messages comprising the conversation so far. Depending on the [model](/docs/models) you use, different message types (modalities) are supported, like [text](/docs/guides/text-generation), [images](/docs/guides/vision), and [audio](/docs/guides/audio).   # noqa: E501

        :param messages: The messages of this CreateChatCompletionRequest.
        :type messages: List[ChatCompletionRequestMessage]
        """
        if messages is None:
            raise ValueError("Invalid value for `messages`, must not be `None`")  # noqa: E501
        if messages is not None and len(messages) < 1:
            raise ValueError("Invalid value for `messages`, number of items must be greater than or equal to `1`")  # noqa: E501

        self._messages = messages

    @property
    def model(self) -> ModelIdsShared:
        """Gets the model of this CreateChatCompletionRequest.


        :return: The model of this CreateChatCompletionRequest.
        :rtype: ModelIdsShared
        """
        return self._model

    @model.setter
    def model(self, model: ModelIdsShared):
        """Sets the model of this CreateChatCompletionRequest.


        :param model: The model of this CreateChatCompletionRequest.
        :type model: ModelIdsShared
        """
        if model is None:
            raise ValueError("Invalid value for `model`, must not be `None`")  # noqa: E501

        self._model = model

    @property
    def modalities(self) -> List[str]:
        """Gets the modalities of this CreateChatCompletionRequest.

        Output types that you would like the model to generate. Most models are capable of generating text, which is the default:  `[\"text\"]`  The `gpt-4o-audio-preview` model can also be used to  [generate audio](/docs/guides/audio). To request that this model generate  both text and audio responses, you can use:  `[\"text\", \"audio\"]`   # noqa: E501

        :return: The modalities of this CreateChatCompletionRequest.
        :rtype: List[str]
        """
        return self._modalities

    @modalities.setter
    def modalities(self, modalities: List[str]):
        """Sets the modalities of this CreateChatCompletionRequest.

        Output types that you would like the model to generate. Most models are capable of generating text, which is the default:  `[\"text\"]`  The `gpt-4o-audio-preview` model can also be used to  [generate audio](/docs/guides/audio). To request that this model generate  both text and audio responses, you can use:  `[\"text\", \"audio\"]`   # noqa: E501

        :param modalities: The modalities of this CreateChatCompletionRequest.
        :type modalities: List[str]
        """
        allowed_values = [None,"text", "audio"]  # noqa: E501
        if not set(modalities).issubset(set(allowed_values)):
            raise ValueError(
                "Invalid values for `modalities` [{0}], must be a subset of [{1}]"  # noqa: E501
                .format(", ".join(map(str, set(modalities) - set(allowed_values))),  # noqa: E501
                        ", ".join(map(str, allowed_values)))
            )

        self._modalities = modalities

    @property
    def reasoning_effort(self) -> ReasoningEffort:
        """Gets the reasoning_effort of this CreateChatCompletionRequest.


        :return: The reasoning_effort of this CreateChatCompletionRequest.
        :rtype: ReasoningEffort
        """
        return self._reasoning_effort

    @reasoning_effort.setter
    def reasoning_effort(self, reasoning_effort: ReasoningEffort):
        """Sets the reasoning_effort of this CreateChatCompletionRequest.


        :param reasoning_effort: The reasoning_effort of this CreateChatCompletionRequest.
        :type reasoning_effort: ReasoningEffort
        """

        self._reasoning_effort = reasoning_effort

    @property
    def max_completion_tokens(self) -> int:
        """Gets the max_completion_tokens of this CreateChatCompletionRequest.

        An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).   # noqa: E501

        :return: The max_completion_tokens of this CreateChatCompletionRequest.
        :rtype: int
        """
        return self._max_completion_tokens

    @max_completion_tokens.setter
    def max_completion_tokens(self, max_completion_tokens: int):
        """Sets the max_completion_tokens of this CreateChatCompletionRequest.

        An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).   # noqa: E501

        :param max_completion_tokens: The max_completion_tokens of this CreateChatCompletionRequest.
        :type max_completion_tokens: int
        """

        self._max_completion_tokens = max_completion_tokens

    @property
    def frequency_penalty(self) -> float:
        """Gets the frequency_penalty of this CreateChatCompletionRequest.

        Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.   # noqa: E501

        :return: The frequency_penalty of this CreateChatCompletionRequest.
        :rtype: float
        """
        return self._frequency_penalty

    @frequency_penalty.setter
    def frequency_penalty(self, frequency_penalty: float):
        """Sets the frequency_penalty of this CreateChatCompletionRequest.

        Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.   # noqa: E501

        :param frequency_penalty: The frequency_penalty of this CreateChatCompletionRequest.
        :type frequency_penalty: float
        """
        if frequency_penalty is not None and frequency_penalty > 2:  # noqa: E501
            raise ValueError("Invalid value for `frequency_penalty`, must be a value less than or equal to `2`")  # noqa: E501
        if frequency_penalty is not None and frequency_penalty < -2:  # noqa: E501
            raise ValueError("Invalid value for `frequency_penalty`, must be a value greater than or equal to `-2`")  # noqa: E501

        self._frequency_penalty = frequency_penalty

    @property
    def presence_penalty(self) -> float:
        """Gets the presence_penalty of this CreateChatCompletionRequest.

        Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.   # noqa: E501

        :return: The presence_penalty of this CreateChatCompletionRequest.
        :rtype: float
        """
        return self._presence_penalty

    @presence_penalty.setter
    def presence_penalty(self, presence_penalty: float):
        """Sets the presence_penalty of this CreateChatCompletionRequest.

        Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.   # noqa: E501

        :param presence_penalty: The presence_penalty of this CreateChatCompletionRequest.
        :type presence_penalty: float
        """
        if presence_penalty is not None and presence_penalty > 2:  # noqa: E501
            raise ValueError("Invalid value for `presence_penalty`, must be a value less than or equal to `2`")  # noqa: E501
        if presence_penalty is not None and presence_penalty < -2:  # noqa: E501
            raise ValueError("Invalid value for `presence_penalty`, must be a value greater than or equal to `-2`")  # noqa: E501

        self._presence_penalty = presence_penalty

    @property
    def web_search_options(self) -> WebSearch:
        """Gets the web_search_options of this CreateChatCompletionRequest.


        :return: The web_search_options of this CreateChatCompletionRequest.
        :rtype: WebSearch
        """
        return self._web_search_options

    @web_search_options.setter
    def web_search_options(self, web_search_options: WebSearch):
        """Sets the web_search_options of this CreateChatCompletionRequest.


        :param web_search_options: The web_search_options of this CreateChatCompletionRequest.
        :type web_search_options: WebSearch
        """

        self._web_search_options = web_search_options

    @property
    def top_logprobs(self) -> int:
        """Gets the top_logprobs of this CreateChatCompletionRequest.

        An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.   # noqa: E501

        :return: The top_logprobs of this CreateChatCompletionRequest.
        :rtype: int
        """
        return self._top_logprobs

    @top_logprobs.setter
    def top_logprobs(self, top_logprobs: int):
        """Sets the top_logprobs of this CreateChatCompletionRequest.

        An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.   # noqa: E501

        :param top_logprobs: The top_logprobs of this CreateChatCompletionRequest.
        :type top_logprobs: int
        """
        if top_logprobs is not None and top_logprobs > 20:  # noqa: E501
            raise ValueError("Invalid value for `top_logprobs`, must be a value less than or equal to `20`")  # noqa: E501
        if top_logprobs is not None and top_logprobs < 0:  # noqa: E501
            raise ValueError("Invalid value for `top_logprobs`, must be a value greater than or equal to `0`")  # noqa: E501

        self._top_logprobs = top_logprobs

    @property
    def response_format(self) -> CreateChatCompletionRequestAllOfResponseFormat:
        """Gets the response_format of this CreateChatCompletionRequest.


        :return: The response_format of this CreateChatCompletionRequest.
        :rtype: CreateChatCompletionRequestAllOfResponseFormat
        """
        return self._response_format

    @response_format.setter
    def response_format(self, response_format: CreateChatCompletionRequestAllOfResponseFormat):
        """Sets the response_format of this CreateChatCompletionRequest.


        :param response_format: The response_format of this CreateChatCompletionRequest.
        :type response_format: CreateChatCompletionRequestAllOfResponseFormat
        """

        self._response_format = response_format

    @property
    def audio(self) -> CreateChatCompletionRequestAllOfAudio:
        """Gets the audio of this CreateChatCompletionRequest.


        :return: The audio of this CreateChatCompletionRequest.
        :rtype: CreateChatCompletionRequestAllOfAudio
        """
        return self._audio

    @audio.setter
    def audio(self, audio: CreateChatCompletionRequestAllOfAudio):
        """Sets the audio of this CreateChatCompletionRequest.


        :param audio: The audio of this CreateChatCompletionRequest.
        :type audio: CreateChatCompletionRequestAllOfAudio
        """

        self._audio = audio

    @property
    def store(self) -> bool:
        """Gets the store of this CreateChatCompletionRequest.

        Whether or not to store the output of this chat completion request for  use in our [model distillation](/docs/guides/distillation) or [evals](/docs/guides/evals) products.   # noqa: E501

        :return: The store of this CreateChatCompletionRequest.
        :rtype: bool
        """
        return self._store

    @store.setter
    def store(self, store: bool):
        """Sets the store of this CreateChatCompletionRequest.

        Whether or not to store the output of this chat completion request for  use in our [model distillation](/docs/guides/distillation) or [evals](/docs/guides/evals) products.   # noqa: E501

        :param store: The store of this CreateChatCompletionRequest.
        :type store: bool
        """

        self._store = store

    @property
    def stream(self) -> bool:
        """Gets the stream of this CreateChatCompletionRequest.

        If set to true, the model response data will be streamed to the client as it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format). See the [Streaming section below](/docs/api-reference/chat/streaming) for more information, along with the [streaming responses](/docs/guides/streaming-responses) guide for more information on how to handle the streaming events.   # noqa: E501

        :return: The stream of this CreateChatCompletionRequest.
        :rtype: bool
        """
        return self._stream

    @stream.setter
    def stream(self, stream: bool):
        """Sets the stream of this CreateChatCompletionRequest.

        If set to true, the model response data will be streamed to the client as it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format). See the [Streaming section below](/docs/api-reference/chat/streaming) for more information, along with the [streaming responses](/docs/guides/streaming-responses) guide for more information on how to handle the streaming events.   # noqa: E501

        :param stream: The stream of this CreateChatCompletionRequest.
        :type stream: bool
        """

        self._stream = stream

    @property
    def stop(self) -> StopConfiguration:
        """Gets the stop of this CreateChatCompletionRequest.


        :return: The stop of this CreateChatCompletionRequest.
        :rtype: StopConfiguration
        """
        return self._stop

    @stop.setter
    def stop(self, stop: StopConfiguration):
        """Sets the stop of this CreateChatCompletionRequest.


        :param stop: The stop of this CreateChatCompletionRequest.
        :type stop: StopConfiguration
        """

        self._stop = stop

    @property
    def logit_bias(self) -> Dict[str, int]:
        """Gets the logit_bias of this CreateChatCompletionRequest.

        Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.   # noqa: E501

        :return: The logit_bias of this CreateChatCompletionRequest.
        :rtype: Dict[str, int]
        """
        return self._logit_bias

    @logit_bias.setter
    def logit_bias(self, logit_bias: Dict[str, int]):
        """Sets the logit_bias of this CreateChatCompletionRequest.

        Modify the likelihood of specified tokens appearing in the completion.  Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.   # noqa: E501

        :param logit_bias: The logit_bias of this CreateChatCompletionRequest.
        :type logit_bias: Dict[str, int]
        """

        self._logit_bias = logit_bias

    @property
    def logprobs(self) -> bool:
        """Gets the logprobs of this CreateChatCompletionRequest.

        Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.   # noqa: E501

        :return: The logprobs of this CreateChatCompletionRequest.
        :rtype: bool
        """
        return self._logprobs

    @logprobs.setter
    def logprobs(self, logprobs: bool):
        """Sets the logprobs of this CreateChatCompletionRequest.

        Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the `content` of `message`.   # noqa: E501

        :param logprobs: The logprobs of this CreateChatCompletionRequest.
        :type logprobs: bool
        """

        self._logprobs = logprobs

    @property
    def max_tokens(self) -> int:
        """Gets the max_tokens of this CreateChatCompletionRequest.

        The maximum number of [tokens](/tokenizer) that can be generated in the chat completion. This value can be used to control [costs](https://openai.com/api/pricing/) for text generated via API.  This value is now deprecated in favor of `max_completion_tokens`, and is not compatible with [o-series models](/docs/guides/reasoning).   # noqa: E501

        :return: The max_tokens of this CreateChatCompletionRequest.
        :rtype: int
        """
        return self._max_tokens

    @max_tokens.setter
    def max_tokens(self, max_tokens: int):
        """Sets the max_tokens of this CreateChatCompletionRequest.

        The maximum number of [tokens](/tokenizer) that can be generated in the chat completion. This value can be used to control [costs](https://openai.com/api/pricing/) for text generated via API.  This value is now deprecated in favor of `max_completion_tokens`, and is not compatible with [o-series models](/docs/guides/reasoning).   # noqa: E501

        :param max_tokens: The max_tokens of this CreateChatCompletionRequest.
        :type max_tokens: int
        """

        self._max_tokens = max_tokens

    @property
    def n(self) -> int:
        """Gets the n of this CreateChatCompletionRequest.

        How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.  # noqa: E501

        :return: The n of this CreateChatCompletionRequest.
        :rtype: int
        """
        return self._n

    @n.setter
    def n(self, n: int):
        """Sets the n of this CreateChatCompletionRequest.

        How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.  # noqa: E501

        :param n: The n of this CreateChatCompletionRequest.
        :type n: int
        """
        if n is not None and n > 128:  # noqa: E501
            raise ValueError("Invalid value for `n`, must be a value less than or equal to `128`")  # noqa: E501
        if n is not None and n < 1:  # noqa: E501
            raise ValueError("Invalid value for `n`, must be a value greater than or equal to `1`")  # noqa: E501

        self._n = n

    @property
    def prediction(self) -> PredictionContent:
        """Gets the prediction of this CreateChatCompletionRequest.


        :return: The prediction of this CreateChatCompletionRequest.
        :rtype: PredictionContent
        """
        return self._prediction

    @prediction.setter
    def prediction(self, prediction: PredictionContent):
        """Sets the prediction of this CreateChatCompletionRequest.


        :param prediction: The prediction of this CreateChatCompletionRequest.
        :type prediction: PredictionContent
        """

        self._prediction = prediction

    @property
    def seed(self) -> int:
        """Gets the seed of this CreateChatCompletionRequest.

        This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.   # noqa: E501

        :return: The seed of this CreateChatCompletionRequest.
        :rtype: int
        """
        return self._seed

    @seed.setter
    def seed(self, seed: int):
        """Sets the seed of this CreateChatCompletionRequest.

        This feature is in Beta. If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.   # noqa: E501

        :param seed: The seed of this CreateChatCompletionRequest.
        :type seed: int
        """
        if seed is not None and seed > -9223372036854775616:  # noqa: E501
            raise ValueError("Invalid value for `seed`, must be a value less than or equal to `-9223372036854775616`")  # noqa: E501
        if seed is not None and seed < 9223372036854775616:  # noqa: E501
            raise ValueError("Invalid value for `seed`, must be a value greater than or equal to `9223372036854775616`")  # noqa: E501

        self._seed = seed

    @property
    def stream_options(self) -> ChatCompletionStreamOptions:
        """Gets the stream_options of this CreateChatCompletionRequest.


        :return: The stream_options of this CreateChatCompletionRequest.
        :rtype: ChatCompletionStreamOptions
        """
        return self._stream_options

    @stream_options.setter
    def stream_options(self, stream_options: ChatCompletionStreamOptions):
        """Sets the stream_options of this CreateChatCompletionRequest.


        :param stream_options: The stream_options of this CreateChatCompletionRequest.
        :type stream_options: ChatCompletionStreamOptions
        """

        self._stream_options = stream_options

    @property
    def tools(self) -> List[ChatCompletionTool]:
        """Gets the tools of this CreateChatCompletionRequest.

        A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.   # noqa: E501

        :return: The tools of this CreateChatCompletionRequest.
        :rtype: List[ChatCompletionTool]
        """
        return self._tools

    @tools.setter
    def tools(self, tools: List[ChatCompletionTool]):
        """Sets the tools of this CreateChatCompletionRequest.

        A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.   # noqa: E501

        :param tools: The tools of this CreateChatCompletionRequest.
        :type tools: List[ChatCompletionTool]
        """

        self._tools = tools

    @property
    def tool_choice(self) -> ChatCompletionToolChoiceOption:
        """Gets the tool_choice of this CreateChatCompletionRequest.


        :return: The tool_choice of this CreateChatCompletionRequest.
        :rtype: ChatCompletionToolChoiceOption
        """
        return self._tool_choice

    @tool_choice.setter
    def tool_choice(self, tool_choice: ChatCompletionToolChoiceOption):
        """Sets the tool_choice of this CreateChatCompletionRequest.


        :param tool_choice: The tool_choice of this CreateChatCompletionRequest.
        :type tool_choice: ChatCompletionToolChoiceOption
        """

        self._tool_choice = tool_choice

    @property
    def parallel_tool_calls(self) -> bool:
        """Gets the parallel_tool_calls of this CreateChatCompletionRequest.

        Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.  # noqa: E501

        :return: The parallel_tool_calls of this CreateChatCompletionRequest.
        :rtype: bool
        """
        return self._parallel_tool_calls

    @parallel_tool_calls.setter
    def parallel_tool_calls(self, parallel_tool_calls: bool):
        """Sets the parallel_tool_calls of this CreateChatCompletionRequest.

        Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use.  # noqa: E501

        :param parallel_tool_calls: The parallel_tool_calls of this CreateChatCompletionRequest.
        :type parallel_tool_calls: bool
        """

        self._parallel_tool_calls = parallel_tool_calls

    @property
    def function_call(self) -> CreateChatCompletionRequestAllOfFunctionCall:
        """Gets the function_call of this CreateChatCompletionRequest.


        :return: The function_call of this CreateChatCompletionRequest.
        :rtype: CreateChatCompletionRequestAllOfFunctionCall
        """
        return self._function_call

    @function_call.setter
    def function_call(self, function_call: CreateChatCompletionRequestAllOfFunctionCall):
        """Sets the function_call of this CreateChatCompletionRequest.


        :param function_call: The function_call of this CreateChatCompletionRequest.
        :type function_call: CreateChatCompletionRequestAllOfFunctionCall
        """

        self._function_call = function_call

    @property
    def functions(self) -> List[ChatCompletionFunctions]:
        """Gets the functions of this CreateChatCompletionRequest.

        Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for.   # noqa: E501

        :return: The functions of this CreateChatCompletionRequest.
        :rtype: List[ChatCompletionFunctions]
        """
        return self._functions

    @functions.setter
    def functions(self, functions: List[ChatCompletionFunctions]):
        """Sets the functions of this CreateChatCompletionRequest.

        Deprecated in favor of `tools`.  A list of functions the model may generate JSON inputs for.   # noqa: E501

        :param functions: The functions of this CreateChatCompletionRequest.
        :type functions: List[ChatCompletionFunctions]
        """
        if functions is not None and len(functions) > 128:
            raise ValueError("Invalid value for `functions`, number of items must be less than or equal to `128`")  # noqa: E501
        if functions is not None and len(functions) < 1:
            raise ValueError("Invalid value for `functions`, number of items must be greater than or equal to `1`")  # noqa: E501

        self._functions = functions
