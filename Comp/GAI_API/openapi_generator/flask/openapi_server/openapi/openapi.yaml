openapi: 3.0.0
info:
  contact:
    name: OpenAI Support
    url: https://help.openai.com/
  description: The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference
    for more details.
  license:
    name: MIT
    url: https://github.com/openai/openai-openapi/blob/master/LICENSE
  termsOfService: https://openai.com/policies/terms-of-use
  title: OpenAI API
  version: 2.3.0
servers:
- url: https://api.openai.com/v1
security:
- ApiKeyAuth: []
tags:
- description: Build Assistants that can call models and use tools.
  name: Assistants
- description: Turn audio into text or text into audio.
  name: Audio
- description: "Given a list of messages comprising a conversation, the model will\
    \ return a response."
  name: Chat
- description: "Given a prompt, the model will return one or more predicted completions,\
    \ and can also return the probabilities of alternative tokens at each position."
  name: Completions
- description: Get a vector representation of a given input that can be easily consumed
    by machine learning models and algorithms.
  name: Embeddings
- description: Manage and run evals in the OpenAI platform.
  name: Evals
- description: Manage fine-tuning jobs to tailor a model to your specific training
    data.
  name: Fine-tuning
- description: Create large batches of API requests to run asynchronously.
  name: Batch
- description: Files are used to upload documents that can be used with features like
    Assistants and Fine-tuning.
  name: Files
- description: Use Uploads to upload large files in multiple parts.
  name: Uploads
- description: "Given a prompt and/or an input image, the model will generate a new\
    \ image."
  name: Images
- description: List and describe the various models available in the API.
  name: Models
- description: "Given text and/or image inputs, classifies if those inputs are potentially\
    \ harmful."
  name: Moderations
- description: List user actions and configuration changes within this organization.
  name: Audit Logs
paths:
  /chat/completions:
    get:
      operationId: list_chat_completions
      parameters:
      - description: The model used to generate the Chat Completions.
        explode: true
        in: query
        name: model
        required: false
        schema:
          type: string
        style: form
      - description: |
          A list of metadata keys to filter the Chat Completions by. Example:

          `metadata[key1]=value1&metadata[key2]=value2`
        explode: true
        in: query
        name: metadata
        required: false
        schema:
          $ref: "#/components/schemas/Metadata"
        style: form
      - description: Identifier for the last chat completion from the previous pagination
          request.
        explode: true
        in: query
        name: after
        required: false
        schema:
          type: string
        style: form
      - description: Number of Chat Completions to retrieve.
        explode: true
        in: query
        name: limit
        required: false
        schema:
          default: 20
          type: integer
        style: form
      - description: Sort order for Chat Completions by timestamp. Use `asc` for ascending
          order or `desc` for descending order. Defaults to `asc`.
        explode: true
        in: query
        name: order
        required: false
        schema:
          default: asc
          enum:
          - asc
          - desc
          type: string
        style: form
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ChatCompletionList"
          description: A list of Chat Completions
      summary: |
        List stored Chat Completions. Only Chat Completions that have been stored
        with the `store` parameter set to `true` will be returned.
      tags:
      - Chat
      x-oaiMeta:
        name: List Chat Completions
        group: chat
        returns: "A list of [Chat Completions](/docs/api-reference/chat/list-object)\
          \ matching the specified filters."
        path: list
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/chat/completions \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json"
            python: |
              from openai import OpenAI
              client = OpenAI()

              completions = client.chat.completions.list()
              print(completions)
          response: |
            {
              "object": "list",
              "data": [
                {
                  "object": "chat.completion",
                  "id": "chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2",
                  "model": "gpt-4.1-2025-04-14",
                  "created": 1738960610,
                  "request_id": "req_ded8ab984ec4bf840f37566c1011c417",
                  "tool_choice": null,
                  "usage": {
                    "total_tokens": 31,
                    "completion_tokens": 18,
                    "prompt_tokens": 13
                  },
                  "seed": 4944116822809979520,
                  "top_p": 1.0,
                  "temperature": 1.0,
                  "presence_penalty": 0.0,
                  "frequency_penalty": 0.0,
                  "system_fingerprint": "fp_50cad350e4",
                  "input_user": null,
                  "service_tier": "default",
                  "tools": null,
                  "metadata": {},
                  "choices": [
                    {
                      "index": 0,
                      "message": {
                        "content": "Mind of circuits hum,  \nLearning patterns in silence—  \nFuture's quiet spark.",
                        "role": "assistant",
                        "tool_calls": null,
                        "function_call": null
                      },
                      "finish_reason": "stop",
                      "logprobs": null
                    }
                  ],
                  "response_format": null
                }
              ],
              "first_id": "chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2",
              "last_id": "chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2",
              "has_more": false
            }
      x-openapi-router-controller: openapi_server.controllers.chat_controller
    post:
      operationId: create_chat_completion
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateChatCompletionRequest"
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CreateChatCompletionResponse"
            text/event-stream:
              schema:
                $ref: "#/components/schemas/CreateChatCompletionStreamResponse"
          description: OK
      summary: "**Starting a new project?** We recommend trying [Responses](/docs/api-reference/responses)\
        \ \nto take advantage of the latest OpenAI platform features. Compare\n[Chat\
        \ Completions with Responses](/docs/guides/responses-vs-chat-completions?api-mode=responses).\n\
        \n---\n\nCreates a model response for the given chat conversation. Learn more\
        \ in the\n[text generation](/docs/guides/text-generation), [vision](/docs/guides/vision),\n\
        and [audio](/docs/guides/audio) guides.\n\nParameter support can differ depending\
        \ on the model used to generate the\nresponse, particularly for newer reasoning\
        \ models. Parameters that are only\nsupported for reasoning models are noted\
        \ below. For the current state of \nunsupported parameters in reasoning models,\
        \ \n[refer to the reasoning guide](/docs/guides/reasoning).\n"
      tags:
      - Chat
      x-oaiMeta:
        name: Create chat completion
        group: chat
        returns: |
          Returns a [chat completion](/docs/api-reference/chat/object) object, or a streamed sequence of [chat completion chunk](/docs/api-reference/chat/streaming) objects if the request is streamed.
        path: create
        examples:
        - title: Default
          request:
            curl: |
              curl https://api.openai.com/v1/chat/completions \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -d '{
                  "model": "VAR_chat_model_id",
                  "messages": [
                    {
                      "role": "developer",
                      "content": "You are a helpful assistant."
                    },
                    {
                      "role": "user",
                      "content": "Hello!"
                    }
                  ]
                }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              completion = client.chat.completions.create(
                model="VAR_chat_model_id",
                messages=[
                  {"role": "developer", "content": "You are a helpful assistant."},
                  {"role": "user", "content": "Hello!"}
                ]
              )

              print(completion.choices[0].message)
            node.js: |
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const completion = await openai.chat.completions.create({
                  messages: [{ role: "developer", content: "You are a helpful assistant." }],
                  model: "VAR_chat_model_id",
                  store: true,
                });

                console.log(completion.choices[0]);
              }

              main();
            csharp: |
              using System;
              using System.Collections.Generic;

              using OpenAI.Chat;

              ChatClient client = new(
                  model: "gpt-4.1",
                  apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
              );

              List<ChatMessage> messages =
              [
                  new SystemChatMessage("You are a helpful assistant."),
                  new UserChatMessage("Hello!")
              ];

              ChatCompletion completion = client.CompleteChat(messages);

              Console.WriteLine(completion.Content[0].Text);
          response: |
            {
              "id": "chatcmpl-B9MBs8CjcvOU2jLn4n570S5qMJKcT",
              "object": "chat.completion",
              "created": 1741569952,
              "model": "gpt-4.1-2025-04-14",
              "choices": [
                {
                  "index": 0,
                  "message": {
                    "role": "assistant",
                    "content": "Hello! How can I assist you today?",
                    "refusal": null,
                    "annotations": []
                  },
                  "logprobs": null,
                  "finish_reason": "stop"
                }
              ],
              "usage": {
                "prompt_tokens": 19,
                "completion_tokens": 10,
                "total_tokens": 29,
                "prompt_tokens_details": {
                  "cached_tokens": 0,
                  "audio_tokens": 0
                },
                "completion_tokens_details": {
                  "reasoning_tokens": 0,
                  "audio_tokens": 0,
                  "accepted_prediction_tokens": 0,
                  "rejected_prediction_tokens": 0
                }
              },
              "service_tier": "default"
            }
        - title: Image input
          request:
            curl: |
              curl https://api.openai.com/v1/chat/completions \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -d '{
                  "model": "gpt-4.1",
                  "messages": [
                    {
                      "role": "user",
                      "content": [
                        {
                          "type": "text",
                          "text": "What is in this image?"
                        },
                        {
                          "type": "image_url",
                          "image_url": {
                            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
                          }
                        }
                      ]
                    }
                  ],
                  "max_tokens": 300
                }'
            python: |
              from openai import OpenAI

              client = OpenAI()

              response = client.chat.completions.create(
                  model="gpt-4.1",
                  messages=[
                      {
                          "role": "user",
                          "content": [
                              {"type": "text", "text": "What's in this image?"},
                              {
                                  "type": "image_url",
                                  "image_url": {
                                      "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                                  }
                              },
                          ],
                      }
                  ],
                  max_tokens=300,
              )

              print(response.choices[0])
            node.js: |
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const response = await openai.chat.completions.create({
                  model: "gpt-4.1",
                  messages: [
                    {
                      role: "user",
                      content: [
                        { type: "text", text: "What's in this image?" },
                        {
                          type: "image_url",
                          image_url: {
                            "url": "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg",
                          },
                        }
                      ],
                    },
                  ],
                });
                console.log(response.choices[0]);
              }
              main();
            csharp: |
              using System;
              using System.Collections.Generic;

              using OpenAI.Chat;

              ChatClient client = new(
                  model: "gpt-4.1",
                  apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
              );

              List<ChatMessage> messages =
              [
                  new UserChatMessage(
                  [
                      ChatMessageContentPart.CreateTextPart("What's in this image?"),
                      ChatMessageContentPart.CreateImagePart(new Uri("https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"))
                  ])
              ];

              ChatCompletion completion = client.CompleteChat(messages);

              Console.WriteLine(completion.Content[0].Text);
          response: |
            {
              "id": "chatcmpl-B9MHDbslfkBeAs8l4bebGdFOJ6PeG",
              "object": "chat.completion",
              "created": 1741570283,
              "model": "gpt-4.1-2025-04-14",
              "choices": [
                {
                  "index": 0,
                  "message": {
                    "role": "assistant",
                    "content": "The image shows a wooden boardwalk path running through a lush green field or meadow. The sky is bright blue with some scattered clouds, giving the scene a serene and peaceful atmosphere. Trees and shrubs are visible in the background.",
                    "refusal": null,
                    "annotations": []
                  },
                  "logprobs": null,
                  "finish_reason": "stop"
                }
              ],
              "usage": {
                "prompt_tokens": 1117,
                "completion_tokens": 46,
                "total_tokens": 1163,
                "prompt_tokens_details": {
                  "cached_tokens": 0,
                  "audio_tokens": 0
                },
                "completion_tokens_details": {
                  "reasoning_tokens": 0,
                  "audio_tokens": 0,
                  "accepted_prediction_tokens": 0,
                  "rejected_prediction_tokens": 0
                }
              },
              "service_tier": "default"
            }
        - title: Streaming
          request:
            curl: |
              curl https://api.openai.com/v1/chat/completions \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -d '{
                  "model": "VAR_chat_model_id",
                  "messages": [
                    {
                      "role": "developer",
                      "content": "You are a helpful assistant."
                    },
                    {
                      "role": "user",
                      "content": "Hello!"
                    }
                  ],
                  "stream": true
                }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              completion = client.chat.completions.create(
                model="VAR_chat_model_id",
                messages=[
                  {"role": "developer", "content": "You are a helpful assistant."},
                  {"role": "user", "content": "Hello!"}
                ],
                stream=True
              )

              for chunk in completion:
                print(chunk.choices[0].delta)
            node.js: |
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const completion = await openai.chat.completions.create({
                  model: "VAR_chat_model_id",
                  messages: [
                    {"role": "developer", "content": "You are a helpful assistant."},
                    {"role": "user", "content": "Hello!"}
                  ],
                  stream: true,
                });

                for await (const chunk of completion) {
                  console.log(chunk.choices[0].delta.content);
                }
              }

              main();
            csharp: |
              using System;
              using System.ClientModel;
              using System.Collections.Generic;
              using System.Threading.Tasks;

              using OpenAI.Chat;

              ChatClient client = new(
                  model: "gpt-4.1",
                  apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
              );

              List<ChatMessage> messages =
              [
                  new SystemChatMessage("You are a helpful assistant."),
                  new UserChatMessage("Hello!")
              ];

              AsyncCollectionResult<StreamingChatCompletionUpdate> completionUpdates = client.CompleteChatStreamingAsync(messages);

              await foreach (StreamingChatCompletionUpdate completionUpdate in completionUpdates)
              {
                  if (completionUpdate.ContentUpdate.Count > 0)
                  {
                      Console.Write(completionUpdate.ContentUpdate[0].Text);
                  }
              }
          response: |
            {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}

            {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"content":"Hello"},"logprobs":null,"finish_reason":null}]}

            ....

            {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{},"logprobs":null,"finish_reason":"stop"}]}
        - title: Functions
          request:
            curl: |
              curl https://api.openai.com/v1/chat/completions \
              -H "Content-Type: application/json" \
              -H "Authorization: Bearer $OPENAI_API_KEY" \
              -d '{
                "model": "gpt-4.1",
                "messages": [
                  {
                    "role": "user",
                    "content": "What is the weather like in Boston today?"
                  }
                ],
                "tools": [
                  {
                    "type": "function",
                    "function": {
                      "name": "get_current_weather",
                      "description": "Get the current weather in a given location",
                      "parameters": {
                        "type": "object",
                        "properties": {
                          "location": {
                            "type": "string",
                            "description": "The city and state, e.g. San Francisco, CA"
                          },
                          "unit": {
                            "type": "string",
                            "enum": ["celsius", "fahrenheit"]
                          }
                        },
                        "required": ["location"]
                      }
                    }
                  }
                ],
                "tool_choice": "auto"
              }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              tools = [
                {
                  "type": "function",
                  "function": {
                    "name": "get_current_weather",
                    "description": "Get the current weather in a given location",
                    "parameters": {
                      "type": "object",
                      "properties": {
                        "location": {
                          "type": "string",
                          "description": "The city and state, e.g. San Francisco, CA",
                        },
                        "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                      },
                      "required": ["location"],
                    },
                  }
                }
              ]
              messages = [{"role": "user", "content": "What's the weather like in Boston today?"}]
              completion = client.chat.completions.create(
                model="VAR_chat_model_id",
                messages=messages,
                tools=tools,
                tool_choice="auto"
              )

              print(completion)
            node.js: |
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const messages = [{"role": "user", "content": "What's the weather like in Boston today?"}];
                const tools = [
                    {
                      "type": "function",
                      "function": {
                        "name": "get_current_weather",
                        "description": "Get the current weather in a given location",
                        "parameters": {
                          "type": "object",
                          "properties": {
                            "location": {
                              "type": "string",
                              "description": "The city and state, e.g. San Francisco, CA",
                            },
                            "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                          },
                          "required": ["location"],
                        },
                      }
                    }
                ];

                const response = await openai.chat.completions.create({
                  model: "gpt-4.1",
                  messages: messages,
                  tools: tools,
                  tool_choice: "auto",
                });

                console.log(response);
              }

              main();
            csharp: |
              using System;
              using System.Collections.Generic;

              using OpenAI.Chat;

              ChatClient client = new(
                  model: "gpt-4.1",
                  apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
              );

              ChatTool getCurrentWeatherTool = ChatTool.CreateFunctionTool(
                  functionName: "get_current_weather",
                  functionDescription: "Get the current weather in a given location",
                  functionParameters: BinaryData.FromString("""
                      {
                          "type": "object",
                          "properties": {
                              "location": {
                                  "type": "string",
                                  "description": "The city and state, e.g. San Francisco, CA"
                              },
                              "unit": {
                                  "type": "string",
                                  "enum": [ "celsius", "fahrenheit" ]
                              }
                          },
                          "required": [ "location" ]
                      }
                  """)
              );

              List<ChatMessage> messages =
              [
                  new UserChatMessage("What's the weather like in Boston today?"),
              ];

              ChatCompletionOptions options = new()
              {
                  Tools =
                  {
                      getCurrentWeatherTool
                  },
                  ToolChoice = ChatToolChoice.CreateAutoChoice(),
              };

              ChatCompletion completion = client.CompleteChat(messages, options);
          response: |
            {
              "id": "chatcmpl-abc123",
              "object": "chat.completion",
              "created": 1699896916,
              "model": "gpt-4o-mini",
              "choices": [
                {
                  "index": 0,
                  "message": {
                    "role": "assistant",
                    "content": null,
                    "tool_calls": [
                      {
                        "id": "call_abc123",
                        "type": "function",
                        "function": {
                          "name": "get_current_weather",
                          "arguments": "{\n\"location\": \"Boston, MA\"\n}"
                        }
                      }
                    ]
                  },
                  "logprobs": null,
                  "finish_reason": "tool_calls"
                }
              ],
              "usage": {
                "prompt_tokens": 82,
                "completion_tokens": 17,
                "total_tokens": 99,
                "completion_tokens_details": {
                  "reasoning_tokens": 0,
                  "accepted_prediction_tokens": 0,
                  "rejected_prediction_tokens": 0
                }
              }
            }
        - title: Logprobs
          request:
            curl: |
              curl https://api.openai.com/v1/chat/completions \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -d '{
                  "model": "VAR_chat_model_id",
                  "messages": [
                    {
                      "role": "user",
                      "content": "Hello!"
                    }
                  ],
                  "logprobs": true,
                  "top_logprobs": 2
                }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              completion = client.chat.completions.create(
                model="VAR_chat_model_id",
                messages=[
                  {"role": "user", "content": "Hello!"}
                ],
                logprobs=True,
                top_logprobs=2
              )

              print(completion.choices[0].message)
              print(completion.choices[0].logprobs)
            node.js: |
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const completion = await openai.chat.completions.create({
                  messages: [{ role: "user", content: "Hello!" }],
                  model: "VAR_chat_model_id",
                  logprobs: true,
                  top_logprobs: 2,
                });

                console.log(completion.choices[0]);
              }

              main();
            csharp: |
              using System;
              using System.Collections.Generic;

              using OpenAI.Chat;

              ChatClient client = new(
                  model: "gpt-4.1",
                  apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
              );

              List<ChatMessage> messages =
              [
                  new UserChatMessage("Hello!")
              ];

              ChatCompletionOptions options = new()
              {
                  IncludeLogProbabilities = true,
                  TopLogProbabilityCount = 2
              };

              ChatCompletion completion = client.CompleteChat(messages, options);

              Console.WriteLine(completion.Content[0].Text);
          response: |
            {
              "id": "chatcmpl-123",
              "object": "chat.completion",
              "created": 1702685778,
              "model": "gpt-4o-mini",
              "choices": [
                {
                  "index": 0,
                  "message": {
                    "role": "assistant",
                    "content": "Hello! How can I assist you today?"
                  },
                  "logprobs": {
                    "content": [
                      {
                        "token": "Hello",
                        "logprob": -0.31725305,
                        "bytes": [72, 101, 108, 108, 111],
                        "top_logprobs": [
                          {
                            "token": "Hello",
                            "logprob": -0.31725305,
                            "bytes": [72, 101, 108, 108, 111]
                          },
                          {
                            "token": "Hi",
                            "logprob": -1.3190403,
                            "bytes": [72, 105]
                          }
                        ]
                      },
                      {
                        "token": "!",
                        "logprob": -0.02380986,
                        "bytes": [
                          33
                        ],
                        "top_logprobs": [
                          {
                            "token": "!",
                            "logprob": -0.02380986,
                            "bytes": [33]
                          },
                          {
                            "token": " there",
                            "logprob": -3.787621,
                            "bytes": [32, 116, 104, 101, 114, 101]
                          }
                        ]
                      },
                      {
                        "token": " How",
                        "logprob": -0.000054669687,
                        "bytes": [32, 72, 111, 119],
                        "top_logprobs": [
                          {
                            "token": " How",
                            "logprob": -0.000054669687,
                            "bytes": [32, 72, 111, 119]
                          },
                          {
                            "token": "<|end|>",
                            "logprob": -10.953937,
                            "bytes": null
                          }
                        ]
                      },
                      {
                        "token": " can",
                        "logprob": -0.015801601,
                        "bytes": [32, 99, 97, 110],
                        "top_logprobs": [
                          {
                            "token": " can",
                            "logprob": -0.015801601,
                            "bytes": [32, 99, 97, 110]
                          },
                          {
                            "token": " may",
                            "logprob": -4.161023,
                            "bytes": [32, 109, 97, 121]
                          }
                        ]
                      },
                      {
                        "token": " I",
                        "logprob": -3.7697225e-6,
                        "bytes": [
                          32,
                          73
                        ],
                        "top_logprobs": [
                          {
                            "token": " I",
                            "logprob": -3.7697225e-6,
                            "bytes": [32, 73]
                          },
                          {
                            "token": " assist",
                            "logprob": -13.596657,
                            "bytes": [32, 97, 115, 115, 105, 115, 116]
                          }
                        ]
                      },
                      {
                        "token": " assist",
                        "logprob": -0.04571125,
                        "bytes": [32, 97, 115, 115, 105, 115, 116],
                        "top_logprobs": [
                          {
                            "token": " assist",
                            "logprob": -0.04571125,
                            "bytes": [32, 97, 115, 115, 105, 115, 116]
                          },
                          {
                            "token": " help",
                            "logprob": -3.1089056,
                            "bytes": [32, 104, 101, 108, 112]
                          }
                        ]
                      },
                      {
                        "token": " you",
                        "logprob": -5.4385737e-6,
                        "bytes": [32, 121, 111, 117],
                        "top_logprobs": [
                          {
                            "token": " you",
                            "logprob": -5.4385737e-6,
                            "bytes": [32, 121, 111, 117]
                          },
                          {
                            "token": " today",
                            "logprob": -12.807695,
                            "bytes": [32, 116, 111, 100, 97, 121]
                          }
                        ]
                      },
                      {
                        "token": " today",
                        "logprob": -0.0040071653,
                        "bytes": [32, 116, 111, 100, 97, 121],
                        "top_logprobs": [
                          {
                            "token": " today",
                            "logprob": -0.0040071653,
                            "bytes": [32, 116, 111, 100, 97, 121]
                          },
                          {
                            "token": "?",
                            "logprob": -5.5247097,
                            "bytes": [63]
                          }
                        ]
                      },
                      {
                        "token": "?",
                        "logprob": -0.0008108172,
                        "bytes": [63],
                        "top_logprobs": [
                          {
                            "token": "?",
                            "logprob": -0.0008108172,
                            "bytes": [63]
                          },
                          {
                            "token": "?\n",
                            "logprob": -7.184561,
                            "bytes": [63, 10]
                          }
                        ]
                      }
                    ]
                  },
                  "finish_reason": "stop"
                }
              ],
              "usage": {
                "prompt_tokens": 9,
                "completion_tokens": 9,
                "total_tokens": 18,
                "completion_tokens_details": {
                  "reasoning_tokens": 0,
                  "accepted_prediction_tokens": 0,
                  "rejected_prediction_tokens": 0
                }
              },
              "system_fingerprint": null
            }
      x-openapi-router-controller: openapi_server.controllers.chat_controller
  /chat/completions/{completion_id}:
    delete:
      operationId: delete_chat_completion
      parameters:
      - description: The ID of the chat completion to delete.
        explode: false
        in: path
        name: completion_id
        required: true
        schema:
          type: string
        style: simple
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ChatCompletionDeleted"
          description: The chat completion was deleted successfully.
      summary: |
        Delete a stored chat completion. Only Chat Completions that have been
        created with the `store` parameter set to `true` can be deleted.
      tags:
      - Chat
      x-oaiMeta:
        name: Delete chat completion
        group: chat
        returns: A deletion confirmation object.
        examples:
          request:
            curl: |
              curl -X DELETE https://api.openai.com/v1/chat/completions/chat_abc123 \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json"
            python: |
              from openai import OpenAI
              client = OpenAI()

              completions = client.chat.completions.list()
              first_id = completions[0].id
              delete_response = client.chat.completions.delete(completion_id=first_id)
              print(delete_response)
          response: |
            {
              "object": "chat.completion.deleted",
              "id": "chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2",
              "deleted": true
            }
      x-openapi-router-controller: openapi_server.controllers.chat_controller
    get:
      operationId: get_chat_completion
      parameters:
      - description: The ID of the chat completion to retrieve.
        explode: false
        in: path
        name: completion_id
        required: true
        schema:
          type: string
        style: simple
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CreateChatCompletionResponse"
          description: A chat completion
      summary: |
        Get a stored chat completion. Only Chat Completions that have been created
        with the `store` parameter set to `true` will be returned.
      tags:
      - Chat
      x-oaiMeta:
        name: Get chat completion
        group: chat
        returns: "The [ChatCompletion](/docs/api-reference/chat/object) object matching\
          \ the specified ID."
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/chat/completions/chatcmpl-abc123 \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json"
            python: |
              from openai import OpenAI
              client = OpenAI()

              completions = client.chat.completions.list()
              first_id = completions[0].id
              first_completion = client.chat.completions.retrieve(completion_id=first_id)
              print(first_completion)
          response: |
            {
              "object": "chat.completion",
              "id": "chatcmpl-abc123",
              "model": "gpt-4o-2024-08-06",
              "created": 1738960610,
              "request_id": "req_ded8ab984ec4bf840f37566c1011c417",
              "tool_choice": null,
              "usage": {
                "total_tokens": 31,
                "completion_tokens": 18,
                "prompt_tokens": 13
              },
              "seed": 4944116822809979520,
              "top_p": 1.0,
              "temperature": 1.0,
              "presence_penalty": 0.0,
              "frequency_penalty": 0.0,
              "system_fingerprint": "fp_50cad350e4",
              "input_user": null,
              "service_tier": "default",
              "tools": null,
              "metadata": {},
              "choices": [
                {
                  "index": 0,
                  "message": {
                    "content": "Mind of circuits hum,  \nLearning patterns in silence—  \nFuture's quiet spark.",
                    "role": "assistant",
                    "tool_calls": null,
                    "function_call": null
                  },
                  "finish_reason": "stop",
                  "logprobs": null
                }
              ],
              "response_format": null
            }
      x-openapi-router-controller: openapi_server.controllers.chat_controller
    post:
      operationId: update_chat_completion
      parameters:
      - description: The ID of the chat completion to update.
        explode: false
        in: path
        name: completion_id
        required: true
        schema:
          type: string
        style: simple
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/updateChatCompletion_request"
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CreateChatCompletionResponse"
          description: A chat completion
      summary: |
        Modify a stored chat completion. Only Chat Completions that have been
        created with the `store` parameter set to `true` can be modified. Currently,
        the only supported modification is to update the `metadata` field.
      tags:
      - Chat
      x-oaiMeta:
        name: Update chat completion
        group: chat
        returns: "The [ChatCompletion](/docs/api-reference/chat/object) object matching\
          \ the specified ID."
        examples:
          request:
            curl: |
              curl -X POST https://api.openai.com/v1/chat/completions/chat_abc123 \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -d '{"metadata": {"foo": "bar"}}'
            python: |
              from openai import OpenAI
              client = OpenAI()

              completions = client.chat.completions.list()
              first_id = completions[0].id
              updated_completion = client.chat.completions.update(completion_id=first_id, request_body={"metadata": {"foo": "bar"}})
              print(updated_completion)
          response: |
            {
              "object": "chat.completion",
              "id": "chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2",
              "model": "gpt-4o-2024-08-06",
              "created": 1738960610,
              "request_id": "req_ded8ab984ec4bf840f37566c1011c417",
              "tool_choice": null,
              "usage": {
                "total_tokens": 31,
                "completion_tokens": 18,
                "prompt_tokens": 13
              },
              "seed": 4944116822809979520,
              "top_p": 1.0,
              "temperature": 1.0,
              "presence_penalty": 0.0,
              "frequency_penalty": 0.0,
              "system_fingerprint": "fp_50cad350e4",
              "input_user": null,
              "service_tier": "default",
              "tools": null,
              "metadata": {
                "foo": "bar"
              },
              "choices": [
                {
                  "index": 0,
                  "message": {
                    "content": "Mind of circuits hum,  \nLearning patterns in silence—  \nFuture's quiet spark.",
                    "role": "assistant",
                    "tool_calls": null,
                    "function_call": null
                  },
                  "finish_reason": "stop",
                  "logprobs": null
                }
              ],
              "response_format": null
            }
      x-openapi-router-controller: openapi_server.controllers.chat_controller
  /chat/completions/{completion_id}/messages:
    get:
      operationId: get_chat_completion_messages
      parameters:
      - description: The ID of the chat completion to retrieve messages from.
        explode: false
        in: path
        name: completion_id
        required: true
        schema:
          type: string
        style: simple
      - description: Identifier for the last message from the previous pagination
          request.
        explode: true
        in: query
        name: after
        required: false
        schema:
          type: string
        style: form
      - description: Number of messages to retrieve.
        explode: true
        in: query
        name: limit
        required: false
        schema:
          default: 20
          type: integer
        style: form
      - description: Sort order for messages by timestamp. Use `asc` for ascending
          order or `desc` for descending order. Defaults to `asc`.
        explode: true
        in: query
        name: order
        required: false
        schema:
          default: asc
          enum:
          - asc
          - desc
          type: string
        style: form
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ChatCompletionMessageList"
          description: A list of messages
      summary: |
        Get the messages in a stored chat completion. Only Chat Completions that
        have been created with the `store` parameter set to `true` will be
        returned.
      tags:
      - Chat
      x-oaiMeta:
        name: Get chat messages
        group: chat
        returns: "A list of [messages](/docs/api-reference/chat/message-list) for\
          \ the specified chat completion."
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/chat/completions/chat_abc123/messages \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json"
            python: |
              from openai import OpenAI
              client = OpenAI()

              completions = client.chat.completions.list()
              first_id = completions[0].id
              first_completion = client.chat.completions.retrieve(completion_id=first_id)
              messages = client.chat.completions.messages.list(completion_id=first_id)
              print(messages)
          response: |
            {
              "object": "list",
              "data": [
                {
                  "id": "chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2-0",
                  "role": "user",
                  "content": "write a haiku about ai",
                  "name": null,
                  "content_parts": null
                }
              ],
              "first_id": "chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2-0",
              "last_id": "chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2-0",
              "has_more": false
            }
      x-openapi-router-controller: openapi_server.controllers.chat_controller
  /completions:
    post:
      operationId: create_completion
      requestBody:
        content:
          application/json:
            schema:
              $ref: "#/components/schemas/CreateCompletionRequest"
        required: true
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/CreateCompletionResponse"
          description: OK
      summary: Creates a completion for the provided prompt and parameters.
      tags:
      - Completions
      x-oaiMeta:
        name: Create completion
        group: completions
        returns: |
          Returns a [completion](/docs/api-reference/completions/object) object, or a sequence of completion objects if the request is streamed.
        legacy: true
        examples:
        - title: No streaming
          request:
            curl: |
              curl https://api.openai.com/v1/completions \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -d '{
                  "model": "VAR_completion_model_id",
                  "prompt": "Say this is a test",
                  "max_tokens": 7,
                  "temperature": 0
                }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.completions.create(
                model="VAR_completion_model_id",
                prompt="Say this is a test",
                max_tokens=7,
                temperature=0
              )
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const completion = await openai.completions.create({
                  model: "VAR_completion_model_id",
                  prompt: "Say this is a test.",
                  max_tokens: 7,
                  temperature: 0,
                });

                console.log(completion);
              }
              main();
          response: |
            {
              "id": "cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7",
              "object": "text_completion",
              "created": 1589478378,
              "model": "VAR_completion_model_id",
              "system_fingerprint": "fp_44709d6fcb",
              "choices": [
                {
                  "text": "\n\nThis is indeed a test",
                  "index": 0,
                  "logprobs": null,
                  "finish_reason": "length"
                }
              ],
              "usage": {
                "prompt_tokens": 5,
                "completion_tokens": 7,
                "total_tokens": 12
              }
            }
        - title: Streaming
          request:
            curl: |
              curl https://api.openai.com/v1/completions \
                -H "Content-Type: application/json" \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -d '{
                  "model": "VAR_completion_model_id",
                  "prompt": "Say this is a test",
                  "max_tokens": 7,
                  "temperature": 0,
                  "stream": true
                }'
            python: |
              from openai import OpenAI
              client = OpenAI()

              for chunk in client.completions.create(
                model="VAR_completion_model_id",
                prompt="Say this is a test",
                max_tokens=7,
                temperature=0,
                stream=True
              ):
                print(chunk.choices[0].text)
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const stream = await openai.completions.create({
                  model: "VAR_completion_model_id",
                  prompt: "Say this is a test.",
                  stream: true,
                });

                for await (const chunk of stream) {
                  console.log(chunk.choices[0].text)
                }
              }
              main();
          response: |
            {
              "id": "cmpl-7iA7iJjj8V2zOkCGvWF2hAkDWBQZe",
              "object": "text_completion",
              "created": 1690759702,
              "choices": [
                {
                  "text": "This",
                  "index": 0,
                  "logprobs": null,
                  "finish_reason": null
                }
              ],
              "model": "gpt-3.5-turbo-instruct"
              "system_fingerprint": "fp_44709d6fcb",
            }
      x-openapi-router-controller: openapi_server.controllers.completions_controller
  /models:
    get:
      operationId: list_models
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/ListModelsResponse"
          description: OK
      summary: "Lists the currently available models, and provides basic information\
        \ about each one such as the owner and availability."
      tags:
      - Models
      x-oaiMeta:
        name: List models
        group: models
        returns: "A list of [model](/docs/api-reference/models/object) objects."
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/models \
                -H "Authorization: Bearer $OPENAI_API_KEY"
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.models.list()
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const list = await openai.models.list();

                for await (const model of list) {
                  console.log(model);
                }
              }
              main();
            csharp: |
              using System;

              using OpenAI.Models;

              OpenAIModelClient client = new(
                  apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
              );

              foreach (var model in client.GetModels().Value)
              {
                  Console.WriteLine(model.Id);
              }
          response: |
            {
              "object": "list",
              "data": [
                {
                  "id": "model-id-0",
                  "object": "model",
                  "created": 1686935002,
                  "owned_by": "organization-owner"
                },
                {
                  "id": "model-id-1",
                  "object": "model",
                  "created": 1686935002,
                  "owned_by": "organization-owner",
                },
                {
                  "id": "model-id-2",
                  "object": "model",
                  "created": 1686935002,
                  "owned_by": "openai"
                },
              ],
              "object": "list"
            }
      x-openapi-router-controller: openapi_server.controllers.models_controller
  /models/{model}:
    delete:
      operationId: delete_model
      parameters:
      - description: The model to delete
        explode: false
        in: path
        name: model
        required: true
        schema:
          example: ft:gpt-4o-mini:acemeco:suffix:abc123
          type: string
        style: simple
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/DeleteModelResponse"
          description: OK
      summary: Delete a fine-tuned model. You must have the Owner role in your organization
        to delete a model.
      tags:
      - Models
      x-oaiMeta:
        name: Delete a fine-tuned model
        group: models
        returns: Deletion status.
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/models/ft:gpt-4o-mini:acemeco:suffix:abc123 \
                -X DELETE \
                -H "Authorization: Bearer $OPENAI_API_KEY"
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.models.delete("ft:gpt-4o-mini:acemeco:suffix:abc123")
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const model = await openai.models.del("ft:gpt-4o-mini:acemeco:suffix:abc123");

                console.log(model);
              }
              main();
            csharp: |
              using System;
              using System.ClientModel;

              using OpenAI.Models;

              OpenAIModelClient client = new(
                  apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
              );

              ClientResult success = client.DeleteModel("ft:gpt-4o-mini:acemeco:suffix:abc123");
              Console.WriteLine(success);
          response: |
            {
              "id": "ft:gpt-4o-mini:acemeco:suffix:abc123",
              "object": "model",
              "deleted": true
            }
      x-openapi-router-controller: openapi_server.controllers.models_controller
    get:
      operationId: retrieve_model
      parameters:
      - description: The ID of the model to use for this request
        explode: false
        in: path
        name: model
        required: true
        schema:
          example: gpt-4o-mini
          type: string
        style: simple
      responses:
        "200":
          content:
            application/json:
              schema:
                $ref: "#/components/schemas/Model"
          description: OK
      summary: "Retrieves a model instance, providing basic information about the\
        \ model such as the owner and permissioning."
      tags:
      - Models
      x-oaiMeta:
        name: Retrieve model
        group: models
        returns: "The [model](/docs/api-reference/models/object) object matching the\
          \ specified ID."
        examples:
          request:
            curl: |
              curl https://api.openai.com/v1/models/VAR_chat_model_id \
                -H "Authorization: Bearer $OPENAI_API_KEY"
            python: |
              from openai import OpenAI
              client = OpenAI()

              client.models.retrieve("VAR_chat_model_id")
            node.js: |-
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                const model = await openai.models.retrieve("VAR_chat_model_id");

                console.log(model);
              }

              main();
            csharp: |
              using System;
              using System.ClientModel;

              using OpenAI.Models;

                OpenAIModelClient client = new(
                  apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
              );

              ClientResult<OpenAIModel> model = client.GetModel("babbage-002");
              Console.WriteLine(model.Value.Id);
          response: |
            {
              "id": "VAR_chat_model_id",
              "object": "model",
              "created": 1686935002,
              "owned_by": "openai"
            }
      x-openapi-router-controller: openapi_server.controllers.models_controller
components:
  schemas:
    ChatCompletionDeleted:
      example:
        deleted: true
        id: id
        object: chat.completion.deleted
      properties:
        object:
          description: The type of object being deleted.
          enum:
          - chat.completion.deleted
          title: object
          type: string
          x-stainless-const: true
        id:
          description: The ID of the chat completion that was deleted.
          title: id
          type: string
        deleted:
          description: Whether the chat completion was deleted.
          title: deleted
          type: boolean
      required:
      - deleted
      - id
      - object
      title: ChatCompletionDeleted
      type: object
    ChatCompletionFunctionCallOption:
      description: |
        Specifying a particular function via `{"name": "my_function"}` forces the model to call that function.
      properties:
        name:
          description: The name of the function to call.
          title: name
          type: string
      required:
      - name
      title: ChatCompletionFunctionCallOption
      type: object
    ChatCompletionFunctions:
      deprecated: true
      example:
        name: name
        description: description
        parameters:
          key: ""
      properties:
        description:
          description: "A description of what the function does, used by the model\
            \ to choose when and how to call the function."
          title: description
          type: string
        name:
          description: "The name of the function to be called. Must be a-z, A-Z, 0-9,\
            \ or contain underscores and dashes, with a maximum length of 64."
          title: name
          type: string
        parameters:
          additionalProperties: true
          description: "The parameters the functions accepts, described as a JSON\
            \ Schema object. See the [guide](/docs/guides/function-calling) for examples,\
            \ and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/)\
            \ for documentation about the format. \n\nOmitting `parameters` defines\
            \ a function with an empty parameter list."
          title: FunctionParameters
          type: object
      required:
      - name
      title: ChatCompletionFunctions
      type: object
    ChatCompletionList:
      description: |
        An object representing a list of Chat Completions.
      example:
        first_id: first_id
        data:
        - created: 3
          usage:
            completion_tokens: 2
            prompt_tokens: 4
            completion_tokens_details:
              accepted_prediction_tokens: 1
              audio_tokens: 1
              reasoning_tokens: 1
              rejected_prediction_tokens: 6
            prompt_tokens_details:
              audio_tokens: 7
              cached_tokens: 1
            total_tokens: 7
          model: model
          service_tier: auto
          id: id
          choices:
          - finish_reason: stop
            index: 0
            message:
              role: assistant
              function_call:
                name: name
                arguments: arguments
              refusal: refusal
              annotations:
              - type: url_citation
                url_citation:
                  start_index: 1
                  end_index: 6
                  title: title
                  url: url
              - type: url_citation
                url_citation:
                  start_index: 1
                  end_index: 6
                  title: title
                  url: url
              tool_calls:
              - function:
                  name: name
                  arguments: arguments
                id: id
                type: function
              - function:
                  name: name
                  arguments: arguments
                id: id
                type: function
              audio:
                expires_at: 5
                transcript: transcript
                data: data
                id: id
              content: content
            logprobs:
              refusal:
              - top_logprobs:
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                logprob: 5.637376656633329
                bytes:
                - 2
                - 2
                token: token
              - top_logprobs:
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                logprob: 5.637376656633329
                bytes:
                - 2
                - 2
                token: token
              content:
              - top_logprobs:
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                logprob: 5.637376656633329
                bytes:
                - 2
                - 2
                token: token
              - top_logprobs:
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                logprob: 5.637376656633329
                bytes:
                - 2
                - 2
                token: token
          - finish_reason: stop
            index: 0
            message:
              role: assistant
              function_call:
                name: name
                arguments: arguments
              refusal: refusal
              annotations:
              - type: url_citation
                url_citation:
                  start_index: 1
                  end_index: 6
                  title: title
                  url: url
              - type: url_citation
                url_citation:
                  start_index: 1
                  end_index: 6
                  title: title
                  url: url
              tool_calls:
              - function:
                  name: name
                  arguments: arguments
                id: id
                type: function
              - function:
                  name: name
                  arguments: arguments
                id: id
                type: function
              audio:
                expires_at: 5
                transcript: transcript
                data: data
                id: id
              content: content
            logprobs:
              refusal:
              - top_logprobs:
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                logprob: 5.637376656633329
                bytes:
                - 2
                - 2
                token: token
              - top_logprobs:
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                logprob: 5.637376656633329
                bytes:
                - 2
                - 2
                token: token
              content:
              - top_logprobs:
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                logprob: 5.637376656633329
                bytes:
                - 2
                - 2
                token: token
              - top_logprobs:
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                logprob: 5.637376656633329
                bytes:
                - 2
                - 2
                token: token
          system_fingerprint: system_fingerprint
          object: chat.completion
        - created: 3
          usage:
            completion_tokens: 2
            prompt_tokens: 4
            completion_tokens_details:
              accepted_prediction_tokens: 1
              audio_tokens: 1
              reasoning_tokens: 1
              rejected_prediction_tokens: 6
            prompt_tokens_details:
              audio_tokens: 7
              cached_tokens: 1
            total_tokens: 7
          model: model
          service_tier: auto
          id: id
          choices:
          - finish_reason: stop
            index: 0
            message:
              role: assistant
              function_call:
                name: name
                arguments: arguments
              refusal: refusal
              annotations:
              - type: url_citation
                url_citation:
                  start_index: 1
                  end_index: 6
                  title: title
                  url: url
              - type: url_citation
                url_citation:
                  start_index: 1
                  end_index: 6
                  title: title
                  url: url
              tool_calls:
              - function:
                  name: name
                  arguments: arguments
                id: id
                type: function
              - function:
                  name: name
                  arguments: arguments
                id: id
                type: function
              audio:
                expires_at: 5
                transcript: transcript
                data: data
                id: id
              content: content
            logprobs:
              refusal:
              - top_logprobs:
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                logprob: 5.637376656633329
                bytes:
                - 2
                - 2
                token: token
              - top_logprobs:
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                logprob: 5.637376656633329
                bytes:
                - 2
                - 2
                token: token
              content:
              - top_logprobs:
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                logprob: 5.637376656633329
                bytes:
                - 2
                - 2
                token: token
              - top_logprobs:
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                logprob: 5.637376656633329
                bytes:
                - 2
                - 2
                token: token
          - finish_reason: stop
            index: 0
            message:
              role: assistant
              function_call:
                name: name
                arguments: arguments
              refusal: refusal
              annotations:
              - type: url_citation
                url_citation:
                  start_index: 1
                  end_index: 6
                  title: title
                  url: url
              - type: url_citation
                url_citation:
                  start_index: 1
                  end_index: 6
                  title: title
                  url: url
              tool_calls:
              - function:
                  name: name
                  arguments: arguments
                id: id
                type: function
              - function:
                  name: name
                  arguments: arguments
                id: id
                type: function
              audio:
                expires_at: 5
                transcript: transcript
                data: data
                id: id
              content: content
            logprobs:
              refusal:
              - top_logprobs:
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                logprob: 5.637376656633329
                bytes:
                - 2
                - 2
                token: token
              - top_logprobs:
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                logprob: 5.637376656633329
                bytes:
                - 2
                - 2
                token: token
              content:
              - top_logprobs:
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                logprob: 5.637376656633329
                bytes:
                - 2
                - 2
                token: token
              - top_logprobs:
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                - logprob: 7.061401241503109
                  bytes:
                  - 9
                  - 9
                  token: token
                logprob: 5.637376656633329
                bytes:
                - 2
                - 2
                token: token
          system_fingerprint: system_fingerprint
          object: chat.completion
        last_id: last_id
        has_more: true
        object: list
      properties:
        object:
          default: list
          description: |
            The type of this object. It is always set to "list".
          enum:
          - list
          title: object
          type: string
          x-stainless-const: true
        data:
          description: |
            An array of chat completion objects.
          items:
            $ref: "#/components/schemas/CreateChatCompletionResponse"
          title: data
          type: array
        first_id:
          description: The identifier of the first chat completion in the data array.
          title: first_id
          type: string
        last_id:
          description: The identifier of the last chat completion in the data array.
          title: last_id
          type: string
        has_more:
          description: Indicates whether there are more Chat Completions available.
          title: has_more
          type: boolean
      required:
      - data
      - first_id
      - has_more
      - last_id
      - object
      title: ChatCompletionList
      type: object
      x-oaiMeta:
        name: The chat completion list object
        group: chat
        example: |
          {
            "object": "list",
            "data": [
              {
                "object": "chat.completion",
                "id": "chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2",
                "model": "gpt-4o-2024-08-06",
                "created": 1738960610,
                "request_id": "req_ded8ab984ec4bf840f37566c1011c417",
                "tool_choice": null,
                "usage": {
                  "total_tokens": 31,
                  "completion_tokens": 18,
                  "prompt_tokens": 13
                },
                "seed": 4944116822809979520,
                "top_p": 1.0,
                "temperature": 1.0,
                "presence_penalty": 0.0,
                "frequency_penalty": 0.0,
                "system_fingerprint": "fp_50cad350e4",
                "input_user": null,
                "service_tier": "default",
                "tools": null,
                "metadata": {},
                "choices": [
                  {
                    "index": 0,
                    "message": {
                      "content": "Mind of circuits hum,  \nLearning patterns in silence—  \nFuture's quiet spark.",
                      "role": "assistant",
                      "tool_calls": null,
                      "function_call": null
                    },
                    "finish_reason": "stop",
                    "logprobs": null
                  }
                ],
                "response_format": null
              }
            ],
            "first_id": "chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2",
            "last_id": "chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2",
            "has_more": false
          }
    ChatCompletionMessageList:
      description: |
        An object representing a list of chat completion messages.
      example:
        first_id: first_id
        data:
        - role: assistant
          function_call:
            name: name
            arguments: arguments
          refusal: refusal
          annotations:
          - type: url_citation
            url_citation:
              start_index: 1
              end_index: 6
              title: title
              url: url
          - type: url_citation
            url_citation:
              start_index: 1
              end_index: 6
              title: title
              url: url
          tool_calls:
          - function:
              name: name
              arguments: arguments
            id: id
            type: function
          - function:
              name: name
              arguments: arguments
            id: id
            type: function
          audio:
            expires_at: 5
            transcript: transcript
            data: data
            id: id
          id: id
          content: content
        - role: assistant
          function_call:
            name: name
            arguments: arguments
          refusal: refusal
          annotations:
          - type: url_citation
            url_citation:
              start_index: 1
              end_index: 6
              title: title
              url: url
          - type: url_citation
            url_citation:
              start_index: 1
              end_index: 6
              title: title
              url: url
          tool_calls:
          - function:
              name: name
              arguments: arguments
            id: id
            type: function
          - function:
              name: name
              arguments: arguments
            id: id
            type: function
          audio:
            expires_at: 5
            transcript: transcript
            data: data
            id: id
          id: id
          content: content
        last_id: last_id
        has_more: true
        object: list
      properties:
        object:
          default: list
          description: |
            The type of this object. It is always set to "list".
          enum:
          - list
          title: object
          type: string
          x-stainless-const: true
        data:
          description: |
            An array of chat completion message objects.
          items:
            $ref: "#/components/schemas/ChatCompletionMessageList_data_inner"
          title: data
          type: array
        first_id:
          description: The identifier of the first chat message in the data array.
          title: first_id
          type: string
        last_id:
          description: The identifier of the last chat message in the data array.
          title: last_id
          type: string
        has_more:
          description: Indicates whether there are more chat messages available.
          title: has_more
          type: boolean
      required:
      - data
      - first_id
      - has_more
      - last_id
      - object
      title: ChatCompletionMessageList
      type: object
      x-oaiMeta:
        name: The chat completion message list object
        group: chat
        example: |
          {
            "object": "list",
            "data": [
              {
                "id": "chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2-0",
                "role": "user",
                "content": "write a haiku about ai",
                "name": null,
                "content_parts": null
              }
            ],
            "first_id": "chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2-0",
            "last_id": "chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2-0",
            "has_more": false
          }
    ChatCompletionMessageToolCall:
      example:
        function:
          name: name
          arguments: arguments
        id: id
        type: function
      properties:
        id:
          description: The ID of the tool call.
          title: id
          type: string
        type:
          description: "The type of the tool. Currently, only `function` is supported."
          enum:
          - function
          title: type
          type: string
          x-stainless-const: true
        function:
          $ref: "#/components/schemas/ChatCompletionMessageToolCall_function"
      required:
      - function
      - id
      - type
      title: ChatCompletionMessageToolCall
      type: object
    ChatCompletionMessageToolCallChunk:
      properties:
        index:
          title: index
          type: integer
        id:
          description: The ID of the tool call.
          title: id
          type: string
        type:
          description: "The type of the tool. Currently, only `function` is supported."
          enum:
          - function
          title: type
          type: string
          x-stainless-const: true
        function:
          $ref: "#/components/schemas/ChatCompletionMessageToolCallChunk_function"
      required:
      - index
      title: ChatCompletionMessageToolCallChunk
      type: object
    ChatCompletionMessageToolCalls:
      description: "The tool calls generated by the model, such as function calls."
      items:
        $ref: "#/components/schemas/ChatCompletionMessageToolCall"
      title: ChatCompletionMessageToolCalls
      type: array
    ChatCompletionNamedToolChoice:
      description: Specifies a tool the model should use. Use to force the model to
        call a specific function.
      properties:
        type:
          description: "The type of the tool. Currently, only `function` is supported."
          enum:
          - function
          title: type
          type: string
          x-stainless-const: true
        function:
          $ref: "#/components/schemas/ChatCompletionNamedToolChoice_function"
      required:
      - function
      - type
      title: ChatCompletionNamedToolChoice
      type: object
    ChatCompletionRequestAssistantMessage:
      description: |
        Messages sent by the model in response to user messages.
      properties:
        content:
          $ref: "#/components/schemas/ChatCompletionRequestAssistantMessage_content"
        refusal:
          description: The refusal message by the assistant.
          nullable: true
          title: refusal
          type: string
        role:
          description: "The role of the messages author, in this case `assistant`."
          enum:
          - assistant
          title: role
          type: string
          x-stainless-const: true
        name:
          description: An optional name for the participant. Provides the model information
            to differentiate between participants of the same role.
          title: name
          type: string
        audio:
          $ref: "#/components/schemas/ChatCompletionRequestAssistantMessage_audio"
        tool_calls:
          description: "The tool calls generated by the model, such as function calls."
          items:
            $ref: "#/components/schemas/ChatCompletionMessageToolCall"
          title: ChatCompletionMessageToolCalls
          type: array
        function_call:
          $ref: "#/components/schemas/ChatCompletionRequestAssistantMessage_function_call"
      required:
      - role
      title: Assistant message
      type: object
    ChatCompletionRequestAssistantMessageContentPart:
      oneOf:
      - $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartText"
      - $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartRefusal"
      title: ChatCompletionRequestAssistantMessageContentPart
    ChatCompletionRequestDeveloperMessage:
      description: |
        Developer-provided instructions that the model should follow, regardless of
        messages sent by the user. With o1 models and newer, `developer` messages
        replace the previous `system` messages.
      example:
        role: developer
        name: name
        content: ChatCompletionRequestDeveloperMessage_content
      properties:
        content:
          $ref: "#/components/schemas/ChatCompletionRequestDeveloperMessage_content"
        role:
          description: "The role of the messages author, in this case `developer`."
          enum:
          - developer
          title: role
          type: string
          x-stainless-const: true
        name:
          description: An optional name for the participant. Provides the model information
            to differentiate between participants of the same role.
          title: name
          type: string
      required:
      - content
      - role
      title: Developer message
      type: object
    ChatCompletionRequestFunctionMessage:
      deprecated: true
      properties:
        role:
          description: "The role of the messages author, in this case `function`."
          enum:
          - function
          title: role
          type: string
          x-stainless-const: true
        content:
          description: The contents of the function message.
          nullable: true
          title: content
          type: string
        name:
          description: The name of the function to call.
          title: name
          type: string
      required:
      - content
      - name
      - role
      title: Function message
      type: object
    ChatCompletionRequestMessage:
      oneOf:
      - $ref: "#/components/schemas/ChatCompletionRequestDeveloperMessage"
      - $ref: "#/components/schemas/ChatCompletionRequestSystemMessage"
      - $ref: "#/components/schemas/ChatCompletionRequestUserMessage"
      - $ref: "#/components/schemas/ChatCompletionRequestAssistantMessage"
      - $ref: "#/components/schemas/ChatCompletionRequestToolMessage"
      - $ref: "#/components/schemas/ChatCompletionRequestFunctionMessage"
      title: ChatCompletionRequestMessage
    ChatCompletionRequestMessageContentPartAudio:
      description: |
        Learn about [audio inputs](/docs/guides/audio).
      properties:
        type:
          description: The type of the content part. Always `input_audio`.
          enum:
          - input_audio
          title: type
          type: string
          x-stainless-const: true
        input_audio:
          $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartAudio_input_audio"
      required:
      - input_audio
      - type
      title: Audio content part
      type: object
    ChatCompletionRequestMessageContentPartFile:
      description: |
        Learn about [file inputs](/docs/guides/text) for text generation.
      properties:
        type:
          description: The type of the content part. Always `file`.
          enum:
          - file
          title: type
          type: string
          x-stainless-const: true
        file:
          $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartFile_file"
      required:
      - file
      - type
      title: File content part
      type: object
    ChatCompletionRequestMessageContentPartImage:
      description: |
        Learn about [image inputs](/docs/guides/vision).
      properties:
        type:
          description: The type of the content part.
          enum:
          - image_url
          title: type
          type: string
          x-stainless-const: true
        image_url:
          $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartImage_image_url"
      required:
      - image_url
      - type
      title: Image content part
      type: object
    ChatCompletionRequestMessageContentPartRefusal:
      properties:
        type:
          description: The type of the content part.
          enum:
          - refusal
          title: type
          type: string
          x-stainless-const: true
        refusal:
          description: The refusal message generated by the model.
          title: refusal
          type: string
      required:
      - refusal
      - type
      title: Refusal content part
      type: object
    ChatCompletionRequestMessageContentPartText:
      description: |
        Learn about [text inputs](/docs/guides/text-generation).
      properties:
        type:
          description: The type of the content part.
          enum:
          - text
          title: type
          type: string
          x-stainless-const: true
        text:
          description: The text content.
          title: text
          type: string
      required:
      - text
      - type
      title: Text content part
      type: object
    ChatCompletionRequestSystemMessage:
      description: |
        Developer-provided instructions that the model should follow, regardless of
        messages sent by the user. With o1 models and newer, use `developer` messages
        for this purpose instead.
      properties:
        content:
          $ref: "#/components/schemas/ChatCompletionRequestSystemMessage_content"
        role:
          description: "The role of the messages author, in this case `system`."
          enum:
          - system
          title: role
          type: string
          x-stainless-const: true
        name:
          description: An optional name for the participant. Provides the model information
            to differentiate between participants of the same role.
          title: name
          type: string
      required:
      - content
      - role
      title: System message
      type: object
    ChatCompletionRequestSystemMessageContentPart:
      $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartText"
    ChatCompletionRequestToolMessage:
      properties:
        role:
          description: "The role of the messages author, in this case `tool`."
          enum:
          - tool
          title: role
          type: string
          x-stainless-const: true
        content:
          $ref: "#/components/schemas/ChatCompletionRequestToolMessage_content"
        tool_call_id:
          description: Tool call that this message is responding to.
          title: tool_call_id
          type: string
      required:
      - content
      - role
      - tool_call_id
      title: Tool message
      type: object
    ChatCompletionRequestToolMessageContentPart:
      $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartText"
    ChatCompletionRequestUserMessage:
      description: |
        Messages sent by an end user, containing prompts or additional context
        information.
      properties:
        content:
          $ref: "#/components/schemas/ChatCompletionRequestUserMessage_content"
        role:
          description: "The role of the messages author, in this case `user`."
          enum:
          - user
          title: role
          type: string
          x-stainless-const: true
        name:
          description: An optional name for the participant. Provides the model information
            to differentiate between participants of the same role.
          title: name
          type: string
      required:
      - content
      - role
      title: User message
      type: object
    ChatCompletionRequestUserMessageContentPart:
      oneOf:
      - $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartText"
      - $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartImage"
      - $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartAudio"
      - $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartFile"
      title: ChatCompletionRequestUserMessageContentPart
    ChatCompletionResponseMessage:
      description: A chat completion message generated by the model.
      example:
        role: assistant
        function_call:
          name: name
          arguments: arguments
        refusal: refusal
        annotations:
        - type: url_citation
          url_citation:
            start_index: 1
            end_index: 6
            title: title
            url: url
        - type: url_citation
          url_citation:
            start_index: 1
            end_index: 6
            title: title
            url: url
        tool_calls:
        - function:
            name: name
            arguments: arguments
          id: id
          type: function
        - function:
            name: name
            arguments: arguments
          id: id
          type: function
        audio:
          expires_at: 5
          transcript: transcript
          data: data
          id: id
        content: content
      properties:
        content:
          description: The contents of the message.
          nullable: true
          title: content
          type: string
        refusal:
          description: The refusal message generated by the model.
          nullable: true
          title: refusal
          type: string
        tool_calls:
          description: "The tool calls generated by the model, such as function calls."
          items:
            $ref: "#/components/schemas/ChatCompletionMessageToolCall"
          title: ChatCompletionMessageToolCalls
          type: array
        annotations:
          description: |
            Annotations for the message, when applicable, as when using the
            [web search tool](/docs/guides/tools-web-search?api-mode=chat).
          items:
            $ref: "#/components/schemas/ChatCompletionResponseMessage_annotations_inner"
          title: annotations
          type: array
        role:
          description: The role of the author of this message.
          enum:
          - assistant
          title: role
          type: string
          x-stainless-const: true
        function_call:
          $ref: "#/components/schemas/ChatCompletionResponseMessage_function_call"
        audio:
          $ref: "#/components/schemas/ChatCompletionResponseMessage_audio"
      required:
      - content
      - refusal
      - role
      title: ChatCompletionResponseMessage
      type: object
    ChatCompletionStreamOptions:
      description: |
        Options for streaming response. Only set this when you set `stream: true`.
      example:
        include_usage: true
      nullable: true
      properties:
        include_usage:
          description: "If set, an additional chunk will be streamed before the `data:\
            \ [DONE]`\nmessage. The `usage` field on this chunk shows the token usage\
            \ statistics\nfor the entire request, and the `choices` field will always\
            \ be an empty\narray. \n\nAll other chunks will also include a `usage`\
            \ field, but with a null\nvalue. **NOTE:** If the stream is interrupted,\
            \ you may not receive the\nfinal usage chunk which contains the total\
            \ token usage for the request.\n"
          title: include_usage
          type: boolean
      title: ChatCompletionStreamOptions
      type: object
    ChatCompletionStreamResponseDelta:
      description: A chat completion delta generated by streamed model responses.
      properties:
        content:
          description: The contents of the chunk message.
          nullable: true
          title: content
          type: string
        function_call:
          $ref: "#/components/schemas/ChatCompletionStreamResponseDelta_function_call"
        tool_calls:
          items:
            $ref: "#/components/schemas/ChatCompletionMessageToolCallChunk"
          title: tool_calls
          type: array
        role:
          description: The role of the author of this message.
          enum:
          - developer
          - system
          - user
          - assistant
          - tool
          title: role
          type: string
        refusal:
          description: The refusal message generated by the model.
          nullable: true
          title: refusal
          type: string
      title: ChatCompletionStreamResponseDelta
      type: object
    ChatCompletionTokenLogprob:
      example:
        top_logprobs:
        - logprob: 7.061401241503109
          bytes:
          - 9
          - 9
          token: token
        - logprob: 7.061401241503109
          bytes:
          - 9
          - 9
          token: token
        logprob: 5.637376656633329
        bytes:
        - 2
        - 2
        token: token
      properties:
        token:
          description: The token.
          title: token
          type: string
        logprob:
          description: "The log probability of this token, if it is within the top\
            \ 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify\
            \ that the token is very unlikely."
          title: logprob
          type: number
        bytes:
          description: A list of integers representing the UTF-8 bytes representation
            of the token. Useful in instances where characters are represented by
            multiple tokens and their byte representations must be combined to generate
            the correct text representation. Can be `null` if there is no bytes representation
            for the token.
          items:
            type: integer
          nullable: true
          title: bytes
          type: array
        top_logprobs:
          description: "List of the most likely tokens and their log probability,\
            \ at this token position. In rare cases, there may be fewer than the number\
            \ of requested `top_logprobs` returned."
          items:
            $ref: "#/components/schemas/ChatCompletionTokenLogprob_top_logprobs_inner"
          title: top_logprobs
          type: array
      required:
      - bytes
      - logprob
      - token
      - top_logprobs
      title: ChatCompletionTokenLogprob
      type: object
    ChatCompletionTool:
      example:
        function:
          name: name
          description: description
          strict: false
          parameters:
            key: ""
        type: function
      properties:
        type:
          description: "The type of the tool. Currently, only `function` is supported."
          enum:
          - function
          title: type
          type: string
          x-stainless-const: true
        function:
          $ref: "#/components/schemas/FunctionObject"
      required:
      - function
      - type
      title: ChatCompletionTool
      type: object
    ChatCompletionToolChoiceOption:
      description: |
        Controls which (if any) tool is called by the model.
        `none` means the model will not call any tool and instead generates a message.
        `auto` means the model can pick between generating a message or calling one or more tools.
        `required` means the model must call one or more tools.
        Specifying a particular tool via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that tool.

        `none` is the default when no tools are present. `auto` is the default if tools are present.
      oneOf:
      - description: |
          `none` means the model will not call any tool and instead generates a message. `auto` means the model can pick between generating a message or calling one or more tools. `required` means the model must call one or more tools.
        enum:
        - none
        - auto
        - required
        type: string
      - $ref: "#/components/schemas/ChatCompletionNamedToolChoice"
      title: ChatCompletionToolChoiceOption
    CompletionUsage:
      description: Usage statistics for the completion request.
      example:
        completion_tokens: 2
        prompt_tokens: 4
        completion_tokens_details:
          accepted_prediction_tokens: 1
          audio_tokens: 1
          reasoning_tokens: 1
          rejected_prediction_tokens: 6
        prompt_tokens_details:
          audio_tokens: 7
          cached_tokens: 1
        total_tokens: 7
      properties:
        completion_tokens:
          default: 0
          description: Number of tokens in the generated completion.
          title: completion_tokens
          type: integer
        prompt_tokens:
          default: 0
          description: Number of tokens in the prompt.
          title: prompt_tokens
          type: integer
        total_tokens:
          default: 0
          description: Total number of tokens used in the request (prompt + completion).
          title: total_tokens
          type: integer
        completion_tokens_details:
          $ref: "#/components/schemas/CompletionUsage_completion_tokens_details"
        prompt_tokens_details:
          $ref: "#/components/schemas/CompletionUsage_prompt_tokens_details"
      required:
      - completion_tokens
      - prompt_tokens
      - total_tokens
      title: CompletionUsage
      type: object
    CreateChatCompletionRequest:
      allOf:
      - $ref: "#/components/schemas/CreateModelResponseProperties"
      - properties:
          messages:
            description: |
              A list of messages comprising the conversation so far. Depending on the
              [model](/docs/models) you use, different message types (modalities) are
              supported, like [text](/docs/guides/text-generation),
              [images](/docs/guides/vision), and [audio](/docs/guides/audio).
            items:
              $ref: "#/components/schemas/ChatCompletionRequestMessage"
            minItems: 1
            type: array
          model:
            $ref: "#/components/schemas/ModelIdsShared"
          modalities:
            $ref: "#/components/schemas/ResponseModalities"
          reasoning_effort:
            $ref: "#/components/schemas/ReasoningEffort"
          max_completion_tokens:
            description: |
              An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).
            nullable: true
            type: integer
          frequency_penalty:
            default: 0
            description: |
              Number between -2.0 and 2.0. Positive values penalize new tokens based on
              their existing frequency in the text so far, decreasing the model's
              likelihood to repeat the same line verbatim.
            maximum: 2
            minimum: -2
            nullable: true
            type: number
          presence_penalty:
            default: 0
            description: |
              Number between -2.0 and 2.0. Positive values penalize new tokens based on
              whether they appear in the text so far, increasing the model's likelihood
              to talk about new topics.
            maximum: 2
            minimum: -2
            nullable: true
            type: number
          web_search_options:
            $ref: "#/components/schemas/Web_search"
          top_logprobs:
            description: |
              An integer between 0 and 20 specifying the number of most likely tokens to
              return at each token position, each with an associated log probability.
              `logprobs` must be set to `true` if this parameter is used.
            maximum: 20
            minimum: 0
            nullable: true
            type: integer
          response_format:
            $ref: "#/components/schemas/CreateChatCompletionRequest_allOf_response_format"
          audio:
            $ref: "#/components/schemas/CreateChatCompletionRequest_allOf_audio"
          store:
            default: false
            description: "Whether or not to store the output of this chat completion\
              \ request for \nuse in our [model distillation](/docs/guides/distillation)\
              \ or\n[evals](/docs/guides/evals) products.\n"
            nullable: true
            type: boolean
          stream:
            default: false
            description: |
              If set to true, the model response data will be streamed to the client
              as it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
              See the [Streaming section below](/docs/api-reference/chat/streaming)
              for more information, along with the [streaming responses](/docs/guides/streaming-responses)
              guide for more information on how to handle the streaming events.
            nullable: true
            type: boolean
          stop:
            $ref: "#/components/schemas/StopConfiguration"
          logit_bias:
            additionalProperties:
              type: integer
            description: |
              Modify the likelihood of specified tokens appearing in the completion.

              Accepts a JSON object that maps tokens (specified by their token ID in the
              tokenizer) to an associated bias value from -100 to 100. Mathematically,
              the bias is added to the logits generated by the model prior to sampling.
              The exact effect will vary per model, but values between -1 and 1 should
              decrease or increase likelihood of selection; values like -100 or 100
              should result in a ban or exclusive selection of the relevant token.
            nullable: true
            type: object
            x-oaiTypeLabel: map
          logprobs:
            default: false
            description: |
              Whether to return log probabilities of the output tokens or not. If true,
              returns the log probabilities of each output token returned in the
              `content` of `message`.
            nullable: true
            type: boolean
          max_tokens:
            deprecated: true
            description: |
              The maximum number of [tokens](/tokenizer) that can be generated in the
              chat completion. This value can be used to control
              [costs](https://openai.com/api/pricing/) for text generated via API.

              This value is now deprecated in favor of `max_completion_tokens`, and is
              not compatible with [o-series models](/docs/guides/reasoning).
            nullable: true
            type: integer
          "n":
            default: 1
            description: How many chat completion choices to generate for each input
              message. Note that you will be charged based on the number of generated
              tokens across all of the choices. Keep `n` as `1` to minimize costs.
            example: 1
            maximum: 128
            minimum: 1
            nullable: true
            type: integer
          prediction:
            $ref: "#/components/schemas/PredictionContent"
          seed:
            description: |
              This feature is in Beta.
              If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.
              Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
            maximum: 9223372036854776000
            minimum: -9223372036854776000
            nullable: true
            type: integer
            x-oaiMeta:
              beta: true
          stream_options:
            $ref: "#/components/schemas/ChatCompletionStreamOptions"
          tools:
            description: |
              A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.
            items:
              $ref: "#/components/schemas/ChatCompletionTool"
            type: array
          tool_choice:
            $ref: "#/components/schemas/ChatCompletionToolChoiceOption"
          parallel_tool_calls:
            $ref: "#/components/schemas/ParallelToolCalls"
          function_call:
            $ref: "#/components/schemas/CreateChatCompletionRequest_allOf_function_call"
          functions:
            deprecated: true
            description: |
              Deprecated in favor of `tools`.

              A list of functions the model may generate JSON inputs for.
            items:
              $ref: "#/components/schemas/ChatCompletionFunctions"
            maxItems: 128
            minItems: 1
            type: array
        required:
        - messages
        - model
        type: object
      example:
        reasoning_effort: medium
        top_logprobs: 11
        metadata:
          key: metadata
        logit_bias:
          key: 5
        seed: 2147483647
        functions:
        - name: name
          description: description
          parameters:
            key: ""
        - name: name
          description: description
          parameters:
            key: ""
        - name: name
          description: description
          parameters:
            key: ""
        - name: name
          description: description
          parameters:
            key: ""
        - name: name
          description: description
          parameters:
            key: ""
        function_call: none
        presence_penalty: -1.413674807798822
        tools:
        - function:
            name: name
            description: description
            strict: false
            parameters:
              key: ""
          type: function
        - function:
            name: name
            description: description
            strict: false
            parameters:
              key: ""
          type: function
        web_search_options:
          search_context_size: medium
          user_location:
            approximate:
              country: country
              city: city
              timezone: timezone
              region: region
            type: approximate
        logprobs: false
        top_p: 1
        max_completion_tokens: 0
        modalities:
        - text
        - text
        frequency_penalty: 0.4109824732281613
        response_format:
          type: text
        stream: false
        temperature: 1
        tool_choice: none
        service_tier: auto
        model: gpt-4o
        audio:
          voice: ash
          format: wav
        max_tokens: 2
        store: false
        "n": 1
        stop: |2+

        parallel_tool_calls: null
        prediction:
          type: content
          content: PredictionContent_content
        messages:
        - role: developer
          name: name
          content: ChatCompletionRequestDeveloperMessage_content
        - role: developer
          name: name
          content: ChatCompletionRequestDeveloperMessage_content
        stream_options:
          include_usage: true
        user: user-1234
      title: CreateChatCompletionRequest
    CreateChatCompletionResponse:
      description: "Represents a chat completion response returned by model, based\
        \ on the provided input."
      example:
        created: 3
        usage:
          completion_tokens: 2
          prompt_tokens: 4
          completion_tokens_details:
            accepted_prediction_tokens: 1
            audio_tokens: 1
            reasoning_tokens: 1
            rejected_prediction_tokens: 6
          prompt_tokens_details:
            audio_tokens: 7
            cached_tokens: 1
          total_tokens: 7
        model: model
        service_tier: auto
        id: id
        choices:
        - finish_reason: stop
          index: 0
          message:
            role: assistant
            function_call:
              name: name
              arguments: arguments
            refusal: refusal
            annotations:
            - type: url_citation
              url_citation:
                start_index: 1
                end_index: 6
                title: title
                url: url
            - type: url_citation
              url_citation:
                start_index: 1
                end_index: 6
                title: title
                url: url
            tool_calls:
            - function:
                name: name
                arguments: arguments
              id: id
              type: function
            - function:
                name: name
                arguments: arguments
              id: id
              type: function
            audio:
              expires_at: 5
              transcript: transcript
              data: data
              id: id
            content: content
          logprobs:
            refusal:
            - top_logprobs:
              - logprob: 7.061401241503109
                bytes:
                - 9
                - 9
                token: token
              - logprob: 7.061401241503109
                bytes:
                - 9
                - 9
                token: token
              logprob: 5.637376656633329
              bytes:
              - 2
              - 2
              token: token
            - top_logprobs:
              - logprob: 7.061401241503109
                bytes:
                - 9
                - 9
                token: token
              - logprob: 7.061401241503109
                bytes:
                - 9
                - 9
                token: token
              logprob: 5.637376656633329
              bytes:
              - 2
              - 2
              token: token
            content:
            - top_logprobs:
              - logprob: 7.061401241503109
                bytes:
                - 9
                - 9
                token: token
              - logprob: 7.061401241503109
                bytes:
                - 9
                - 9
                token: token
              logprob: 5.637376656633329
              bytes:
              - 2
              - 2
              token: token
            - top_logprobs:
              - logprob: 7.061401241503109
                bytes:
                - 9
                - 9
                token: token
              - logprob: 7.061401241503109
                bytes:
                - 9
                - 9
                token: token
              logprob: 5.637376656633329
              bytes:
              - 2
              - 2
              token: token
        - finish_reason: stop
          index: 0
          message:
            role: assistant
            function_call:
              name: name
              arguments: arguments
            refusal: refusal
            annotations:
            - type: url_citation
              url_citation:
                start_index: 1
                end_index: 6
                title: title
                url: url
            - type: url_citation
              url_citation:
                start_index: 1
                end_index: 6
                title: title
                url: url
            tool_calls:
            - function:
                name: name
                arguments: arguments
              id: id
              type: function
            - function:
                name: name
                arguments: arguments
              id: id
              type: function
            audio:
              expires_at: 5
              transcript: transcript
              data: data
              id: id
            content: content
          logprobs:
            refusal:
            - top_logprobs:
              - logprob: 7.061401241503109
                bytes:
                - 9
                - 9
                token: token
              - logprob: 7.061401241503109
                bytes:
                - 9
                - 9
                token: token
              logprob: 5.637376656633329
              bytes:
              - 2
              - 2
              token: token
            - top_logprobs:
              - logprob: 7.061401241503109
                bytes:
                - 9
                - 9
                token: token
              - logprob: 7.061401241503109
                bytes:
                - 9
                - 9
                token: token
              logprob: 5.637376656633329
              bytes:
              - 2
              - 2
              token: token
            content:
            - top_logprobs:
              - logprob: 7.061401241503109
                bytes:
                - 9
                - 9
                token: token
              - logprob: 7.061401241503109
                bytes:
                - 9
                - 9
                token: token
              logprob: 5.637376656633329
              bytes:
              - 2
              - 2
              token: token
            - top_logprobs:
              - logprob: 7.061401241503109
                bytes:
                - 9
                - 9
                token: token
              - logprob: 7.061401241503109
                bytes:
                - 9
                - 9
                token: token
              logprob: 5.637376656633329
              bytes:
              - 2
              - 2
              token: token
        system_fingerprint: system_fingerprint
        object: chat.completion
      properties:
        id:
          description: A unique identifier for the chat completion.
          title: id
          type: string
        choices:
          description: A list of chat completion choices. Can be more than one if
            `n` is greater than 1.
          items:
            $ref: "#/components/schemas/CreateChatCompletionResponse_choices_inner"
          title: choices
          type: array
        created:
          description: The Unix timestamp (in seconds) of when the chat completion
            was created.
          title: created
          type: integer
        model:
          description: The model used for the chat completion.
          title: model
          type: string
        service_tier:
          $ref: "#/components/schemas/ServiceTier"
        system_fingerprint:
          description: |
            This fingerprint represents the backend configuration that the model runs with.

            Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
          title: system_fingerprint
          type: string
        object:
          description: "The object type, which is always `chat.completion`."
          enum:
          - chat.completion
          title: object
          type: string
          x-stainless-const: true
        usage:
          $ref: "#/components/schemas/CompletionUsage"
      required:
      - choices
      - created
      - id
      - model
      - object
      title: CreateChatCompletionResponse
      type: object
      x-oaiMeta:
        name: The chat completion object
        group: chat
        example: |
          {
            "id": "chatcmpl-B9MHDbslfkBeAs8l4bebGdFOJ6PeG",
            "object": "chat.completion",
            "created": 1741570283,
            "model": "gpt-4o-2024-08-06",
            "choices": [
              {
                "index": 0,
                "message": {
                  "role": "assistant",
                  "content": "The image shows a wooden boardwalk path running through a lush green field or meadow. The sky is bright blue with some scattered clouds, giving the scene a serene and peaceful atmosphere. Trees and shrubs are visible in the background.",
                  "refusal": null,
                  "annotations": []
                },
                "logprobs": null,
                "finish_reason": "stop"
              }
            ],
            "usage": {
              "prompt_tokens": 1117,
              "completion_tokens": 46,
              "total_tokens": 1163,
              "prompt_tokens_details": {
                "cached_tokens": 0,
                "audio_tokens": 0
              },
              "completion_tokens_details": {
                "reasoning_tokens": 0,
                "audio_tokens": 0,
                "accepted_prediction_tokens": 0,
                "rejected_prediction_tokens": 0
              }
            },
            "service_tier": "default",
            "system_fingerprint": "fp_fc9f1d7035"
          }
    CreateChatCompletionStreamResponse:
      description: "Represents a streamed chunk of a chat completion response returned\n\
        by the model, based on the provided input. \n[Learn more](/docs/guides/streaming-responses).\n"
      properties:
        id:
          description: A unique identifier for the chat completion. Each chunk has
            the same ID.
          title: id
          type: string
        choices:
          description: |
            A list of chat completion choices. Can contain more than one elements if `n` is greater than 1. Can also be empty for the
            last chunk if you set `stream_options: {"include_usage": true}`.
          items:
            $ref: "#/components/schemas/CreateChatCompletionStreamResponse_choices_inner"
          title: choices
          type: array
        created:
          description: The Unix timestamp (in seconds) of when the chat completion
            was created. Each chunk has the same timestamp.
          title: created
          type: integer
        model:
          description: The model to generate the completion.
          title: model
          type: string
        service_tier:
          $ref: "#/components/schemas/ServiceTier"
        system_fingerprint:
          description: |
            This fingerprint represents the backend configuration that the model runs with.
            Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
          title: system_fingerprint
          type: string
        object:
          description: "The object type, which is always `chat.completion.chunk`."
          enum:
          - chat.completion.chunk
          title: object
          type: string
          x-stainless-const: true
        usage:
          $ref: "#/components/schemas/CompletionUsage"
      required:
      - choices
      - created
      - id
      - model
      - object
      title: CreateChatCompletionStreamResponse
      type: object
      x-oaiMeta:
        name: The chat completion chunk object
        group: chat
        example: |
          {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}

          {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"content":"Hello"},"logprobs":null,"finish_reason":null}]}

          ....

          {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{},"logprobs":null,"finish_reason":"stop"}]}
    CreateCompletionRequest:
      example:
        logit_bias:
          key: 1
        seed: 2
        max_tokens: 16
        presence_penalty: 0.25495066265333133
        echo: false
        suffix: test.
        "n": 1
        logprobs: 2
        top_p: 1
        frequency_penalty: 0.4109824732281613
        best_of: 1
        stop: |2+

        stream: false
        temperature: 1
        model: CreateCompletionRequest_model
        stream_options:
          include_usage: true
        prompt: This is a test.
        user: user-1234
      properties:
        model:
          $ref: "#/components/schemas/CreateCompletionRequest_model"
        prompt:
          $ref: "#/components/schemas/CreateCompletionRequest_prompt"
        best_of:
          default: 1
          description: |
            Generates `best_of` completions server-side and returns the "best" (the one with the highest log probability per token). Results cannot be streamed.

            When used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return – `best_of` must be greater than `n`.

            **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
          maximum: 20
          minimum: 0
          nullable: true
          title: best_of
          type: integer
        echo:
          default: false
          description: |
            Echo back the prompt in addition to the completion
          nullable: true
          title: echo
          type: boolean
        frequency_penalty:
          default: 0
          description: |
            Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.

            [See more information about frequency and presence penalties.](/docs/guides/text-generation)
          maximum: 2
          minimum: -2
          nullable: true
          title: frequency_penalty
          type: number
        logit_bias:
          additionalProperties:
            type: integer
          description: |
            Modify the likelihood of specified tokens appearing in the completion.

            Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

            As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token from being generated.
          nullable: true
          title: logit_bias
          type: object
          x-oaiTypeLabel: map
        logprobs:
          description: |
            Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.

            The maximum value for `logprobs` is 5.
          maximum: 5
          minimum: 0
          nullable: true
          title: logprobs
          type: integer
        max_tokens:
          default: 16
          description: |
            The maximum number of [tokens](/tokenizer) that can be generated in the completion.

            The token count of your prompt plus `max_tokens` cannot exceed the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.
          example: 16
          minimum: 0
          nullable: true
          title: max_tokens
          type: integer
        "n":
          default: 1
          description: |
            How many completions to generate for each prompt.

            **Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.
          example: 1
          maximum: 128
          minimum: 1
          nullable: true
          title: "n"
          type: integer
        presence_penalty:
          default: 0
          description: |
            Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.

            [See more information about frequency and presence penalties.](/docs/guides/text-generation)
          maximum: 2
          minimum: -2
          nullable: true
          title: presence_penalty
          type: number
        seed:
          description: |
            If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.

            Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.
          format: int64
          nullable: true
          title: seed
          type: integer
        stop:
          $ref: "#/components/schemas/StopConfiguration"
        stream:
          default: false
          description: |
            Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).
          nullable: true
          title: stream
          type: boolean
        stream_options:
          $ref: "#/components/schemas/ChatCompletionStreamOptions"
        suffix:
          description: |
            The suffix that comes after a completion of inserted text.

            This parameter is only supported for `gpt-3.5-turbo-instruct`.
          example: test.
          nullable: true
          title: suffix
          type: string
        temperature:
          default: 1
          description: |
            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.

            We generally recommend altering this or `top_p` but not both.
          example: 1
          maximum: 2
          minimum: 0
          nullable: true
          title: temperature
          type: number
        top_p:
          default: 1
          description: |
            An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.

            We generally recommend altering this or `temperature` but not both.
          example: 1
          maximum: 1
          minimum: 0
          nullable: true
          title: top_p
          type: number
        user:
          description: |
            A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).
          example: user-1234
          title: user
          type: string
      required:
      - model
      - prompt
      title: CreateCompletionRequest
      type: object
    CreateCompletionResponse:
      description: |
        Represents a completion response from the API. Note: both the streamed and non-streamed response objects share the same shape (unlike the chat endpoint).
      example:
        created: 5
        usage:
          completion_tokens: 2
          prompt_tokens: 4
          completion_tokens_details:
            accepted_prediction_tokens: 1
            audio_tokens: 1
            reasoning_tokens: 1
            rejected_prediction_tokens: 6
          prompt_tokens_details:
            audio_tokens: 7
            cached_tokens: 1
          total_tokens: 7
        model: model
        id: id
        choices:
        - finish_reason: stop
          index: 0
          text: text
          logprobs:
            top_logprobs:
            - key: 5.962133916683182
            - key: 5.962133916683182
            token_logprobs:
            - 1.4658129805029452
            - 1.4658129805029452
            tokens:
            - tokens
            - tokens
            text_offset:
            - 6
            - 6
        - finish_reason: stop
          index: 0
          text: text
          logprobs:
            top_logprobs:
            - key: 5.962133916683182
            - key: 5.962133916683182
            token_logprobs:
            - 1.4658129805029452
            - 1.4658129805029452
            tokens:
            - tokens
            - tokens
            text_offset:
            - 6
            - 6
        system_fingerprint: system_fingerprint
        object: text_completion
      properties:
        id:
          description: A unique identifier for the completion.
          title: id
          type: string
        choices:
          description: The list of completion choices the model generated for the
            input prompt.
          items:
            $ref: "#/components/schemas/CreateCompletionResponse_choices_inner"
          title: choices
          type: array
        created:
          description: The Unix timestamp (in seconds) of when the completion was
            created.
          title: created
          type: integer
        model:
          description: The model used for completion.
          title: model
          type: string
        system_fingerprint:
          description: |
            This fingerprint represents the backend configuration that the model runs with.

            Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
          title: system_fingerprint
          type: string
        object:
          description: "The object type, which is always \"text_completion\""
          enum:
          - text_completion
          title: object
          type: string
          x-stainless-const: true
        usage:
          $ref: "#/components/schemas/CompletionUsage"
      required:
      - choices
      - created
      - id
      - model
      - object
      title: CreateCompletionResponse
      type: object
      x-oaiMeta:
        name: The completion object
        legacy: true
        example: |
          {
            "id": "cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7",
            "object": "text_completion",
            "created": 1589478378,
            "model": "gpt-4-turbo",
            "choices": [
              {
                "text": "\n\nThis is indeed a test",
                "index": 0,
                "logprobs": null,
                "finish_reason": "length"
              }
            ],
            "usage": {
              "prompt_tokens": 5,
              "completion_tokens": 7,
              "total_tokens": 12
            }
          }
    CreateModelResponseProperties:
      allOf:
      - $ref: "#/components/schemas/ModelResponseProperties"
      title: CreateModelResponseProperties
    DeleteModelResponse:
      example:
        deleted: true
        id: id
        object: object
      properties:
        id:
          title: id
          type: string
        deleted:
          title: deleted
          type: boolean
        object:
          title: object
          type: string
      required:
      - deleted
      - id
      - object
      title: DeleteModelResponse
      type: object
    FunctionObject:
      example:
        name: name
        description: description
        strict: false
        parameters:
          key: ""
      properties:
        description:
          description: "A description of what the function does, used by the model\
            \ to choose when and how to call the function."
          title: description
          type: string
        name:
          description: "The name of the function to be called. Must be a-z, A-Z, 0-9,\
            \ or contain underscores and dashes, with a maximum length of 64."
          title: name
          type: string
        parameters:
          additionalProperties: true
          description: "The parameters the functions accepts, described as a JSON\
            \ Schema object. See the [guide](/docs/guides/function-calling) for examples,\
            \ and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/)\
            \ for documentation about the format. \n\nOmitting `parameters` defines\
            \ a function with an empty parameter list."
          title: FunctionParameters
          type: object
        strict:
          default: false
          description: "Whether to enable strict schema adherence when generating\
            \ the function call. If set to true, the model will follow the exact schema\
            \ defined in the `parameters` field. Only a subset of JSON Schema is supported\
            \ when `strict` is `true`. Learn more about Structured Outputs in the\
            \ [function calling guide](docs/guides/function-calling)."
          nullable: true
          title: strict
          type: boolean
      required:
      - name
      title: FunctionObject
      type: object
    FunctionParameters:
      additionalProperties: true
      description: "The parameters the functions accepts, described as a JSON Schema\
        \ object. See the [guide](/docs/guides/function-calling) for examples, and\
        \ the [JSON Schema reference](https://json-schema.org/understanding-json-schema/)\
        \ for documentation about the format. \n\nOmitting `parameters` defines a\
        \ function with an empty parameter list."
      title: FunctionParameters
      type: object
    ListModelsResponse:
      example:
        data:
        - created: 0
          owned_by: owned_by
          id: id
          object: model
        - created: 0
          owned_by: owned_by
          id: id
          object: model
        object: list
      properties:
        object:
          enum:
          - list
          title: object
          type: string
          x-stainless-const: true
        data:
          items:
            $ref: "#/components/schemas/Model"
          title: data
          type: array
      required:
      - data
      - object
      title: ListModelsResponse
      type: object
    Metadata:
      additionalProperties:
        type: string
      description: "Set of 16 key-value pairs that can be attached to an object. This\
        \ can be\nuseful for storing additional information about the object in a\
        \ structured\nformat, and querying for objects via API or the dashboard. \n\
        \nKeys are strings with a maximum length of 64 characters. Values are strings\n\
        with a maximum length of 512 characters.\n"
      nullable: true
      title: Metadata
      type: object
      x-oaiTypeLabel: map
    Model:
      description: Describes an OpenAI model offering that can be used with the API.
      example:
        created: 0
        owned_by: owned_by
        id: id
        object: model
      properties:
        id:
          description: "The model identifier, which can be referenced in the API endpoints."
          title: id
          type: string
        created:
          description: The Unix timestamp (in seconds) when the model was created.
          title: created
          type: integer
        object:
          description: "The object type, which is always \"model\"."
          enum:
          - model
          title: object
          type: string
          x-stainless-const: true
        owned_by:
          description: The organization that owns the model.
          title: owned_by
          type: string
      required:
      - created
      - id
      - object
      - owned_by
      title: Model
      x-oaiMeta:
        name: The model object
        example: |
          {
            "id": "VAR_chat_model_id",
            "object": "model",
            "created": 1686935002,
            "owned_by": "openai"
          }
    ModelIdsShared:
      anyOf:
      - type: string
      - enum:
        - gpt-4.1
        - gpt-4.1-mini
        - gpt-4.1-nano
        - gpt-4.1-2025-04-14
        - gpt-4.1-mini-2025-04-14
        - gpt-4.1-nano-2025-04-14
        - o4-mini
        - o4-mini-2025-04-16
        - o3
        - o3-2025-04-16
        - o3-mini
        - o3-mini-2025-01-31
        - o1
        - o1-2024-12-17
        - o1-preview
        - o1-preview-2024-09-12
        - o1-mini
        - o1-mini-2024-09-12
        - gpt-4o
        - gpt-4o-2024-11-20
        - gpt-4o-2024-08-06
        - gpt-4o-2024-05-13
        - gpt-4o-audio-preview
        - gpt-4o-audio-preview-2024-10-01
        - gpt-4o-audio-preview-2024-12-17
        - gpt-4o-mini-audio-preview
        - gpt-4o-mini-audio-preview-2024-12-17
        - gpt-4o-search-preview
        - gpt-4o-mini-search-preview
        - gpt-4o-search-preview-2025-03-11
        - gpt-4o-mini-search-preview-2025-03-11
        - chatgpt-4o-latest
        - gpt-4o-mini
        - gpt-4o-mini-2024-07-18
        - gpt-4-turbo
        - gpt-4-turbo-2024-04-09
        - gpt-4-0125-preview
        - gpt-4-turbo-preview
        - gpt-4-1106-preview
        - gpt-4-vision-preview
        - gpt-4
        - gpt-4-0314
        - gpt-4-0613
        - gpt-4-32k
        - gpt-4-32k-0314
        - gpt-4-32k-0613
        - gpt-3.5-turbo
        - gpt-3.5-turbo-16k
        - gpt-3.5-turbo-0301
        - gpt-3.5-turbo-0613
        - gpt-3.5-turbo-1106
        - gpt-3.5-turbo-0125
        - gpt-3.5-turbo-16k-0613
        type: string
      example: gpt-4o
      title: ModelIdsShared
    ModelResponseProperties:
      properties:
        metadata:
          additionalProperties:
            type: string
          description: "Set of 16 key-value pairs that can be attached to an object.\
            \ This can be\nuseful for storing additional information about the object\
            \ in a structured\nformat, and querying for objects via API or the dashboard.\
            \ \n\nKeys are strings with a maximum length of 64 characters. Values\
            \ are strings\nwith a maximum length of 512 characters.\n"
          nullable: true
          title: Metadata
          type: object
          x-oaiTypeLabel: map
        temperature:
          default: 1
          description: |
            What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.
            We generally recommend altering this or `top_p` but not both.
          example: 1
          maximum: 2
          minimum: 0
          nullable: true
          title: temperature
          type: number
        top_p:
          default: 1
          description: |
            An alternative to sampling with temperature, called nucleus sampling,
            where the model considers the results of the tokens with top_p probability
            mass. So 0.1 means only the tokens comprising the top 10% probability mass
            are considered.

            We generally recommend altering this or `temperature` but not both.
          example: 1
          maximum: 1
          minimum: 0
          nullable: true
          title: top_p
          type: number
        user:
          description: |
            A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).
          example: user-1234
          title: user
          type: string
        service_tier:
          $ref: "#/components/schemas/ServiceTier"
      title: ModelResponseProperties
      type: object
    ParallelToolCalls:
      default: true
      description: "Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling)\
        \ during tool use."
      type: boolean
    PredictionContent:
      description: |
        Static predicted output content, such as the content of a text file that is
        being regenerated.
      example:
        type: content
        content: PredictionContent_content
      properties:
        type:
          description: |
            The type of the predicted content you want to provide. This type is
            currently always `content`.
          enum:
          - content
          title: type
          type: string
          x-stainless-const: true
        content:
          $ref: "#/components/schemas/PredictionContent_content"
      required:
      - content
      - type
      title: Static Content
      type: object
    ReasoningEffort:
      default: medium
      description: "**o-series models only** \n\nConstrains effort on reasoning for\
        \ \n[reasoning models](https://platform.openai.com/docs/guides/reasoning).\n\
        Currently supported values are `low`, `medium`, and `high`. Reducing\nreasoning\
        \ effort can result in faster responses and fewer tokens used\non reasoning\
        \ in a response.\n"
      enum:
      - low
      - medium
      - high
      nullable: true
      title: ReasoningEffort
      type: string
    ResponseFormatJsonObject:
      description: |
        JSON object response format. An older method of generating JSON responses.
        Using `json_schema` is recommended for models that support it. Note that the
        model will not generate JSON without a system or user message instructing it
        to do so.
      properties:
        type:
          description: The type of response format being defined. Always `json_object`.
          enum:
          - json_object
          title: type
          type: string
          x-stainless-const: true
      required:
      - type
      title: JSON object
      type: object
    ResponseFormatJsonSchema:
      description: |
        JSON Schema response format. Used to generate structured JSON responses.
        Learn more about [Structured Outputs](/docs/guides/structured-outputs).
      properties:
        type:
          description: The type of response format being defined. Always `json_schema`.
          enum:
          - json_schema
          title: type
          type: string
          x-stainless-const: true
        json_schema:
          $ref: "#/components/schemas/JSON_schema"
      required:
      - json_schema
      - type
      title: JSON schema
      type: object
    ResponseFormatJsonSchemaSchema:
      additionalProperties: true
      description: |
        The schema for the response format, described as a JSON Schema object.
        Learn how to build JSON schemas [here](https://json-schema.org/).
      title: JSON schema
      type: object
    ResponseFormatText:
      description: |
        Default response format. Used to generate text responses.
      example:
        type: text
      properties:
        type:
          description: The type of response format being defined. Always `text`.
          enum:
          - text
          title: type
          type: string
          x-stainless-const: true
      required:
      - type
      title: Text
      type: object
    ResponseModalities:
      description: "Output types that you would like the model to generate.\nMost\
        \ models are capable of generating text, which is the default:\n\n`[\"text\"\
        ]`\n\nThe `gpt-4o-audio-preview` model can also be used to \n[generate audio](/docs/guides/audio).\
        \ To request that this model generate \nboth text and audio responses, you\
        \ can use:\n\n`[\"text\", \"audio\"]`\n"
      items:
        enum:
        - text
        - audio
        type: string
      nullable: true
      type: array
    ServiceTier:
      default: auto
      description: |
        Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:
          - If set to 'auto', and the Project is Scale tier enabled, the system
            will utilize scale tier credits until they are exhausted.
          - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
          - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
          - If set to 'flex', the request will be processed with the Flex Processing service tier. [Learn more](/docs/guides/flex-processing).
          - When not set, the default behavior is 'auto'.

          When this parameter is set, the response body will include the `service_tier` utilized.
      enum:
      - auto
      - default
      - flex
      nullable: true
      title: ServiceTier
      type: string
    StopConfiguration:
      default: null
      description: |
        Not supported with latest reasoning models `o3` and `o4-mini`.

        Up to 4 sequences where the API will stop generating further tokens. The
        returned text will not contain the stop sequence.
      nullable: true
      oneOf:
      - default: <|endoftext|>
        example: |2+

        nullable: true
        type: string
      - items:
          example: "[\"\\n\"]"
          type: string
        maxItems: 4
        minItems: 1
        type: array
      title: StopConfiguration
    VoiceIdsShared:
      anyOf:
      - type: string
      - enum:
        - alloy
        - ash
        - ballad
        - coral
        - echo
        - fable
        - onyx
        - nova
        - sage
        - shimmer
        - verse
        type: string
      example: ash
      title: VoiceIdsShared
    WebSearchContextSize:
      default: medium
      description: "High level guidance for the amount of context window space to\
        \ use for the \nsearch. One of `low`, `medium`, or `high`. `medium` is the\
        \ default.\n"
      enum:
      - low
      - medium
      - high
      title: WebSearchContextSize
      type: string
    WebSearchLocation:
      description: Approximate location parameters for the search.
      example:
        country: country
        city: city
        timezone: timezone
        region: region
      properties:
        country:
          description: "The two-letter \n[ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1)\
            \ of the user,\ne.g. `US`.\n"
          title: country
          type: string
        region:
          description: |
            Free text input for the region of the user, e.g. `California`.
          title: region
          type: string
        city:
          description: |
            Free text input for the city of the user, e.g. `San Francisco`.
          title: city
          type: string
        timezone:
          description: "The [IANA timezone](https://timeapi.io/documentation/iana-timezones)\
            \ \nof the user, e.g. `America/Los_Angeles`.\n"
          title: timezone
          type: string
      title: Web search location
      type: object
    updateChatCompletion_request:
      properties:
        metadata:
          additionalProperties:
            type: string
          description: "Set of 16 key-value pairs that can be attached to an object.\
            \ This can be\nuseful for storing additional information about the object\
            \ in a structured\nformat, and querying for objects via API or the dashboard.\
            \ \n\nKeys are strings with a maximum length of 64 characters. Values\
            \ are strings\nwith a maximum length of 512 characters.\n"
          nullable: true
          title: Metadata
          type: object
          x-oaiTypeLabel: map
      required:
      - metadata
      title: updateChatCompletion_request
      type: object
    ChatCompletionMessageList_data_inner:
      allOf:
      - $ref: "#/components/schemas/ChatCompletionResponseMessage"
      - properties:
          id:
            description: The identifier of the chat message.
            type: string
        required:
        - id
        type: object
      example:
        role: assistant
        function_call:
          name: name
          arguments: arguments
        refusal: refusal
        annotations:
        - type: url_citation
          url_citation:
            start_index: 1
            end_index: 6
            title: title
            url: url
        - type: url_citation
          url_citation:
            start_index: 1
            end_index: 6
            title: title
            url: url
        tool_calls:
        - function:
            name: name
            arguments: arguments
          id: id
          type: function
        - function:
            name: name
            arguments: arguments
          id: id
          type: function
        audio:
          expires_at: 5
          transcript: transcript
          data: data
          id: id
        id: id
        content: content
      title: ChatCompletionMessageList_data_inner
    ChatCompletionMessageToolCall_function:
      description: The function that the model called.
      example:
        name: name
        arguments: arguments
      properties:
        name:
          description: The name of the function to call.
          title: name
          type: string
        arguments:
          description: "The arguments to call the function with, as generated by the\
            \ model in JSON format. Note that the model does not always generate valid\
            \ JSON, and may hallucinate parameters not defined by your function schema.\
            \ Validate the arguments in your code before calling your function."
          title: arguments
          type: string
      required:
      - arguments
      - name
      title: ChatCompletionMessageToolCall_function
      type: object
    ChatCompletionMessageToolCallChunk_function:
      properties:
        name:
          description: The name of the function to call.
          title: name
          type: string
        arguments:
          description: "The arguments to call the function with, as generated by the\
            \ model in JSON format. Note that the model does not always generate valid\
            \ JSON, and may hallucinate parameters not defined by your function schema.\
            \ Validate the arguments in your code before calling your function."
          title: arguments
          type: string
      title: ChatCompletionMessageToolCallChunk_function
      type: object
    ChatCompletionNamedToolChoice_function:
      properties:
        name:
          description: The name of the function to call.
          title: name
          type: string
      required:
      - name
      title: ChatCompletionNamedToolChoice_function
      type: object
    ChatCompletionRequestAssistantMessage_content:
      description: |
        The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified.
      nullable: true
      oneOf:
      - description: The contents of the assistant message.
        title: Text content
        type: string
      - description: "An array of content parts with a defined type. Can be one or\
          \ more of type `text`, or exactly one of type `refusal`."
        items:
          $ref: "#/components/schemas/ChatCompletionRequestAssistantMessageContentPart"
        minItems: 1
        title: Array of content parts
        type: array
      title: ChatCompletionRequestAssistantMessage_content
    ChatCompletionRequestAssistantMessage_audio:
      description: "Data about a previous audio response from the model. \n[Learn\
        \ more](/docs/guides/audio).\n"
      nullable: true
      properties:
        id:
          description: |
            Unique identifier for a previous audio response from the model.
          title: id
          type: string
      required:
      - id
      title: ChatCompletionRequestAssistantMessage_audio
      type: object
    ChatCompletionRequestAssistantMessage_function_call:
      deprecated: true
      description: "Deprecated and replaced by `tool_calls`. The name and arguments\
        \ of a function that should be called, as generated by the model."
      nullable: true
      properties:
        arguments:
          description: "The arguments to call the function with, as generated by the\
            \ model in JSON format. Note that the model does not always generate valid\
            \ JSON, and may hallucinate parameters not defined by your function schema.\
            \ Validate the arguments in your code before calling your function."
          title: arguments
          type: string
        name:
          description: The name of the function to call.
          title: name
          type: string
      required:
      - arguments
      - name
      title: ChatCompletionRequestAssistantMessage_function_call
      type: object
    ChatCompletionRequestDeveloperMessage_content:
      description: The contents of the developer message.
      oneOf:
      - description: The contents of the developer message.
        title: Text content
        type: string
      - description: "An array of content parts with a defined type. For developer\
          \ messages, only type `text` is supported."
        items:
          $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartText"
        minItems: 1
        title: Array of content parts
        type: array
      title: ChatCompletionRequestDeveloperMessage_content
    ChatCompletionRequestMessageContentPartAudio_input_audio:
      properties:
        data:
          description: Base64 encoded audio data.
          title: data
          type: string
        format:
          description: |
            The format of the encoded audio data. Currently supports "wav" and "mp3".
          enum:
          - wav
          - mp3
          title: format
          type: string
      required:
      - data
      - format
      title: ChatCompletionRequestMessageContentPartAudio_input_audio
      type: object
    ChatCompletionRequestMessageContentPartFile_file:
      properties:
        filename:
          description: "The name of the file, used when passing the file to the model\
            \ as a \nstring.\n"
          title: filename
          type: string
        file_data:
          description: "The base64 encoded file data, used when passing the file to\
            \ the model \nas a string.\n"
          title: file_data
          type: string
        file_id:
          description: |
            The ID of an uploaded file to use as input.
          title: file_id
          type: string
      title: ChatCompletionRequestMessageContentPartFile_file
      type: object
    ChatCompletionRequestMessageContentPartImage_image_url:
      properties:
        url:
          description: Either a URL of the image or the base64 encoded image data.
          format: uri
          title: url
          type: string
        detail:
          default: auto
          description: "Specifies the detail level of the image. Learn more in the\
            \ [Vision guide](/docs/guides/vision#low-or-high-fidelity-image-understanding)."
          enum:
          - auto
          - low
          - high
          title: detail
          type: string
      required:
      - url
      title: ChatCompletionRequestMessageContentPartImage_image_url
      type: object
    ChatCompletionRequestSystemMessage_content:
      description: The contents of the system message.
      oneOf:
      - description: The contents of the system message.
        title: Text content
        type: string
      - description: "An array of content parts with a defined type. For system messages,\
          \ only type `text` is supported."
        items:
          $ref: "#/components/schemas/ChatCompletionRequestSystemMessageContentPart"
        minItems: 1
        title: Array of content parts
        type: array
      title: ChatCompletionRequestSystemMessage_content
    ChatCompletionRequestToolMessage_content:
      description: The contents of the tool message.
      oneOf:
      - description: The contents of the tool message.
        title: Text content
        type: string
      - description: "An array of content parts with a defined type. For tool messages,\
          \ only type `text` is supported."
        items:
          $ref: "#/components/schemas/ChatCompletionRequestToolMessageContentPart"
        minItems: 1
        title: Array of content parts
        type: array
      title: ChatCompletionRequestToolMessage_content
    ChatCompletionRequestUserMessage_content:
      description: |
        The contents of the user message.
      oneOf:
      - description: The text contents of the message.
        title: Text content
        type: string
      - description: "An array of content parts with a defined type. Supported options\
          \ differ based on the [model](/docs/models) being used to generate the response.\
          \ Can contain text, image, or audio inputs."
        items:
          $ref: "#/components/schemas/ChatCompletionRequestUserMessageContentPart"
        minItems: 1
        title: Array of content parts
        type: array
      title: ChatCompletionRequestUserMessage_content
    ChatCompletionResponseMessage_annotations_inner_url_citation:
      description: A URL citation when using web search.
      example:
        start_index: 1
        end_index: 6
        title: title
        url: url
      properties:
        end_index:
          description: The index of the last character of the URL citation in the
            message.
          title: end_index
          type: integer
        start_index:
          description: The index of the first character of the URL citation in the
            message.
          title: start_index
          type: integer
        url:
          description: The URL of the web resource.
          title: url
          type: string
        title:
          description: The title of the web resource.
          title: title
          type: string
      required:
      - end_index
      - start_index
      - title
      - url
      title: ChatCompletionResponseMessage_annotations_inner_url_citation
      type: object
    ChatCompletionResponseMessage_annotations_inner:
      description: |
        A URL citation when using web search.
      example:
        type: url_citation
        url_citation:
          start_index: 1
          end_index: 6
          title: title
          url: url
      properties:
        type:
          description: The type of the URL citation. Always `url_citation`.
          enum:
          - url_citation
          title: type
          type: string
          x-stainless-const: true
        url_citation:
          $ref: "#/components/schemas/ChatCompletionResponseMessage_annotations_inner_url_citation"
      required:
      - type
      - url_citation
      title: ChatCompletionResponseMessage_annotations_inner
      type: object
    ChatCompletionResponseMessage_function_call:
      deprecated: true
      description: "Deprecated and replaced by `tool_calls`. The name and arguments\
        \ of a function that should be called, as generated by the model."
      example:
        name: name
        arguments: arguments
      properties:
        arguments:
          description: "The arguments to call the function with, as generated by the\
            \ model in JSON format. Note that the model does not always generate valid\
            \ JSON, and may hallucinate parameters not defined by your function schema.\
            \ Validate the arguments in your code before calling your function."
          title: arguments
          type: string
        name:
          description: The name of the function to call.
          title: name
          type: string
      required:
      - arguments
      - name
      title: ChatCompletionResponseMessage_function_call
      type: object
    ChatCompletionResponseMessage_audio:
      description: |
        If the audio output modality is requested, this object contains data
        about the audio response from the model. [Learn more](/docs/guides/audio).
      example:
        expires_at: 5
        transcript: transcript
        data: data
        id: id
      nullable: true
      properties:
        id:
          description: Unique identifier for this audio response.
          title: id
          type: string
        expires_at:
          description: |
            The Unix timestamp (in seconds) for when this audio response will
            no longer be accessible on the server for use in multi-turn
            conversations.
          title: expires_at
          type: integer
        data:
          description: |
            Base64 encoded audio bytes generated by the model, in the format
            specified in the request.
          title: data
          type: string
        transcript:
          description: Transcript of the audio generated by the model.
          title: transcript
          type: string
      required:
      - data
      - expires_at
      - id
      - transcript
      title: ChatCompletionResponseMessage_audio
      type: object
    ChatCompletionStreamResponseDelta_function_call:
      deprecated: true
      description: "Deprecated and replaced by `tool_calls`. The name and arguments\
        \ of a function that should be called, as generated by the model."
      properties:
        arguments:
          description: "The arguments to call the function with, as generated by the\
            \ model in JSON format. Note that the model does not always generate valid\
            \ JSON, and may hallucinate parameters not defined by your function schema.\
            \ Validate the arguments in your code before calling your function."
          title: arguments
          type: string
        name:
          description: The name of the function to call.
          title: name
          type: string
      title: ChatCompletionStreamResponseDelta_function_call
      type: object
    ChatCompletionTokenLogprob_top_logprobs_inner:
      example:
        logprob: 7.061401241503109
        bytes:
        - 9
        - 9
        token: token
      properties:
        token:
          description: The token.
          title: token
          type: string
        logprob:
          description: "The log probability of this token, if it is within the top\
            \ 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify\
            \ that the token is very unlikely."
          title: logprob
          type: number
        bytes:
          description: A list of integers representing the UTF-8 bytes representation
            of the token. Useful in instances where characters are represented by
            multiple tokens and their byte representations must be combined to generate
            the correct text representation. Can be `null` if there is no bytes representation
            for the token.
          items:
            type: integer
          nullable: true
          title: bytes
          type: array
      required:
      - bytes
      - logprob
      - token
      title: ChatCompletionTokenLogprob_top_logprobs_inner
      type: object
    CompletionUsage_completion_tokens_details:
      description: Breakdown of tokens used in a completion.
      example:
        accepted_prediction_tokens: 1
        audio_tokens: 1
        reasoning_tokens: 1
        rejected_prediction_tokens: 6
      properties:
        accepted_prediction_tokens:
          default: 0
          description: |
            When using Predicted Outputs, the number of tokens in the
            prediction that appeared in the completion.
          title: accepted_prediction_tokens
          type: integer
        audio_tokens:
          default: 0
          description: Audio input tokens generated by the model.
          title: audio_tokens
          type: integer
        reasoning_tokens:
          default: 0
          description: Tokens generated by the model for reasoning.
          title: reasoning_tokens
          type: integer
        rejected_prediction_tokens:
          default: 0
          description: |
            When using Predicted Outputs, the number of tokens in the
            prediction that did not appear in the completion. However, like
            reasoning tokens, these tokens are still counted in the total
            completion tokens for purposes of billing, output, and context window
            limits.
          title: rejected_prediction_tokens
          type: integer
      title: CompletionUsage_completion_tokens_details
      type: object
    CompletionUsage_prompt_tokens_details:
      description: Breakdown of tokens used in the prompt.
      example:
        audio_tokens: 7
        cached_tokens: 1
      properties:
        audio_tokens:
          default: 0
          description: Audio input tokens present in the prompt.
          title: audio_tokens
          type: integer
        cached_tokens:
          default: 0
          description: Cached tokens present in the prompt.
          title: cached_tokens
          type: integer
      title: CompletionUsage_prompt_tokens_details
      type: object
    Web_search_user_location:
      description: |
        Approximate location parameters for the search.
      example:
        approximate:
          country: country
          city: city
          timezone: timezone
          region: region
        type: approximate
      nullable: true
      properties:
        type:
          description: |
            The type of location approximation. Always `approximate`.
          enum:
          - approximate
          title: type
          type: string
          x-stainless-const: true
        approximate:
          $ref: "#/components/schemas/WebSearchLocation"
      required:
      - approximate
      - type
      title: Web_search_user_location
      type: object
    Web_search:
      description: |
        This tool searches the web for relevant results to use in a response.
        Learn more about the [web search tool](/docs/guides/tools-web-search?api-mode=chat).
      example:
        search_context_size: medium
        user_location:
          approximate:
            country: country
            city: city
            timezone: timezone
            region: region
          type: approximate
      properties:
        user_location:
          $ref: "#/components/schemas/Web_search_user_location"
        search_context_size:
          $ref: "#/components/schemas/WebSearchContextSize"
      title: Web search
      type: object
    CreateChatCompletionRequest_allOf_response_format:
      description: |
        An object specifying the format that the model must output.

        Setting to `{ "type": "json_schema", "json_schema": {...} }` enables
        Structured Outputs which ensures the model will match your supplied JSON
        schema. Learn more in the [Structured Outputs
        guide](/docs/guides/structured-outputs).

        Setting to `{ "type": "json_object" }` enables the older JSON mode, which
        ensures the message the model generates is valid JSON. Using `json_schema`
        is preferred for models that support it.
      oneOf:
      - $ref: "#/components/schemas/ResponseFormatText"
      - $ref: "#/components/schemas/ResponseFormatJsonSchema"
      - $ref: "#/components/schemas/ResponseFormatJsonObject"
      title: CreateChatCompletionRequest_allOf_response_format
    CreateChatCompletionRequest_allOf_audio:
      description: |
        Parameters for audio output. Required when audio output is requested with
        `modalities: ["audio"]`. [Learn more](/docs/guides/audio).
      example:
        voice: ash
        format: wav
      nullable: true
      properties:
        voice:
          $ref: "#/components/schemas/VoiceIdsShared"
        format:
          description: |
            Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`,
            `opus`, or `pcm16`.
          enum:
          - wav
          - aac
          - mp3
          - flac
          - opus
          - pcm16
          title: format
          type: string
      required:
      - format
      - voice
      title: CreateChatCompletionRequest_allOf_audio
      type: object
    CreateChatCompletionRequest_allOf_function_call:
      deprecated: true
      description: |
        Deprecated in favor of `tool_choice`.

        Controls which (if any) function is called by the model.

        `none` means the model will not call a function and instead generates a
        message.

        `auto` means the model can pick between generating a message or calling a
        function.

        Specifying a particular function via `{"name": "my_function"}` forces the
        model to call that function.

        `none` is the default when no functions are present. `auto` is the default
        if functions are present.
      oneOf:
      - description: |
          `none` means the model will not call a function and instead generates a message. `auto` means the model can pick between generating a message or calling a function.
        enum:
        - none
        - auto
        type: string
      - $ref: "#/components/schemas/ChatCompletionFunctionCallOption"
      title: CreateChatCompletionRequest_allOf_function_call
    CreateChatCompletionResponse_choices_inner_logprobs:
      description: Log probability information for the choice.
      example:
        refusal:
        - top_logprobs:
          - logprob: 7.061401241503109
            bytes:
            - 9
            - 9
            token: token
          - logprob: 7.061401241503109
            bytes:
            - 9
            - 9
            token: token
          logprob: 5.637376656633329
          bytes:
          - 2
          - 2
          token: token
        - top_logprobs:
          - logprob: 7.061401241503109
            bytes:
            - 9
            - 9
            token: token
          - logprob: 7.061401241503109
            bytes:
            - 9
            - 9
            token: token
          logprob: 5.637376656633329
          bytes:
          - 2
          - 2
          token: token
        content:
        - top_logprobs:
          - logprob: 7.061401241503109
            bytes:
            - 9
            - 9
            token: token
          - logprob: 7.061401241503109
            bytes:
            - 9
            - 9
            token: token
          logprob: 5.637376656633329
          bytes:
          - 2
          - 2
          token: token
        - top_logprobs:
          - logprob: 7.061401241503109
            bytes:
            - 9
            - 9
            token: token
          - logprob: 7.061401241503109
            bytes:
            - 9
            - 9
            token: token
          logprob: 5.637376656633329
          bytes:
          - 2
          - 2
          token: token
      nullable: true
      properties:
        content:
          description: A list of message content tokens with log probability information.
          items:
            $ref: "#/components/schemas/ChatCompletionTokenLogprob"
          nullable: true
          title: content
          type: array
        refusal:
          description: A list of message refusal tokens with log probability information.
          items:
            $ref: "#/components/schemas/ChatCompletionTokenLogprob"
          nullable: true
          title: refusal
          type: array
      required:
      - content
      - refusal
      title: CreateChatCompletionResponse_choices_inner_logprobs
      type: object
    CreateChatCompletionResponse_choices_inner:
      example:
        finish_reason: stop
        index: 0
        message:
          role: assistant
          function_call:
            name: name
            arguments: arguments
          refusal: refusal
          annotations:
          - type: url_citation
            url_citation:
              start_index: 1
              end_index: 6
              title: title
              url: url
          - type: url_citation
            url_citation:
              start_index: 1
              end_index: 6
              title: title
              url: url
          tool_calls:
          - function:
              name: name
              arguments: arguments
            id: id
            type: function
          - function:
              name: name
              arguments: arguments
            id: id
            type: function
          audio:
            expires_at: 5
            transcript: transcript
            data: data
            id: id
          content: content
        logprobs:
          refusal:
          - top_logprobs:
            - logprob: 7.061401241503109
              bytes:
              - 9
              - 9
              token: token
            - logprob: 7.061401241503109
              bytes:
              - 9
              - 9
              token: token
            logprob: 5.637376656633329
            bytes:
            - 2
            - 2
            token: token
          - top_logprobs:
            - logprob: 7.061401241503109
              bytes:
              - 9
              - 9
              token: token
            - logprob: 7.061401241503109
              bytes:
              - 9
              - 9
              token: token
            logprob: 5.637376656633329
            bytes:
            - 2
            - 2
            token: token
          content:
          - top_logprobs:
            - logprob: 7.061401241503109
              bytes:
              - 9
              - 9
              token: token
            - logprob: 7.061401241503109
              bytes:
              - 9
              - 9
              token: token
            logprob: 5.637376656633329
            bytes:
            - 2
            - 2
            token: token
          - top_logprobs:
            - logprob: 7.061401241503109
              bytes:
              - 9
              - 9
              token: token
            - logprob: 7.061401241503109
              bytes:
              - 9
              - 9
              token: token
            logprob: 5.637376656633329
            bytes:
            - 2
            - 2
            token: token
      properties:
        finish_reason:
          description: |
            The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
            `length` if the maximum number of tokens specified in the request was reached,
            `content_filter` if content was omitted due to a flag from our content filters,
            `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
          enum:
          - stop
          - length
          - tool_calls
          - content_filter
          - function_call
          title: finish_reason
          type: string
        index:
          description: The index of the choice in the list of choices.
          title: index
          type: integer
        message:
          $ref: "#/components/schemas/ChatCompletionResponseMessage"
        logprobs:
          $ref: "#/components/schemas/CreateChatCompletionResponse_choices_inner_logprobs"
      required:
      - finish_reason
      - index
      - logprobs
      - message
      title: CreateChatCompletionResponse_choices_inner
      type: object
    CreateChatCompletionStreamResponse_choices_inner:
      properties:
        delta:
          $ref: "#/components/schemas/ChatCompletionStreamResponseDelta"
        logprobs:
          $ref: "#/components/schemas/CreateChatCompletionResponse_choices_inner_logprobs"
        finish_reason:
          description: |
            The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
            `length` if the maximum number of tokens specified in the request was reached,
            `content_filter` if content was omitted due to a flag from our content filters,
            `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
          enum:
          - stop
          - length
          - tool_calls
          - content_filter
          - function_call
          nullable: true
          title: finish_reason
          type: string
        index:
          description: The index of the choice in the list of choices.
          title: index
          type: integer
      required:
      - delta
      - finish_reason
      - index
      title: CreateChatCompletionStreamResponse_choices_inner
      type: object
    CreateCompletionRequest_model:
      anyOf:
      - type: string
      - enum:
        - gpt-3.5-turbo-instruct
        - davinci-002
        - babbage-002
        type: string
      description: |
        ID of the model to use. You can use the [List models](/docs/api-reference/models/list) API to see all of your available models, or see our [Model overview](/docs/models) for descriptions of them.
      title: CreateCompletionRequest_model
      x-oaiTypeLabel: string
    CreateCompletionRequest_prompt:
      default: <|endoftext|>
      description: |
        The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.

        Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.
      nullable: true
      oneOf:
      - default: ""
        example: This is a test.
        type: string
      - items:
          default: ""
          example: This is a test.
          type: string
        type: array
      - example: "[1212, 318, 257, 1332, 13]"
        items:
          type: integer
        minItems: 1
        type: array
      - example: "[[1212, 318, 257, 1332, 13]]"
        items:
          items:
            type: integer
          minItems: 1
          type: array
        minItems: 1
        type: array
      title: CreateCompletionRequest_prompt
    CreateCompletionResponse_choices_inner_logprobs:
      example:
        top_logprobs:
        - key: 5.962133916683182
        - key: 5.962133916683182
        token_logprobs:
        - 1.4658129805029452
        - 1.4658129805029452
        tokens:
        - tokens
        - tokens
        text_offset:
        - 6
        - 6
      nullable: true
      properties:
        text_offset:
          items:
            type: integer
          title: text_offset
          type: array
        token_logprobs:
          items:
            type: number
          title: token_logprobs
          type: array
        tokens:
          items:
            type: string
          title: tokens
          type: array
        top_logprobs:
          items:
            additionalProperties:
              type: number
            type: object
          title: top_logprobs
          type: array
      title: CreateCompletionResponse_choices_inner_logprobs
      type: object
    CreateCompletionResponse_choices_inner:
      example:
        finish_reason: stop
        index: 0
        text: text
        logprobs:
          top_logprobs:
          - key: 5.962133916683182
          - key: 5.962133916683182
          token_logprobs:
          - 1.4658129805029452
          - 1.4658129805029452
          tokens:
          - tokens
          - tokens
          text_offset:
          - 6
          - 6
      properties:
        finish_reason:
          description: |
            The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,
            `length` if the maximum number of tokens specified in the request was reached,
            or `content_filter` if content was omitted due to a flag from our content filters.
          enum:
          - stop
          - length
          - content_filter
          title: finish_reason
          type: string
        index:
          title: index
          type: integer
        logprobs:
          $ref: "#/components/schemas/CreateCompletionResponse_choices_inner_logprobs"
        text:
          title: text
          type: string
      required:
      - finish_reason
      - index
      - logprobs
      - text
      title: CreateCompletionResponse_choices_inner
      type: object
    PredictionContent_content:
      description: |
        The content that should be matched when generating a model response.
        If generated tokens would match this content, the entire model response
        can be returned much more quickly.
      oneOf:
      - description: |
          The content used for a Predicted Output. This is often the
          text of a file you are regenerating with minor changes.
        title: Text content
        type: string
      - description: "An array of content parts with a defined type. Supported options\
          \ differ based on the [model](/docs/models) being used to generate the response.\
          \ Can contain text inputs."
        items:
          $ref: "#/components/schemas/ChatCompletionRequestMessageContentPartText"
        minItems: 1
        title: Array of content parts
        type: array
      title: PredictionContent_content
    JSON_schema:
      description: |
        Structured Outputs configuration options, including a JSON Schema.
      properties:
        description:
          description: |
            A description of what the response format is for, used by the model to
            determine how to respond in the format.
          title: description
          type: string
        name:
          description: |
            The name of the response format. Must be a-z, A-Z, 0-9, or contain
            underscores and dashes, with a maximum length of 64.
          title: name
          type: string
        schema:
          additionalProperties: true
          description: |
            The schema for the response format, described as a JSON Schema object.
            Learn how to build JSON schemas [here](https://json-schema.org/).
          title: JSON schema
          type: object
        strict:
          default: false
          description: |
            Whether to enable strict schema adherence when generating the output.
            If set to true, the model will always follow the exact schema defined
            in the `schema` field. Only a subset of JSON Schema is supported when
            `strict` is `true`. To learn more, read the [Structured Outputs
            guide](/docs/guides/structured-outputs).
          nullable: true
          title: strict
          type: boolean
      required:
      - name
      title: JSON schema
      type: object
  securitySchemes:
    ApiKeyAuth:
      scheme: bearer
      type: http
      x-bearerInfoFunc: openapi_server.controllers.security_controller.info_from_ApiKeyAuth
x-oaiMeta:
  navigationGroups:
  - id: responses
    title: Responses
  - id: chat
    title: Chat Completions
  - id: realtime
    title: Realtime
    beta: true
  - id: endpoints
    title: Platform APIs
  - id: vector_stores
    title: Vector stores
  - id: assistants
    title: Assistants
    beta: true
  - id: administration
    title: Administration
  - id: legacy
    title: Legacy
  groups:
  - id: responses
    title: Responses
    description: |
      OpenAI's most advanced interface for generating model responses. Supports
      text and image inputs, and text outputs. Create stateful interactions
      with the model, using the output of previous responses as input. Extend
      the model's capabilities with built-in tools for file search, web search,
      computer use, and more. Allow the model access to external systems and data
      using function calling.

      Related guides:
      - [Quickstart](/docs/quickstart?api-mode=responses)
      - [Text inputs and outputs](/docs/guides/text?api-mode=responses)
      - [Image inputs](/docs/guides/images?api-mode=responses)
      - [Structured Outputs](/docs/guides/structured-outputs?api-mode=responses)
      - [Function calling](/docs/guides/function-calling?api-mode=responses)
      - [Conversation state](/docs/guides/conversation-state?api-mode=responses)
      - [Extend the models with tools](/docs/guides/tools?api-mode=responses)
    navigationGroup: responses
    sections:
    - type: endpoint
      key: createResponse
      path: create
    - type: endpoint
      key: getResponse
      path: get
    - type: endpoint
      key: deleteResponse
      path: delete
    - type: endpoint
      key: listInputItems
      path: input-items
    - type: object
      key: Response
      path: object
    - type: object
      key: ResponseItemList
      path: list
  - id: responses-streaming
    title: Streaming
    description: |
      When you [create a Response](/docs/api-reference/responses/create) with
      `stream` set to `true`, the server will emit server-sent events to the
      client as the Response is generated. This section contains the events that
      are emitted by the server.

      [Learn more about streaming responses](/docs/guides/streaming-responses?api-mode=responses).
    navigationGroup: responses
    sections:
    - type: object
      key: ResponseCreatedEvent
      path: <auto>
    - type: object
      key: ResponseInProgressEvent
      path: <auto>
    - type: object
      key: ResponseCompletedEvent
      path: <auto>
    - type: object
      key: ResponseFailedEvent
      path: <auto>
    - type: object
      key: ResponseIncompleteEvent
      path: <auto>
    - type: object
      key: ResponseOutputItemAddedEvent
      path: <auto>
    - type: object
      key: ResponseOutputItemDoneEvent
      path: <auto>
    - type: object
      key: ResponseContentPartAddedEvent
      path: <auto>
    - type: object
      key: ResponseContentPartDoneEvent
      path: <auto>
    - type: object
      key: ResponseTextDeltaEvent
      path: <auto>
    - type: object
      key: ResponseTextAnnotationDeltaEvent
      path: <auto>
    - type: object
      key: ResponseTextDoneEvent
      path: <auto>
    - type: object
      key: ResponseRefusalDeltaEvent
      path: <auto>
    - type: object
      key: ResponseRefusalDoneEvent
      path: <auto>
    - type: object
      key: ResponseFunctionCallArgumentsDeltaEvent
      path: <auto>
    - type: object
      key: ResponseFunctionCallArgumentsDoneEvent
      path: <auto>
    - type: object
      key: ResponseFileSearchCallInProgressEvent
      path: <auto>
    - type: object
      key: ResponseFileSearchCallSearchingEvent
      path: <auto>
    - type: object
      key: ResponseFileSearchCallCompletedEvent
      path: <auto>
    - type: object
      key: ResponseWebSearchCallInProgressEvent
      path: <auto>
    - type: object
      key: ResponseWebSearchCallSearchingEvent
      path: <auto>
    - type: object
      key: ResponseWebSearchCallCompletedEvent
      path: <auto>
    - type: object
      key: ResponseReasoningSummaryPartAddedEvent
      path: <auto>
    - type: object
      key: ResponseReasoningSummaryPartDoneEvent
      path: <auto>
    - type: object
      key: ResponseReasoningSummaryTextDeltaEvent
      path: <auto>
    - type: object
      key: ResponseReasoningSummaryTextDoneEvent
      path: <auto>
    - type: object
      key: ResponseErrorEvent
      path: <auto>
  - id: chat
    title: Chat Completions
    description: |
      The Chat Completions API endpoint will generate a model response from a
      list of messages comprising a conversation.

      Related guides:
      - [Quickstart](/docs/quickstart?api-mode=chat)
      - [Text inputs and outputs](/docs/guides/text?api-mode=chat)
      - [Image inputs](/docs/guides/images?api-mode=chat)
      - [Audio inputs and outputs](/docs/guides/audio?api-mode=chat)
      - [Structured Outputs](/docs/guides/structured-outputs?api-mode=chat)
      - [Function calling](/docs/guides/function-calling?api-mode=chat)
      - [Conversation state](/docs/guides/conversation-state?api-mode=chat)

      **Starting a new project?** We recommend trying [Responses](/docs/api-reference/responses)
      to take advantage of the latest OpenAI platform features. Compare
      [Chat Completions with Responses](/docs/guides/responses-vs-chat-completions?api-mode=responses).
    navigationGroup: chat
    sections:
    - type: endpoint
      key: createChatCompletion
      path: create
    - type: endpoint
      key: getChatCompletion
      path: get
    - type: endpoint
      key: getChatCompletionMessages
      path: getMessages
    - type: endpoint
      key: listChatCompletions
      path: list
    - type: endpoint
      key: updateChatCompletion
      path: update
    - type: endpoint
      key: deleteChatCompletion
      path: delete
    - type: object
      key: CreateChatCompletionResponse
      path: object
    - type: object
      key: ChatCompletionList
      path: list-object
    - type: object
      key: ChatCompletionMessageList
      path: message-list
  - id: chat-streaming
    title: Streaming
    description: |
      Stream Chat Completions in real time. Receive chunks of completions
      returned from the model using server-sent events.
      [Learn more](/docs/guides/streaming-responses?api-mode=chat).
    navigationGroup: chat
    sections:
    - type: object
      key: CreateChatCompletionStreamResponse
      path: streaming
  - id: realtime
    title: Realtime
    beta: true
    description: |
      Communicate with a GPT-4o class model in real time using WebRTC or
      WebSockets. Supports text and audio inputs and ouputs, along with audio
      transcriptions.
      [Learn more about the Realtime API](/docs/guides/realtime).
    navigationGroup: realtime
  - id: realtime-sessions
    title: Session tokens
    description: |
      REST API endpoint to generate ephemeral session tokens for use in client-side
      applications.
    navigationGroup: realtime
    sections:
    - type: endpoint
      key: create-realtime-session
      path: create
    - type: endpoint
      key: create-realtime-transcription-session
      path: create-transcription
    - type: object
      key: RealtimeSessionCreateResponse
      path: session_object
    - type: object
      key: RealtimeTranscriptionSessionCreateResponse
      path: transcription_session_object
  - id: realtime-client-events
    title: Client events
    description: |
      These are events that the OpenAI Realtime WebSocket server will accept from the client.
    navigationGroup: realtime
    sections:
    - type: object
      key: RealtimeClientEventSessionUpdate
      path: <auto>
    - type: object
      key: RealtimeClientEventInputAudioBufferAppend
      path: <auto>
    - type: object
      key: RealtimeClientEventInputAudioBufferCommit
      path: <auto>
    - type: object
      key: RealtimeClientEventInputAudioBufferClear
      path: <auto>
    - type: object
      key: RealtimeClientEventConversationItemCreate
      path: <auto>
    - type: object
      key: RealtimeClientEventConversationItemRetrieve
      path: <auto>
    - type: object
      key: RealtimeClientEventConversationItemTruncate
      path: <auto>
    - type: object
      key: RealtimeClientEventConversationItemDelete
      path: <auto>
    - type: object
      key: RealtimeClientEventResponseCreate
      path: <auto>
    - type: object
      key: RealtimeClientEventResponseCancel
      path: <auto>
    - type: object
      key: RealtimeClientEventTranscriptionSessionUpdate
      path: <auto>
    - type: object
      key: RealtimeClientEventOutputAudioBufferClear
      path: <auto>
  - id: realtime-server-events
    title: Server events
    description: |
      These are events emitted from the OpenAI Realtime WebSocket server to the client.
    navigationGroup: realtime
    sections:
    - type: object
      key: RealtimeServerEventError
      path: <auto>
    - type: object
      key: RealtimeServerEventSessionCreated
      path: <auto>
    - type: object
      key: RealtimeServerEventSessionUpdated
      path: <auto>
    - type: object
      key: RealtimeServerEventConversationCreated
      path: <auto>
    - type: object
      key: RealtimeServerEventConversationItemCreated
      path: <auto>
    - type: object
      key: RealtimeServerEventConversationItemRetrieved
      path: <auto>
    - type: object
      key: RealtimeServerEventConversationItemInputAudioTranscriptionCompleted
      path: <auto>
    - type: object
      key: RealtimeServerEventConversationItemInputAudioTranscriptionDelta
      path: <auto>
    - type: object
      key: RealtimeServerEventConversationItemInputAudioTranscriptionFailed
      path: <auto>
    - type: object
      key: RealtimeServerEventConversationItemTruncated
      path: <auto>
    - type: object
      key: RealtimeServerEventConversationItemDeleted
      path: <auto>
    - type: object
      key: RealtimeServerEventInputAudioBufferCommitted
      path: <auto>
    - type: object
      key: RealtimeServerEventInputAudioBufferCleared
      path: <auto>
    - type: object
      key: RealtimeServerEventInputAudioBufferSpeechStarted
      path: <auto>
    - type: object
      key: RealtimeServerEventInputAudioBufferSpeechStopped
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseCreated
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseDone
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseOutputItemAdded
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseOutputItemDone
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseContentPartAdded
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseContentPartDone
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseTextDelta
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseTextDone
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseAudioTranscriptDelta
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseAudioTranscriptDone
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseAudioDelta
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseAudioDone
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseFunctionCallArgumentsDelta
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseFunctionCallArgumentsDone
      path: <auto>
    - type: object
      key: RealtimeServerEventTranscriptionSessionUpdated
      path: <auto>
    - type: object
      key: RealtimeServerEventRateLimitsUpdated
      path: <auto>
    - type: object
      key: RealtimeServerEventOutputAudioBufferStarted
      path: <auto>
    - type: object
      key: RealtimeServerEventOutputAudioBufferStopped
      path: <auto>
    - type: object
      key: RealtimeServerEventOutputAudioBufferCleared
      path: <auto>
  - id: audio
    title: Audio
    description: |
      Learn how to turn audio into text or text into audio.

      Related guide: [Speech to text](/docs/guides/speech-to-text)
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createSpeech
      path: createSpeech
    - type: endpoint
      key: createTranscription
      path: createTranscription
    - type: endpoint
      key: createTranslation
      path: createTranslation
    - type: object
      key: CreateTranscriptionResponseJson
      path: json-object
    - type: object
      key: CreateTranscriptionResponseVerboseJson
      path: verbose-json-object
    - type: object
      key: TranscriptTextDeltaEvent
      path: transcript-text-delta-event
    - type: object
      key: TranscriptTextDoneEvent
      path: transcript-text-done-event
  - id: images
    title: Images
    description: |
      Given a prompt and/or an input image, the model will generate a new image.
      Related guide: [Image generation](/docs/guides/images)
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createImage
      path: create
    - type: endpoint
      key: createImageEdit
      path: createEdit
    - type: endpoint
      key: createImageVariation
      path: createVariation
    - type: object
      key: ImagesResponse
      path: object
  - id: embeddings
    title: Embeddings
    description: |
      Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms.
      Related guide: [Embeddings](/docs/guides/embeddings)
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createEmbedding
      path: create
    - type: object
      key: Embedding
      path: object
  - id: evals
    title: Evals
    description: |
      Create, manage, and run evals in the OpenAI platform.
      Related guide: [Evals](/docs/guides/evals)
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createEval
      path: create
    - type: endpoint
      key: getEval
      path: get
    - type: endpoint
      key: updateEval
      path: update
    - type: endpoint
      key: deleteEval
      path: delete
    - type: endpoint
      key: listEvals
      path: list
    - type: endpoint
      key: getEvalRuns
      path: getRuns
    - type: endpoint
      key: getEvalRun
      path: getRun
    - type: endpoint
      key: createEvalRun
      path: createRun
    - type: endpoint
      key: cancelEvalRun
      path: cancelRun
    - type: endpoint
      key: deleteEvalRun
      path: deleteRun
    - type: endpoint
      key: getEvalRunOutputItem
      path: getRunOutputItem
    - type: endpoint
      key: getEvalRunOutputItems
      path: getRunOutputItems
    - type: object
      key: Eval
      path: object
    - type: object
      key: EvalRun
      path: run-object
    - type: object
      key: EvalRunOutputItem
      path: run-output-item-object
  - id: fine-tuning
    title: Fine-tuning
    description: |
      Manage fine-tuning jobs to tailor a model to your specific training data.
      Related guide: [Fine-tune models](/docs/guides/fine-tuning)
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createFineTuningJob
      path: create
    - type: endpoint
      key: listPaginatedFineTuningJobs
      path: list
    - type: endpoint
      key: listFineTuningEvents
      path: list-events
    - type: endpoint
      key: listFineTuningJobCheckpoints
      path: list-checkpoints
    - type: endpoint
      key: listFineTuningCheckpointPermissions
      path: list-permissions
    - type: endpoint
      key: createFineTuningCheckpointPermission
      path: create-permission
    - type: endpoint
      key: deleteFineTuningCheckpointPermission
      path: delete-permission
    - type: endpoint
      key: retrieveFineTuningJob
      path: retrieve
    - type: endpoint
      key: cancelFineTuningJob
      path: cancel
    - type: object
      key: FineTuneChatRequestInput
      path: chat-input
    - type: object
      key: FineTunePreferenceRequestInput
      path: preference-input
    - type: object
      key: FineTuneCompletionRequestInput
      path: completions-input
    - type: object
      key: FineTuningJob
      path: object
    - type: object
      key: FineTuningJobEvent
      path: event-object
    - type: object
      key: FineTuningJobCheckpoint
      path: checkpoint-object
    - type: object
      key: FineTuningCheckpointPermission
      path: permission-object
  - id: batch
    title: Batch
    description: |
      Create large batches of API requests for asynchronous processing. The Batch API returns completions within 24 hours for a 50% discount.
      Related guide: [Batch](/docs/guides/batch)
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createBatch
      path: create
    - type: endpoint
      key: retrieveBatch
      path: retrieve
    - type: endpoint
      key: cancelBatch
      path: cancel
    - type: endpoint
      key: listBatches
      path: list
    - type: object
      key: Batch
      path: object
    - type: object
      key: BatchRequestInput
      path: request-input
    - type: object
      key: BatchRequestOutput
      path: request-output
  - id: files
    title: Files
    description: |
      Files are used to upload documents that can be used with features like [Assistants](/docs/api-reference/assistants), [Fine-tuning](/docs/api-reference/fine-tuning), and [Batch API](/docs/guides/batch).
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createFile
      path: create
    - type: endpoint
      key: listFiles
      path: list
    - type: endpoint
      key: retrieveFile
      path: retrieve
    - type: endpoint
      key: deleteFile
      path: delete
    - type: endpoint
      key: downloadFile
      path: retrieve-contents
    - type: object
      key: OpenAIFile
      path: object
  - id: uploads
    title: Uploads
    description: |
      Allows you to upload large files in multiple parts.
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createUpload
      path: create
    - type: endpoint
      key: addUploadPart
      path: add-part
    - type: endpoint
      key: completeUpload
      path: complete
    - type: endpoint
      key: cancelUpload
      path: cancel
    - type: object
      key: Upload
      path: object
    - type: object
      key: UploadPart
      path: part-object
  - id: models
    title: Models
    description: |
      List and describe the various models available in the API. You can refer to the [Models](/docs/models) documentation to understand what models are available and the differences between them.
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: listModels
      path: list
    - type: endpoint
      key: retrieveModel
      path: retrieve
    - type: endpoint
      key: deleteModel
      path: delete
    - type: object
      key: Model
      path: object
  - id: moderations
    title: Moderations
    description: |
      Given text and/or image inputs, classifies if those inputs are potentially harmful across several categories.
      Related guide: [Moderations](/docs/guides/moderation)
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createModeration
      path: create
    - type: object
      key: CreateModerationResponse
      path: object
  - id: vector-stores
    title: Vector stores
    description: |
      Vector stores power semantic search for the Retrieval API and the `file_search` tool in the Responses and Assistants APIs.

      Related guide: [File Search](/docs/assistants/tools/file-search)
    navigationGroup: vector_stores
    sections:
    - type: endpoint
      key: createVectorStore
      path: create
    - type: endpoint
      key: listVectorStores
      path: list
    - type: endpoint
      key: getVectorStore
      path: retrieve
    - type: endpoint
      key: modifyVectorStore
      path: modify
    - type: endpoint
      key: deleteVectorStore
      path: delete
    - type: endpoint
      key: searchVectorStore
      path: search
    - type: object
      key: VectorStoreObject
      path: object
  - id: vector-stores-files
    title: Vector store files
    description: |
      Vector store files represent files inside a vector store.

      Related guide: [File Search](/docs/assistants/tools/file-search)
    navigationGroup: vector_stores
    sections:
    - type: endpoint
      key: createVectorStoreFile
      path: createFile
    - type: endpoint
      key: listVectorStoreFiles
      path: listFiles
    - type: endpoint
      key: getVectorStoreFile
      path: getFile
    - type: endpoint
      key: retrieveVectorStoreFileContent
      path: getContent
    - type: endpoint
      key: updateVectorStoreFileAttributes
      path: updateAttributes
    - type: endpoint
      key: deleteVectorStoreFile
      path: deleteFile
    - type: object
      key: VectorStoreFileObject
      path: file-object
  - id: vector-stores-file-batches
    title: Vector store file batches
    description: |
      Vector store file batches represent operations to add multiple files to a vector store.
      Related guide: [File Search](/docs/assistants/tools/file-search)
    navigationGroup: vector_stores
    sections:
    - type: endpoint
      key: createVectorStoreFileBatch
      path: createBatch
    - type: endpoint
      key: getVectorStoreFileBatch
      path: getBatch
    - type: endpoint
      key: cancelVectorStoreFileBatch
      path: cancelBatch
    - type: endpoint
      key: listFilesInVectorStoreBatch
      path: listBatchFiles
    - type: object
      key: VectorStoreFileBatchObject
      path: batch-object
  - id: assistants
    title: Assistants
    beta: true
    description: |
      Build assistants that can call models and use tools to perform tasks.

      [Get started with the Assistants API](/docs/assistants)
    navigationGroup: assistants
    sections:
    - type: endpoint
      key: createAssistant
      path: createAssistant
    - type: endpoint
      key: listAssistants
      path: listAssistants
    - type: endpoint
      key: getAssistant
      path: getAssistant
    - type: endpoint
      key: modifyAssistant
      path: modifyAssistant
    - type: endpoint
      key: deleteAssistant
      path: deleteAssistant
    - type: object
      key: AssistantObject
      path: object
  - id: threads
    title: Threads
    beta: true
    description: |
      Create threads that assistants can interact with.

      Related guide: [Assistants](/docs/assistants/overview)
    navigationGroup: assistants
    sections:
    - type: endpoint
      key: createThread
      path: createThread
    - type: endpoint
      key: getThread
      path: getThread
    - type: endpoint
      key: modifyThread
      path: modifyThread
    - type: endpoint
      key: deleteThread
      path: deleteThread
    - type: object
      key: ThreadObject
      path: object
  - id: messages
    title: Messages
    beta: true
    description: |
      Create messages within threads

      Related guide: [Assistants](/docs/assistants/overview)
    navigationGroup: assistants
    sections:
    - type: endpoint
      key: createMessage
      path: createMessage
    - type: endpoint
      key: listMessages
      path: listMessages
    - type: endpoint
      key: getMessage
      path: getMessage
    - type: endpoint
      key: modifyMessage
      path: modifyMessage
    - type: endpoint
      key: deleteMessage
      path: deleteMessage
    - type: object
      key: MessageObject
      path: object
  - id: runs
    title: Runs
    beta: true
    description: |
      Represents an execution run on a thread.

      Related guide: [Assistants](/docs/assistants/overview)
    navigationGroup: assistants
    sections:
    - type: endpoint
      key: createRun
      path: createRun
    - type: endpoint
      key: createThreadAndRun
      path: createThreadAndRun
    - type: endpoint
      key: listRuns
      path: listRuns
    - type: endpoint
      key: getRun
      path: getRun
    - type: endpoint
      key: modifyRun
      path: modifyRun
    - type: endpoint
      key: submitToolOuputsToRun
      path: submitToolOutputs
    - type: endpoint
      key: cancelRun
      path: cancelRun
    - type: object
      key: RunObject
      path: object
  - id: run-steps
    title: Run steps
    beta: true
    description: |
      Represents the steps (model and tool calls) taken during the run.

      Related guide: [Assistants](/docs/assistants/overview)
    navigationGroup: assistants
    sections:
    - type: endpoint
      key: listRunSteps
      path: listRunSteps
    - type: endpoint
      key: getRunStep
      path: getRunStep
    - type: object
      key: RunStepObject
      path: step-object
  - id: assistants-streaming
    title: Streaming
    beta: true
    description: |
      Stream the result of executing a Run or resuming a Run after submitting tool outputs.
      You can stream events from the [Create Thread and Run](/docs/api-reference/runs/createThreadAndRun),
      [Create Run](/docs/api-reference/runs/createRun), and [Submit Tool Outputs](/docs/api-reference/runs/submitToolOutputs)
      endpoints by passing `"stream": true`. The response will be a [Server-Sent events](https://html.spec.whatwg.org/multipage/server-sent-events.html#server-sent-events) stream.
      Our Node and Python SDKs provide helpful utilities to make streaming easy. Reference the
      [Assistants API quickstart](/docs/assistants/overview) to learn more.
    navigationGroup: assistants
    sections:
    - type: object
      key: MessageDeltaObject
      path: message-delta-object
    - type: object
      key: RunStepDeltaObject
      path: run-step-delta-object
    - type: object
      key: AssistantStreamEvent
      path: events
  - id: administration
    title: Administration
    description: |
      Programmatically manage your organization.
      The Audit Logs endpoint provides a log of all actions taken in the organization for security and monitoring purposes.
      To access these endpoints please generate an Admin API Key through the [API Platform Organization overview](/organization/admin-keys). Admin API keys cannot be used for non-administration endpoints.
      For best practices on setting up your organization, please refer to this [guide](/docs/guides/production-best-practices#setting-up-your-organization)
    navigationGroup: administration
  - id: admin-api-keys
    title: Admin API Keys
    description: |
      Admin API keys enable Organization Owners to programmatically manage various aspects of their organization, including users, projects, and API keys. These keys provide administrative capabilities, such as creating, updating, and deleting users; managing projects; and overseeing API key lifecycles.

      Key Features of Admin API Keys:

      - User Management: Invite new users, update roles, and remove users from the organization.

      - Project Management: Create, update, archive projects, and manage user assignments within projects.

      - API Key Oversight: List, retrieve, and delete API keys associated with projects.

      Only Organization Owners have the authority to create and utilize Admin API keys. To manage these keys, Organization Owners can navigate to the Admin Keys section of their API Platform dashboard.

      For direct access to the Admin Keys management page, Organization Owners can use the following link:

      [https://platform.openai.com/settings/organization/admin-keys](https://platform.openai.com/settings/organization/admin-keys)

      It's crucial to handle Admin API keys with care due to their elevated permissions. Adhering to best practices, such as regular key rotation and assigning appropriate permissions, enhances security and ensures proper governance within the organization.
    navigationGroup: administration
    sections:
    - type: endpoint
      key: admin-api-keys-list
      path: list
    - type: endpoint
      key: admin-api-keys-create
      path: create
    - type: endpoint
      key: admin-api-keys-get
      path: listget
    - type: endpoint
      key: admin-api-keys-delete
      path: delete
    - type: object
      key: AdminApiKey
      path: object
  - id: invite
    title: Invites
    description: Invite and manage invitations for an organization.
    navigationGroup: administration
    sections:
    - type: endpoint
      key: list-invites
      path: list
    - type: endpoint
      key: inviteUser
      path: create
    - type: endpoint
      key: retrieve-invite
      path: retrieve
    - type: endpoint
      key: delete-invite
      path: delete
    - type: object
      key: Invite
      path: object
  - id: users
    title: Users
    description: |
      Manage users and their role in an organization.
    navigationGroup: administration
    sections:
    - type: endpoint
      key: list-users
      path: list
    - type: endpoint
      key: modify-user
      path: modify
    - type: endpoint
      key: retrieve-user
      path: retrieve
    - type: endpoint
      key: delete-user
      path: delete
    - type: object
      key: User
      path: object
  - id: projects
    title: Projects
    description: |
      Manage the projects within an orgnanization includes creation, updating, and archiving or projects.
      The Default project cannot be archived.
    navigationGroup: administration
    sections:
    - type: endpoint
      key: list-projects
      path: list
    - type: endpoint
      key: create-project
      path: create
    - type: endpoint
      key: retrieve-project
      path: retrieve
    - type: endpoint
      key: modify-project
      path: modify
    - type: endpoint
      key: archive-project
      path: archive
    - type: object
      key: Project
      path: object
  - id: project-users
    title: Project users
    description: |
      Manage users within a project, including adding, updating roles, and removing users.
    navigationGroup: administration
    sections:
    - type: endpoint
      key: list-project-users
      path: list
    - type: endpoint
      key: create-project-user
      path: creeate
    - type: endpoint
      key: retrieve-project-user
      path: retrieve
    - type: endpoint
      key: modify-project-user
      path: modify
    - type: endpoint
      key: delete-project-user
      path: delete
    - type: object
      key: ProjectUser
      path: object
  - id: project-service-accounts
    title: Project service accounts
    description: |
      Manage service accounts within a project. A service account is a bot user that is not associated with a user.
      If a user leaves an organization, their keys and membership in projects will no longer work. Service accounts
      do not have this limitation. However, service accounts can also be deleted from a project.
    navigationGroup: administration
    sections:
    - type: endpoint
      key: list-project-service-accounts
      path: list
    - type: endpoint
      key: create-project-service-account
      path: create
    - type: endpoint
      key: retrieve-project-service-account
      path: retrieve
    - type: endpoint
      key: delete-project-service-account
      path: delete
    - type: object
      key: ProjectServiceAccount
      path: object
  - id: project-api-keys
    title: Project API keys
    description: |
      Manage API keys for a given project. Supports listing and deleting keys for users.
      This API does not allow issuing keys for users, as users need to authorize themselves to generate keys.
    navigationGroup: administration
    sections:
    - type: endpoint
      key: list-project-api-keys
      path: list
    - type: endpoint
      key: retrieve-project-api-key
      path: retrieve
    - type: endpoint
      key: delete-project-api-key
      path: delete
    - type: object
      key: ProjectApiKey
      path: object
  - id: project-rate-limits
    title: Project rate limits
    description: |
      Manage rate limits per model for projects. Rate limits may be configured to be equal to or lower than the organization's rate limits.
    navigationGroup: administration
    sections:
    - type: endpoint
      key: list-project-rate-limits
      path: list
    - type: endpoint
      key: update-project-rate-limits
      path: update
    - type: object
      key: ProjectRateLimit
      path: object
  - id: audit-logs
    title: Audit logs
    description: |
      Logs of user actions and configuration changes within this organization.
      To log events, you must activate logging in the [Organization Settings](/settings/organization/general).
      Once activated, for security reasons, logging cannot be deactivated.
    navigationGroup: administration
    sections:
    - type: endpoint
      key: list-audit-logs
      path: list
    - type: object
      key: AuditLog
      path: object
  - id: usage
    title: Usage
    description: |
      The **Usage API** provides detailed insights into your activity across the OpenAI API. It also includes a separate [Costs endpoint](/docs/api-reference/usage/costs), which offers visibility into your spend, breaking down consumption by invoice line items and project IDs.

      While the Usage API delivers granular usage data, it may not always reconcile perfectly with the Costs due to minor differences in how usage and spend are recorded. For financial purposes, we recommend using the [Costs endpoint](/docs/api-reference/usage/costs) or the [Costs tab](/settings/organization/usage) in the Usage Dashboard, which will reconcile back to your billing invoice.
    navigationGroup: administration
    sections:
    - type: endpoint
      key: usage-completions
      path: completions
    - type: object
      key: UsageCompletionsResult
      path: completions_object
    - type: endpoint
      key: usage-embeddings
      path: embeddings
    - type: object
      key: UsageEmbeddingsResult
      path: embeddings_object
    - type: endpoint
      key: usage-moderations
      path: moderations
    - type: object
      key: UsageModerationsResult
      path: moderations_object
    - type: endpoint
      key: usage-images
      path: images
    - type: object
      key: UsageImagesResult
      path: images_object
    - type: endpoint
      key: usage-audio-speeches
      path: audio_speeches
    - type: object
      key: UsageAudioSpeechesResult
      path: audio_speeches_object
    - type: endpoint
      key: usage-audio-transcriptions
      path: audio_transcriptions
    - type: object
      key: UsageAudioTranscriptionsResult
      path: audio_transcriptions_object
    - type: endpoint
      key: usage-vector-stores
      path: vector_stores
    - type: object
      key: UsageVectorStoresResult
      path: vector_stores_object
    - type: endpoint
      key: usage-code-interpreter-sessions
      path: code_interpreter_sessions
    - type: object
      key: UsageCodeInterpreterSessionsResult
      path: code_interpreter_sessions_object
    - type: endpoint
      key: usage-costs
      path: costs
    - type: object
      key: CostsResult
      path: costs_object
  - id: certificates
    beta: true
    title: Certificates
    description: |
      Manage Mutual TLS certificates across your organization and projects.

      [Learn more about Mutual TLS.](https://help.openai.com/en/articles/10876024-openai-mutual-tls-beta-program)
    navigationGroup: administration
    sections:
    - type: endpoint
      key: uploadCertificate
      path: uploadCertificate
    - type: endpoint
      key: getCertificate
      path: getCertificate
    - type: endpoint
      key: modifyCertificate
      path: modifyCertificate
    - type: endpoint
      key: deleteCertificate
      path: deleteCertificate
    - type: endpoint
      key: listOrganizationCertificates
      path: listOrganizationCertificates
    - type: endpoint
      key: listProjectCertificates
      path: listProjectCertificates
    - type: endpoint
      key: activateOrganizationCertificates
      path: activateOrganizationCertificates
    - type: endpoint
      key: deactivateOrganizationCertificates
      path: deactivateOrganizationCertificates
    - type: endpoint
      key: activateProjectCertificates
      path: activateProjectCertificates
    - type: endpoint
      key: deactivateProjectCertificates
      path: deactivateProjectCertificates
    - type: object
      key: Certificate
      path: object
  - id: completions
    title: Completions
    legacy: true
    navigationGroup: legacy
    description: |
      Given a prompt, the model will return one or more predicted completions along with the probabilities of alternative tokens at each position. Most developer should use our [Chat Completions API](/docs/guides/text-generation#text-generation-models) to leverage our best and newest models.
    sections:
    - type: endpoint
      key: createCompletion
      path: create
    - type: object
      key: CreateCompletionResponse
      path: object
