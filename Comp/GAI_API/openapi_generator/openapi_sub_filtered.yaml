openapi: 3.0.0
info:
  title: OpenAI API
  description: The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference
    for more details.
  version: 2.3.0
  termsOfService: https://openai.com/policies/terms-of-use
  contact:
    name: OpenAI Support
    url: https://help.openai.com/
  license:
    name: MIT
    url: https://github.com/openai/openai-openapi/blob/master/LICENSE
servers:
- url: https://api.openai.com/v1
security:
- ApiKeyAuth: []
tags:
- name: Assistants
  description: Build Assistants that can call models and use tools.
- name: Audio
  description: Turn audio into text or text into audio.
- name: Chat
  description: Given a list of messages comprising a conversation, the model will
    return a response.
- name: Completions
  description: Given a prompt, the model will return one or more predicted completions,
    and can also return the probabilities of alternative tokens at each position.
- name: Embeddings
  description: Get a vector representation of a given input that can be easily consumed
    by machine learning models and algorithms.
- name: Evals
  description: Manage and run evals in the OpenAI platform.
- name: Fine-tuning
  description: Manage fine-tuning jobs to tailor a model to your specific training
    data.
- name: Batch
  description: Create large batches of API requests to run asynchronously.
- name: Files
  description: Files are used to upload documents that can be used with features like
    Assistants and Fine-tuning.
- name: Uploads
  description: Use Uploads to upload large files in multiple parts.
- name: Images
  description: Given a prompt and/or an input image, the model will generate a new
    image.
- name: Models
  description: List and describe the various models available in the API.
- name: Moderations
  description: Given text and/or image inputs, classifies if those inputs are potentially
    harmful.
- name: Audit Logs
  description: List user actions and configuration changes within this organization.
paths:
  /chat/completions:
    get:
      operationId: listChatCompletions
      tags:
      - Chat
      summary: 'List stored Chat Completions. Only Chat Completions that have been
        stored

        with the `store` parameter set to `true` will be returned.

        '
      parameters:
      - name: model
        in: query
        description: The model used to generate the Chat Completions.
        required: false
        schema:
          type: string
      - name: metadata
        in: query
        description: 'A list of metadata keys to filter the Chat Completions by. Example:


          `metadata[key1]=value1&metadata[key2]=value2`

          '
        required: false
        schema:
          $ref: '#/components/schemas/Metadata'
      - name: after
        in: query
        description: Identifier for the last chat completion from the previous pagination
          request.
        required: false
        schema:
          type: string
      - name: limit
        in: query
        description: Number of Chat Completions to retrieve.
        required: false
        schema:
          type: integer
          default: 20
      - name: order
        in: query
        description: Sort order for Chat Completions by timestamp. Use `asc` for ascending
          order or `desc` for descending order. Defaults to `asc`.
        required: false
        schema:
          type: string
          enum:
          - asc
          - desc
          default: asc
      responses:
        '200':
          description: A list of Chat Completions
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ChatCompletionList'
      x-oaiMeta:
        name: List Chat Completions
        group: chat
        returns: A list of [Chat Completions](/docs/api-reference/chat/list-object)
          matching the specified filters.
        path: list
        examples:
          request:
            curl: "curl https://api.openai.com/v1/chat/completions \\\n  -H \"Authorization:\
              \ Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type: application/json\"\
              \n"
            python: 'from openai import OpenAI

              client = OpenAI()


              completions = client.chat.completions.list()

              print(completions)

              '
          response: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\"\
            : \"chat.completion\",\n      \"id\": \"chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2\"\
            ,\n      \"model\": \"gpt-4.1-2025-04-14\",\n      \"created\": 1738960610,\n\
            \      \"request_id\": \"req_ded8ab984ec4bf840f37566c1011c417\",\n   \
            \   \"tool_choice\": null,\n      \"usage\": {\n        \"total_tokens\"\
            : 31,\n        \"completion_tokens\": 18,\n        \"prompt_tokens\":\
            \ 13\n      },\n      \"seed\": 4944116822809979520,\n      \"top_p\"\
            : 1.0,\n      \"temperature\": 1.0,\n      \"presence_penalty\": 0.0,\n\
            \      \"frequency_penalty\": 0.0,\n      \"system_fingerprint\": \"fp_50cad350e4\"\
            ,\n      \"input_user\": null,\n      \"service_tier\": \"default\",\n\
            \      \"tools\": null,\n      \"metadata\": {},\n      \"choices\": [\n\
            \        {\n          \"index\": 0,\n          \"message\": {\n      \
            \      \"content\": \"Mind of circuits hum,  \\nLearning patterns in silence—\
            \  \\nFuture's quiet spark.\",\n            \"role\": \"assistant\",\n\
            \            \"tool_calls\": null,\n            \"function_call\": null\n\
            \          },\n          \"finish_reason\": \"stop\",\n          \"logprobs\"\
            : null\n        }\n      ],\n      \"response_format\": null\n    }\n\
            \  ],\n  \"first_id\": \"chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2\",\n \
            \ \"last_id\": \"chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2\",\n  \"has_more\"\
            : false\n}\n"
    post:
      operationId: createChatCompletion
      tags:
      - Chat
      summary: "**Starting a new project?** We recommend trying [Responses](/docs/api-reference/responses)\
        \ \nto take advantage of the latest OpenAI platform features. Compare\n[Chat\
        \ Completions with Responses](/docs/guides/responses-vs-chat-completions?api-mode=responses).\n\
        \n---\n\nCreates a model response for the given chat conversation. Learn more\
        \ in the\n[text generation](/docs/guides/text-generation), [vision](/docs/guides/vision),\n\
        and [audio](/docs/guides/audio) guides.\n\nParameter support can differ depending\
        \ on the model used to generate the\nresponse, particularly for newer reasoning\
        \ models. Parameters that are only\nsupported for reasoning models are noted\
        \ below. For the current state of \nunsupported parameters in reasoning models,\
        \ \n[refer to the reasoning guide](/docs/guides/reasoning).\n"
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateChatCompletionRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateChatCompletionResponse'
            text/event-stream:
              schema:
                $ref: '#/components/schemas/CreateChatCompletionStreamResponse'
      x-oaiMeta:
        name: Create chat completion
        group: chat
        returns: 'Returns a [chat completion](/docs/api-reference/chat/object) object,
          or a streamed sequence of [chat completion chunk](/docs/api-reference/chat/streaming)
          objects if the request is streamed.

          '
        path: create
        examples:
        - title: Default
          request:
            curl: "curl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\
              \ \\\n  -d '{\n    \"model\": \"VAR_chat_model_id\",\n    \"messages\"\
              : [\n      {\n        \"role\": \"developer\",\n        \"content\"\
              : \"You are a helpful assistant.\"\n      },\n      {\n        \"role\"\
              : \"user\",\n        \"content\": \"Hello!\"\n      }\n    ]\n  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\ncompletion =\
              \ client.chat.completions.create(\n  model=\"VAR_chat_model_id\",\n\
              \  messages=[\n    {\"role\": \"developer\", \"content\": \"You are\
              \ a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"\
              Hello!\"}\n  ]\n)\n\nprint(completion.choices[0].message)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const completion = await openai.chat.completions.create({\n\
              \    messages: [{ role: \"developer\", content: \"You are a helpful\
              \ assistant.\" }],\n    model: \"VAR_chat_model_id\",\n    store: true,\n\
              \  });\n\n  console.log(completion.choices[0]);\n}\n\nmain();\n"
            csharp: "using System;\nusing System.Collections.Generic;\n\nusing OpenAI.Chat;\n\
              \nChatClient client = new(\n    model: \"gpt-4.1\",\n    apiKey: Environment.GetEnvironmentVariable(\"\
              OPENAI_API_KEY\")\n);\n\nList<ChatMessage> messages =\n[\n    new SystemChatMessage(\"\
              You are a helpful assistant.\"),\n    new UserChatMessage(\"Hello!\"\
              )\n];\n\nChatCompletion completion = client.CompleteChat(messages);\n\
              \nConsole.WriteLine(completion.Content[0].Text);\n"
          response: "{\n  \"id\": \"chatcmpl-B9MBs8CjcvOU2jLn4n570S5qMJKcT\",\n  \"\
            object\": \"chat.completion\",\n  \"created\": 1741569952,\n  \"model\"\
            : \"gpt-4.1-2025-04-14\",\n  \"choices\": [\n    {\n      \"index\": 0,\n\
            \      \"message\": {\n        \"role\": \"assistant\",\n        \"content\"\
            : \"Hello! How can I assist you today?\",\n        \"refusal\": null,\n\
            \        \"annotations\": []\n      },\n      \"logprobs\": null,\n  \
            \    \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"\
            prompt_tokens\": 19,\n    \"completion_tokens\": 10,\n    \"total_tokens\"\
            : 29,\n    \"prompt_tokens_details\": {\n      \"cached_tokens\": 0,\n\
            \      \"audio_tokens\": 0\n    },\n    \"completion_tokens_details\"\
            : {\n      \"reasoning_tokens\": 0,\n      \"audio_tokens\": 0,\n    \
            \  \"accepted_prediction_tokens\": 0,\n      \"rejected_prediction_tokens\"\
            : 0\n    }\n  },\n  \"service_tier\": \"default\"\n}\n"
        - title: Image input
          request:
            curl: "curl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\
              \ \\\n  -d '{\n    \"model\": \"gpt-4.1\",\n    \"messages\": [\n  \
              \    {\n        \"role\": \"user\",\n        \"content\": [\n      \
              \    {\n            \"type\": \"text\",\n            \"text\": \"What\
              \ is in this image?\"\n          },\n          {\n            \"type\"\
              : \"image_url\",\n            \"image_url\": {\n              \"url\"\
              : \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\
              \n            }\n          }\n        ]\n      }\n    ],\n    \"max_tokens\"\
              : 300\n  }'\n"
            python: "from openai import OpenAI\n\nclient = OpenAI()\n\nresponse =\
              \ client.chat.completions.create(\n    model=\"gpt-4.1\",\n    messages=[\n\
              \        {\n            \"role\": \"user\",\n            \"content\"\
              : [\n                {\"type\": \"text\", \"text\": \"What's in this\
              \ image?\"},\n                {\n                    \"type\": \"image_url\"\
              ,\n                    \"image_url\": {\n                        \"\
              url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\
              ,\n                    }\n                },\n            ],\n     \
              \   }\n    ],\n    max_tokens=300,\n)\n\nprint(response.choices[0])\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const response = await openai.chat.completions.create({\n\
              \    model: \"gpt-4.1\",\n    messages: [\n      {\n        role: \"\
              user\",\n        content: [\n          { type: \"text\", text: \"What's\
              \ in this image?\" },\n          {\n            type: \"image_url\"\
              ,\n            image_url: {\n              \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\
              ,\n            },\n          }\n        ],\n      },\n    ],\n  });\n\
              \  console.log(response.choices[0]);\n}\nmain();\n"
            csharp: "using System;\nusing System.Collections.Generic;\n\nusing OpenAI.Chat;\n\
              \nChatClient client = new(\n    model: \"gpt-4.1\",\n    apiKey: Environment.GetEnvironmentVariable(\"\
              OPENAI_API_KEY\")\n);\n\nList<ChatMessage> messages =\n[\n    new UserChatMessage(\n\
              \    [\n        ChatMessageContentPart.CreateTextPart(\"What's in this\
              \ image?\"),\n        ChatMessageContentPart.CreateImagePart(new Uri(\"\
              https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\
              ))\n    ])\n];\n\nChatCompletion completion = client.CompleteChat(messages);\n\
              \nConsole.WriteLine(completion.Content[0].Text);\n"
          response: "{\n  \"id\": \"chatcmpl-B9MHDbslfkBeAs8l4bebGdFOJ6PeG\",\n  \"\
            object\": \"chat.completion\",\n  \"created\": 1741570283,\n  \"model\"\
            : \"gpt-4.1-2025-04-14\",\n  \"choices\": [\n    {\n      \"index\": 0,\n\
            \      \"message\": {\n        \"role\": \"assistant\",\n        \"content\"\
            : \"The image shows a wooden boardwalk path running through a lush green\
            \ field or meadow. The sky is bright blue with some scattered clouds,\
            \ giving the scene a serene and peaceful atmosphere. Trees and shrubs\
            \ are visible in the background.\",\n        \"refusal\": null,\n    \
            \    \"annotations\": []\n      },\n      \"logprobs\": null,\n      \"\
            finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\"\
            : 1117,\n    \"completion_tokens\": 46,\n    \"total_tokens\": 1163,\n\
            \    \"prompt_tokens_details\": {\n      \"cached_tokens\": 0,\n     \
            \ \"audio_tokens\": 0\n    },\n    \"completion_tokens_details\": {\n\
            \      \"reasoning_tokens\": 0,\n      \"audio_tokens\": 0,\n      \"\
            accepted_prediction_tokens\": 0,\n      \"rejected_prediction_tokens\"\
            : 0\n    }\n  },\n  \"service_tier\": \"default\"\n}\n"
        - title: Streaming
          request:
            curl: "curl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\
              \ \\\n  -d '{\n    \"model\": \"VAR_chat_model_id\",\n    \"messages\"\
              : [\n      {\n        \"role\": \"developer\",\n        \"content\"\
              : \"You are a helpful assistant.\"\n      },\n      {\n        \"role\"\
              : \"user\",\n        \"content\": \"Hello!\"\n      }\n    ],\n    \"\
              stream\": true\n  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\ncompletion =\
              \ client.chat.completions.create(\n  model=\"VAR_chat_model_id\",\n\
              \  messages=[\n    {\"role\": \"developer\", \"content\": \"You are\
              \ a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"\
              Hello!\"}\n  ],\n  stream=True\n)\n\nfor chunk in completion:\n  print(chunk.choices[0].delta)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const completion = await openai.chat.completions.create({\n\
              \    model: \"VAR_chat_model_id\",\n    messages: [\n      {\"role\"\
              : \"developer\", \"content\": \"You are a helpful assistant.\"},\n \
              \     {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    stream:\
              \ true,\n  });\n\n  for await (const chunk of completion) {\n    console.log(chunk.choices[0].delta.content);\n\
              \  }\n}\n\nmain();\n"
            csharp: "using System;\nusing System.ClientModel;\nusing System.Collections.Generic;\n\
              using System.Threading.Tasks;\n\nusing OpenAI.Chat;\n\nChatClient client\
              \ = new(\n    model: \"gpt-4.1\",\n    apiKey: Environment.GetEnvironmentVariable(\"\
              OPENAI_API_KEY\")\n);\n\nList<ChatMessage> messages =\n[\n    new SystemChatMessage(\"\
              You are a helpful assistant.\"),\n    new UserChatMessage(\"Hello!\"\
              )\n];\n\nAsyncCollectionResult<StreamingChatCompletionUpdate> completionUpdates\
              \ = client.CompleteChatStreamingAsync(messages);\n\nawait foreach (StreamingChatCompletionUpdate\
              \ completionUpdate in completionUpdates)\n{\n    if (completionUpdate.ContentUpdate.Count\
              \ > 0)\n    {\n        Console.Write(completionUpdate.ContentUpdate[0].Text);\n\
              \    }\n}\n"
          response: '{"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
            "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}


            {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
            "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"content":"Hello"},"logprobs":null,"finish_reason":null}]}


            ....


            {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
            "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{},"logprobs":null,"finish_reason":"stop"}]}

            '
        - title: Functions
          request:
            curl: "curl https://api.openai.com/v1/chat/completions \\\n-H \"Content-Type:\
              \ application/json\" \\\n-H \"Authorization: Bearer $OPENAI_API_KEY\"\
              \ \\\n-d '{\n  \"model\": \"gpt-4.1\",\n  \"messages\": [\n    {\n \
              \     \"role\": \"user\",\n      \"content\": \"What is the weather\
              \ like in Boston today?\"\n    }\n  ],\n  \"tools\": [\n    {\n    \
              \  \"type\": \"function\",\n      \"function\": {\n        \"name\"\
              : \"get_current_weather\",\n        \"description\": \"Get the current\
              \ weather in a given location\",\n        \"parameters\": {\n      \
              \    \"type\": \"object\",\n          \"properties\": {\n          \
              \  \"location\": {\n              \"type\": \"string\",\n          \
              \    \"description\": \"The city and state, e.g. San Francisco, CA\"\
              \n            },\n            \"unit\": {\n              \"type\": \"\
              string\",\n              \"enum\": [\"celsius\", \"fahrenheit\"]\n \
              \           }\n          },\n          \"required\": [\"location\"]\n\
              \        }\n      }\n    }\n  ],\n  \"tool_choice\": \"auto\"\n}'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\ntools = [\n \
              \ {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\"\
              : \"get_current_weather\",\n      \"description\": \"Get the current\
              \ weather in a given location\",\n      \"parameters\": {\n        \"\
              type\": \"object\",\n        \"properties\": {\n          \"location\"\
              : {\n            \"type\": \"string\",\n            \"description\"\
              : \"The city and state, e.g. San Francisco, CA\",\n          },\n  \
              \        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"\
              fahrenheit\"]},\n        },\n        \"required\": [\"location\"],\n\
              \      },\n    }\n  }\n]\nmessages = [{\"role\": \"user\", \"content\"\
              : \"What's the weather like in Boston today?\"}]\ncompletion = client.chat.completions.create(\n\
              \  model=\"VAR_chat_model_id\",\n  messages=messages,\n  tools=tools,\n\
              \  tool_choice=\"auto\"\n)\n\nprint(completion)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const messages = [{\"role\": \"user\",\
              \ \"content\": \"What's the weather like in Boston today?\"}];\n  const\
              \ tools = [\n      {\n        \"type\": \"function\",\n        \"function\"\
              : {\n          \"name\": \"get_current_weather\",\n          \"description\"\
              : \"Get the current weather in a given location\",\n          \"parameters\"\
              : {\n            \"type\": \"object\",\n            \"properties\":\
              \ {\n              \"location\": {\n                \"type\": \"string\"\
              ,\n                \"description\": \"The city and state, e.g. San Francisco,\
              \ CA\",\n              },\n              \"unit\": {\"type\": \"string\"\
              , \"enum\": [\"celsius\", \"fahrenheit\"]},\n            },\n      \
              \      \"required\": [\"location\"],\n          },\n        }\n    \
              \  }\n  ];\n\n  const response = await openai.chat.completions.create({\n\
              \    model: \"gpt-4.1\",\n    messages: messages,\n    tools: tools,\n\
              \    tool_choice: \"auto\",\n  });\n\n  console.log(response);\n}\n\n\
              main();\n"
            csharp: "using System;\nusing System.Collections.Generic;\n\nusing OpenAI.Chat;\n\
              \nChatClient client = new(\n    model: \"gpt-4.1\",\n    apiKey: Environment.GetEnvironmentVariable(\"\
              OPENAI_API_KEY\")\n);\n\nChatTool getCurrentWeatherTool = ChatTool.CreateFunctionTool(\n\
              \    functionName: \"get_current_weather\",\n    functionDescription:\
              \ \"Get the current weather in a given location\",\n    functionParameters:\
              \ BinaryData.FromString(\"\"\"\n        {\n            \"type\": \"\
              object\",\n            \"properties\": {\n                \"location\"\
              : {\n                    \"type\": \"string\",\n                   \
              \ \"description\": \"The city and state, e.g. San Francisco, CA\"\n\
              \                },\n                \"unit\": {\n                 \
              \   \"type\": \"string\",\n                    \"enum\": [ \"celsius\"\
              , \"fahrenheit\" ]\n                }\n            },\n            \"\
              required\": [ \"location\" ]\n        }\n    \"\"\")\n);\n\nList<ChatMessage>\
              \ messages =\n[\n    new UserChatMessage(\"What's the weather like in\
              \ Boston today?\"),\n];\n\nChatCompletionOptions options = new()\n{\n\
              \    Tools =\n    {\n        getCurrentWeatherTool\n    },\n    ToolChoice\
              \ = ChatToolChoice.CreateAutoChoice(),\n};\n\nChatCompletion completion\
              \ = client.CompleteChat(messages, options);\n"
          response: "{\n  \"id\": \"chatcmpl-abc123\",\n  \"object\": \"chat.completion\"\
            ,\n  \"created\": 1699896916,\n  \"model\": \"gpt-4o-mini\",\n  \"choices\"\
            : [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\"\
            : \"assistant\",\n        \"content\": null,\n        \"tool_calls\":\
            \ [\n          {\n            \"id\": \"call_abc123\",\n            \"\
            type\": \"function\",\n            \"function\": {\n              \"name\"\
            : \"get_current_weather\",\n              \"arguments\": \"{\\n\\\"location\\\
            \": \\\"Boston, MA\\\"\\n}\"\n            }\n          }\n        ]\n\
            \      },\n      \"logprobs\": null,\n      \"finish_reason\": \"tool_calls\"\
            \n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 82,\n    \"completion_tokens\"\
            : 17,\n    \"total_tokens\": 99,\n    \"completion_tokens_details\": {\n\
            \      \"reasoning_tokens\": 0,\n      \"accepted_prediction_tokens\"\
            : 0,\n      \"rejected_prediction_tokens\": 0\n    }\n  }\n}\n"
        - title: Logprobs
          request:
            curl: "curl https://api.openai.com/v1/chat/completions \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\
              \ \\\n  -d '{\n    \"model\": \"VAR_chat_model_id\",\n    \"messages\"\
              : [\n      {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\
              \n      }\n    ],\n    \"logprobs\": true,\n    \"top_logprobs\": 2\n\
              \  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\ncompletion =\
              \ client.chat.completions.create(\n  model=\"VAR_chat_model_id\",\n\
              \  messages=[\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n \
              \ ],\n  logprobs=True,\n  top_logprobs=2\n)\n\nprint(completion.choices[0].message)\n\
              print(completion.choices[0].logprobs)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const completion = await openai.chat.completions.create({\n\
              \    messages: [{ role: \"user\", content: \"Hello!\" }],\n    model:\
              \ \"VAR_chat_model_id\",\n    logprobs: true,\n    top_logprobs: 2,\n\
              \  });\n\n  console.log(completion.choices[0]);\n}\n\nmain();\n"
            csharp: "using System;\nusing System.Collections.Generic;\n\nusing OpenAI.Chat;\n\
              \nChatClient client = new(\n    model: \"gpt-4.1\",\n    apiKey: Environment.GetEnvironmentVariable(\"\
              OPENAI_API_KEY\")\n);\n\nList<ChatMessage> messages =\n[\n    new UserChatMessage(\"\
              Hello!\")\n];\n\nChatCompletionOptions options = new()\n{\n    IncludeLogProbabilities\
              \ = true,\n    TopLogProbabilityCount = 2\n};\n\nChatCompletion completion\
              \ = client.CompleteChat(messages, options);\n\nConsole.WriteLine(completion.Content[0].Text);\n"
          response: "{\n  \"id\": \"chatcmpl-123\",\n  \"object\": \"chat.completion\"\
            ,\n  \"created\": 1702685778,\n  \"model\": \"gpt-4o-mini\",\n  \"choices\"\
            : [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\"\
            : \"assistant\",\n        \"content\": \"Hello! How can I assist you today?\"\
            \n      },\n      \"logprobs\": {\n        \"content\": [\n          {\n\
            \            \"token\": \"Hello\",\n            \"logprob\": -0.31725305,\n\
            \            \"bytes\": [72, 101, 108, 108, 111],\n            \"top_logprobs\"\
            : [\n              {\n                \"token\": \"Hello\",\n        \
            \        \"logprob\": -0.31725305,\n                \"bytes\": [72, 101,\
            \ 108, 108, 111]\n              },\n              {\n                \"\
            token\": \"Hi\",\n                \"logprob\": -1.3190403,\n         \
            \       \"bytes\": [72, 105]\n              }\n            ]\n       \
            \   },\n          {\n            \"token\": \"!\",\n            \"logprob\"\
            : -0.02380986,\n            \"bytes\": [\n              33\n         \
            \   ],\n            \"top_logprobs\": [\n              {\n           \
            \     \"token\": \"!\",\n                \"logprob\": -0.02380986,\n \
            \               \"bytes\": [33]\n              },\n              {\n \
            \               \"token\": \" there\",\n                \"logprob\": -3.787621,\n\
            \                \"bytes\": [32, 116, 104, 101, 114, 101]\n          \
            \    }\n            ]\n          },\n          {\n            \"token\"\
            : \" How\",\n            \"logprob\": -0.000054669687,\n            \"\
            bytes\": [32, 72, 111, 119],\n            \"top_logprobs\": [\n      \
            \        {\n                \"token\": \" How\",\n                \"logprob\"\
            : -0.000054669687,\n                \"bytes\": [32, 72, 111, 119]\n  \
            \            },\n              {\n                \"token\": \"<|end|>\"\
            ,\n                \"logprob\": -10.953937,\n                \"bytes\"\
            : null\n              }\n            ]\n          },\n          {\n  \
            \          \"token\": \" can\",\n            \"logprob\": -0.015801601,\n\
            \            \"bytes\": [32, 99, 97, 110],\n            \"top_logprobs\"\
            : [\n              {\n                \"token\": \" can\",\n         \
            \       \"logprob\": -0.015801601,\n                \"bytes\": [32, 99,\
            \ 97, 110]\n              },\n              {\n                \"token\"\
            : \" may\",\n                \"logprob\": -4.161023,\n               \
            \ \"bytes\": [32, 109, 97, 121]\n              }\n            ]\n    \
            \      },\n          {\n            \"token\": \" I\",\n            \"\
            logprob\": -3.7697225e-6,\n            \"bytes\": [\n              32,\n\
            \              73\n            ],\n            \"top_logprobs\": [\n \
            \             {\n                \"token\": \" I\",\n                \"\
            logprob\": -3.7697225e-6,\n                \"bytes\": [32, 73]\n     \
            \         },\n              {\n                \"token\": \" assist\"\
            ,\n                \"logprob\": -13.596657,\n                \"bytes\"\
            : [32, 97, 115, 115, 105, 115, 116]\n              }\n            ]\n\
            \          },\n          {\n            \"token\": \" assist\",\n    \
            \        \"logprob\": -0.04571125,\n            \"bytes\": [32, 97, 115,\
            \ 115, 105, 115, 116],\n            \"top_logprobs\": [\n            \
            \  {\n                \"token\": \" assist\",\n                \"logprob\"\
            : -0.04571125,\n                \"bytes\": [32, 97, 115, 115, 105, 115,\
            \ 116]\n              },\n              {\n                \"token\":\
            \ \" help\",\n                \"logprob\": -3.1089056,\n             \
            \   \"bytes\": [32, 104, 101, 108, 112]\n              }\n           \
            \ ]\n          },\n          {\n            \"token\": \" you\",\n   \
            \         \"logprob\": -5.4385737e-6,\n            \"bytes\": [32, 121,\
            \ 111, 117],\n            \"top_logprobs\": [\n              {\n     \
            \           \"token\": \" you\",\n                \"logprob\": -5.4385737e-6,\n\
            \                \"bytes\": [32, 121, 111, 117]\n              },\n  \
            \            {\n                \"token\": \" today\",\n             \
            \   \"logprob\": -12.807695,\n                \"bytes\": [32, 116, 111,\
            \ 100, 97, 121]\n              }\n            ]\n          },\n      \
            \    {\n            \"token\": \" today\",\n            \"logprob\": -0.0040071653,\n\
            \            \"bytes\": [32, 116, 111, 100, 97, 121],\n            \"\
            top_logprobs\": [\n              {\n                \"token\": \" today\"\
            ,\n                \"logprob\": -0.0040071653,\n                \"bytes\"\
            : [32, 116, 111, 100, 97, 121]\n              },\n              {\n  \
            \              \"token\": \"?\",\n                \"logprob\": -5.5247097,\n\
            \                \"bytes\": [63]\n              }\n            ]\n   \
            \       },\n          {\n            \"token\": \"?\",\n            \"\
            logprob\": -0.0008108172,\n            \"bytes\": [63],\n            \"\
            top_logprobs\": [\n              {\n                \"token\": \"?\",\n\
            \                \"logprob\": -0.0008108172,\n                \"bytes\"\
            : [63]\n              },\n              {\n                \"token\":\
            \ \"?\\n\",\n                \"logprob\": -7.184561,\n               \
            \ \"bytes\": [63, 10]\n              }\n            ]\n          }\n \
            \       ]\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n\
            \  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"completion_tokens\"\
            : 9,\n    \"total_tokens\": 18,\n    \"completion_tokens_details\": {\n\
            \      \"reasoning_tokens\": 0,\n      \"accepted_prediction_tokens\"\
            : 0,\n      \"rejected_prediction_tokens\": 0\n    }\n  },\n  \"system_fingerprint\"\
            : null\n}\n"
  /chat/completions/{completion_id}:
    get:
      operationId: getChatCompletion
      tags:
      - Chat
      summary: 'Get a stored chat completion. Only Chat Completions that have been
        created

        with the `store` parameter set to `true` will be returned.

        '
      parameters:
      - in: path
        name: completion_id
        required: true
        schema:
          type: string
        description: The ID of the chat completion to retrieve.
      responses:
        '200':
          description: A chat completion
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateChatCompletionResponse'
      x-oaiMeta:
        name: Get chat completion
        group: chat
        returns: The [ChatCompletion](/docs/api-reference/chat/object) object matching
          the specified ID.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/chat/completions/chatcmpl-abc123\
              \ \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type:\
              \ application/json\"\n"
            python: 'from openai import OpenAI

              client = OpenAI()


              completions = client.chat.completions.list()

              first_id = completions[0].id

              first_completion = client.chat.completions.retrieve(completion_id=first_id)

              print(first_completion)

              '
          response: "{\n  \"object\": \"chat.completion\",\n  \"id\": \"chatcmpl-abc123\"\
            ,\n  \"model\": \"gpt-4o-2024-08-06\",\n  \"created\": 1738960610,\n \
            \ \"request_id\": \"req_ded8ab984ec4bf840f37566c1011c417\",\n  \"tool_choice\"\
            : null,\n  \"usage\": {\n    \"total_tokens\": 31,\n    \"completion_tokens\"\
            : 18,\n    \"prompt_tokens\": 13\n  },\n  \"seed\": 4944116822809979520,\n\
            \  \"top_p\": 1.0,\n  \"temperature\": 1.0,\n  \"presence_penalty\": 0.0,\n\
            \  \"frequency_penalty\": 0.0,\n  \"system_fingerprint\": \"fp_50cad350e4\"\
            ,\n  \"input_user\": null,\n  \"service_tier\": \"default\",\n  \"tools\"\
            : null,\n  \"metadata\": {},\n  \"choices\": [\n    {\n      \"index\"\
            : 0,\n      \"message\": {\n        \"content\": \"Mind of circuits hum,\
            \  \\nLearning patterns in silence—  \\nFuture's quiet spark.\",\n   \
            \     \"role\": \"assistant\",\n        \"tool_calls\": null,\n      \
            \  \"function_call\": null\n      },\n      \"finish_reason\": \"stop\"\
            ,\n      \"logprobs\": null\n    }\n  ],\n  \"response_format\": null\n\
            }\n"
    post:
      operationId: updateChatCompletion
      tags:
      - Chat
      summary: 'Modify a stored chat completion. Only Chat Completions that have been

        created with the `store` parameter set to `true` can be modified. Currently,

        the only supported modification is to update the `metadata` field.

        '
      parameters:
      - in: path
        name: completion_id
        required: true
        schema:
          type: string
        description: The ID of the chat completion to update.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              type: object
              required:
              - metadata
              properties:
                metadata:
                  $ref: '#/components/schemas/Metadata'
      responses:
        '200':
          description: A chat completion
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateChatCompletionResponse'
      x-oaiMeta:
        name: Update chat completion
        group: chat
        returns: The [ChatCompletion](/docs/api-reference/chat/object) object matching
          the specified ID.
        examples:
          request:
            curl: "curl -X POST https://api.openai.com/v1/chat/completions/chat_abc123\
              \ \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -d '{\"metadata\": {\"foo\": \"bar\"}}'\n"
            python: 'from openai import OpenAI

              client = OpenAI()


              completions = client.chat.completions.list()

              first_id = completions[0].id

              updated_completion = client.chat.completions.update(completion_id=first_id,
              request_body={"metadata": {"foo": "bar"}})

              print(updated_completion)

              '
          response: "{\n  \"object\": \"chat.completion\",\n  \"id\": \"chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2\"\
            ,\n  \"model\": \"gpt-4o-2024-08-06\",\n  \"created\": 1738960610,\n \
            \ \"request_id\": \"req_ded8ab984ec4bf840f37566c1011c417\",\n  \"tool_choice\"\
            : null,\n  \"usage\": {\n    \"total_tokens\": 31,\n    \"completion_tokens\"\
            : 18,\n    \"prompt_tokens\": 13\n  },\n  \"seed\": 4944116822809979520,\n\
            \  \"top_p\": 1.0,\n  \"temperature\": 1.0,\n  \"presence_penalty\": 0.0,\n\
            \  \"frequency_penalty\": 0.0,\n  \"system_fingerprint\": \"fp_50cad350e4\"\
            ,\n  \"input_user\": null,\n  \"service_tier\": \"default\",\n  \"tools\"\
            : null,\n  \"metadata\": {\n    \"foo\": \"bar\"\n  },\n  \"choices\"\
            : [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"content\"\
            : \"Mind of circuits hum,  \\nLearning patterns in silence—  \\nFuture's\
            \ quiet spark.\",\n        \"role\": \"assistant\",\n        \"tool_calls\"\
            : null,\n        \"function_call\": null\n      },\n      \"finish_reason\"\
            : \"stop\",\n      \"logprobs\": null\n    }\n  ],\n  \"response_format\"\
            : null\n}\n"
    delete:
      operationId: deleteChatCompletion
      tags:
      - Chat
      summary: 'Delete a stored chat completion. Only Chat Completions that have been

        created with the `store` parameter set to `true` can be deleted.

        '
      parameters:
      - in: path
        name: completion_id
        required: true
        schema:
          type: string
        description: The ID of the chat completion to delete.
      responses:
        '200':
          description: The chat completion was deleted successfully.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ChatCompletionDeleted'
      x-oaiMeta:
        name: Delete chat completion
        group: chat
        returns: A deletion confirmation object.
        examples:
          request:
            curl: "curl -X DELETE https://api.openai.com/v1/chat/completions/chat_abc123\
              \ \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type:\
              \ application/json\"\n"
            python: 'from openai import OpenAI

              client = OpenAI()


              completions = client.chat.completions.list()

              first_id = completions[0].id

              delete_response = client.chat.completions.delete(completion_id=first_id)

              print(delete_response)

              '
          response: "{\n  \"object\": \"chat.completion.deleted\",\n  \"id\": \"chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2\"\
            ,\n  \"deleted\": true\n}\n"
  /chat/completions/{completion_id}/messages:
    get:
      operationId: getChatCompletionMessages
      tags:
      - Chat
      summary: 'Get the messages in a stored chat completion. Only Chat Completions
        that

        have been created with the `store` parameter set to `true` will be

        returned.

        '
      parameters:
      - in: path
        name: completion_id
        required: true
        schema:
          type: string
        description: The ID of the chat completion to retrieve messages from.
      - name: after
        in: query
        description: Identifier for the last message from the previous pagination
          request.
        required: false
        schema:
          type: string
      - name: limit
        in: query
        description: Number of messages to retrieve.
        required: false
        schema:
          type: integer
          default: 20
      - name: order
        in: query
        description: Sort order for messages by timestamp. Use `asc` for ascending
          order or `desc` for descending order. Defaults to `asc`.
        required: false
        schema:
          type: string
          enum:
          - asc
          - desc
          default: asc
      responses:
        '200':
          description: A list of messages
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ChatCompletionMessageList'
      x-oaiMeta:
        name: Get chat messages
        group: chat
        returns: A list of [messages](/docs/api-reference/chat/message-list) for the
          specified chat completion.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/chat/completions/chat_abc123/messages\
              \ \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n  -H \"Content-Type:\
              \ application/json\"\n"
            python: 'from openai import OpenAI

              client = OpenAI()


              completions = client.chat.completions.list()

              first_id = completions[0].id

              first_completion = client.chat.completions.retrieve(completion_id=first_id)

              messages = client.chat.completions.messages.list(completion_id=first_id)

              print(messages)

              '
          response: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\"\
            : \"chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2-0\",\n      \"role\": \"user\"\
            ,\n      \"content\": \"write a haiku about ai\",\n      \"name\": null,\n\
            \      \"content_parts\": null\n    }\n  ],\n  \"first_id\": \"chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2-0\"\
            ,\n  \"last_id\": \"chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2-0\",\n  \"\
            has_more\": false\n}\n"
  /completions:
    post:
      operationId: createCompletion
      tags:
      - Completions
      summary: Creates a completion for the provided prompt and parameters.
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateCompletionRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/CreateCompletionResponse'
      x-oaiMeta:
        name: Create completion
        group: completions
        returns: 'Returns a [completion](/docs/api-reference/completions/object) object,
          or a sequence of completion objects if the request is streamed.

          '
        legacy: true
        examples:
        - title: No streaming
          request:
            curl: "curl https://api.openai.com/v1/completions \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\
              \ \\\n  -d '{\n    \"model\": \"VAR_completion_model_id\",\n    \"prompt\"\
              : \"Say this is a test\",\n    \"max_tokens\": 7,\n    \"temperature\"\
              : 0\n  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nclient.completions.create(\n\
              \  model=\"VAR_completion_model_id\",\n  prompt=\"Say this is a test\"\
              ,\n  max_tokens=7,\n  temperature=0\n)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const completion = await openai.completions.create({\n\
              \    model: \"VAR_completion_model_id\",\n    prompt: \"Say this is\
              \ a test.\",\n    max_tokens: 7,\n    temperature: 0,\n  });\n\n  console.log(completion);\n\
              }\nmain();"
          response: "{\n  \"id\": \"cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7\",\n  \"object\"\
            : \"text_completion\",\n  \"created\": 1589478378,\n  \"model\": \"VAR_completion_model_id\"\
            ,\n  \"system_fingerprint\": \"fp_44709d6fcb\",\n  \"choices\": [\n  \
            \  {\n      \"text\": \"\\n\\nThis is indeed a test\",\n      \"index\"\
            : 0,\n      \"logprobs\": null,\n      \"finish_reason\": \"length\"\n\
            \    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 5,\n    \"completion_tokens\"\
            : 7,\n    \"total_tokens\": 12\n  }\n}\n"
        - title: Streaming
          request:
            curl: "curl https://api.openai.com/v1/completions \\\n  -H \"Content-Type:\
              \ application/json\" \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\
              \ \\\n  -d '{\n    \"model\": \"VAR_completion_model_id\",\n    \"prompt\"\
              : \"Say this is a test\",\n    \"max_tokens\": 7,\n    \"temperature\"\
              : 0,\n    \"stream\": true\n  }'\n"
            python: "from openai import OpenAI\nclient = OpenAI()\n\nfor chunk in\
              \ client.completions.create(\n  model=\"VAR_completion_model_id\",\n\
              \  prompt=\"Say this is a test\",\n  max_tokens=7,\n  temperature=0,\n\
              \  stream=True\n):\n  print(chunk.choices[0].text)\n"
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const stream = await openai.completions.create({\n\
              \    model: \"VAR_completion_model_id\",\n    prompt: \"Say this is\
              \ a test.\",\n    stream: true,\n  });\n\n  for await (const chunk of\
              \ stream) {\n    console.log(chunk.choices[0].text)\n  }\n}\nmain();"
          response: "{\n  \"id\": \"cmpl-7iA7iJjj8V2zOkCGvWF2hAkDWBQZe\",\n  \"object\"\
            : \"text_completion\",\n  \"created\": 1690759702,\n  \"choices\": [\n\
            \    {\n      \"text\": \"This\",\n      \"index\": 0,\n      \"logprobs\"\
            : null,\n      \"finish_reason\": null\n    }\n  ],\n  \"model\": \"gpt-3.5-turbo-instruct\"\
            \n  \"system_fingerprint\": \"fp_44709d6fcb\",\n}\n"
  /models:
    get:
      operationId: listModels
      tags:
      - Models
      summary: Lists the currently available models, and provides basic information
        about each one such as the owner and availability.
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ListModelsResponse'
      x-oaiMeta:
        name: List models
        group: models
        returns: A list of [model](/docs/api-reference/models/object) objects.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/models \\\n  -H \"Authorization:\
              \ Bearer $OPENAI_API_KEY\"\n"
            python: 'from openai import OpenAI

              client = OpenAI()


              client.models.list()

              '
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const list = await openai.models.list();\n\
              \n  for await (const model of list) {\n    console.log(model);\n  }\n\
              }\nmain();"
            csharp: "using System;\n\nusing OpenAI.Models;\n\nOpenAIModelClient client\
              \ = new(\n    apiKey: Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\"\
              )\n);\n\nforeach (var model in client.GetModels().Value)\n{\n    Console.WriteLine(model.Id);\n\
              }\n"
          response: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\"\
            : \"model-id-0\",\n      \"object\": \"model\",\n      \"created\": 1686935002,\n\
            \      \"owned_by\": \"organization-owner\"\n    },\n    {\n      \"id\"\
            : \"model-id-1\",\n      \"object\": \"model\",\n      \"created\": 1686935002,\n\
            \      \"owned_by\": \"organization-owner\",\n    },\n    {\n      \"\
            id\": \"model-id-2\",\n      \"object\": \"model\",\n      \"created\"\
            : 1686935002,\n      \"owned_by\": \"openai\"\n    },\n  ],\n  \"object\"\
            : \"list\"\n}\n"
  /models/{model}:
    get:
      operationId: retrieveModel
      tags:
      - Models
      summary: Retrieves a model instance, providing basic information about the model
        such as the owner and permissioning.
      parameters:
      - in: path
        name: model
        required: true
        schema:
          type: string
          example: gpt-4o-mini
        description: The ID of the model to use for this request
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Model'
      x-oaiMeta:
        name: Retrieve model
        group: models
        returns: The [model](/docs/api-reference/models/object) object matching the
          specified ID.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/models/VAR_chat_model_id \\\n  -H\
              \ \"Authorization: Bearer $OPENAI_API_KEY\"\n"
            python: 'from openai import OpenAI

              client = OpenAI()


              client.models.retrieve("VAR_chat_model_id")

              '
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const model = await openai.models.retrieve(\"\
              VAR_chat_model_id\");\n\n  console.log(model);\n}\n\nmain();"
            csharp: "using System;\nusing System.ClientModel;\n\nusing OpenAI.Models;\n\
              \n  OpenAIModelClient client = new(\n    apiKey: Environment.GetEnvironmentVariable(\"\
              OPENAI_API_KEY\")\n);\n\nClientResult<OpenAIModel> model = client.GetModel(\"\
              babbage-002\");\nConsole.WriteLine(model.Value.Id);\n"
          response: "{\n  \"id\": \"VAR_chat_model_id\",\n  \"object\": \"model\"\
            ,\n  \"created\": 1686935002,\n  \"owned_by\": \"openai\"\n}\n"
    delete:
      operationId: deleteModel
      tags:
      - Models
      summary: Delete a fine-tuned model. You must have the Owner role in your organization
        to delete a model.
      parameters:
      - in: path
        name: model
        required: true
        schema:
          type: string
          example: ft:gpt-4o-mini:acemeco:suffix:abc123
        description: The model to delete
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/DeleteModelResponse'
      x-oaiMeta:
        name: Delete a fine-tuned model
        group: models
        returns: Deletion status.
        examples:
          request:
            curl: "curl https://api.openai.com/v1/models/ft:gpt-4o-mini:acemeco:suffix:abc123\
              \ \\\n  -X DELETE \\\n  -H \"Authorization: Bearer $OPENAI_API_KEY\"\
              \n"
            python: 'from openai import OpenAI

              client = OpenAI()


              client.models.delete("ft:gpt-4o-mini:acemeco:suffix:abc123")

              '
            node.js: "import OpenAI from \"openai\";\n\nconst openai = new OpenAI();\n\
              \nasync function main() {\n  const model = await openai.models.del(\"\
              ft:gpt-4o-mini:acemeco:suffix:abc123\");\n\n  console.log(model);\n\
              }\nmain();"
            csharp: "using System;\nusing System.ClientModel;\n\nusing OpenAI.Models;\n\
              \nOpenAIModelClient client = new(\n    apiKey: Environment.GetEnvironmentVariable(\"\
              OPENAI_API_KEY\")\n);\n\nClientResult success = client.DeleteModel(\"\
              ft:gpt-4o-mini:acemeco:suffix:abc123\");\nConsole.WriteLine(success);\n"
          response: "{\n  \"id\": \"ft:gpt-4o-mini:acemeco:suffix:abc123\",\n  \"\
            object\": \"model\",\n  \"deleted\": true\n}\n"
components:
  schemas:
    ChatCompletionDeleted:
      type: object
      properties:
        object:
          type: string
          description: The type of object being deleted.
          enum:
          - chat.completion.deleted
          x-stainless-const: true
        id:
          type: string
          description: The ID of the chat completion that was deleted.
        deleted:
          type: boolean
          description: Whether the chat completion was deleted.
      required:
      - object
      - id
      - deleted
    ChatCompletionFunctionCallOption:
      type: object
      description: 'Specifying a particular function via `{"name": "my_function"}`
        forces the model to call that function.

        '
      properties:
        name:
          type: string
          description: The name of the function to call.
      required:
      - name
    ChatCompletionFunctions:
      type: object
      deprecated: true
      properties:
        description:
          type: string
          description: A description of what the function does, used by the model
            to choose when and how to call the function.
        name:
          type: string
          description: The name of the function to be called. Must be a-z, A-Z, 0-9,
            or contain underscores and dashes, with a maximum length of 64.
        parameters:
          $ref: '#/components/schemas/FunctionParameters'
      required:
      - name
    ChatCompletionList:
      type: object
      title: ChatCompletionList
      description: 'An object representing a list of Chat Completions.

        '
      properties:
        object:
          type: string
          enum:
          - list
          default: list
          description: 'The type of this object. It is always set to "list".

            '
          x-stainless-const: true
        data:
          type: array
          description: 'An array of chat completion objects.

            '
          items:
            $ref: '#/components/schemas/CreateChatCompletionResponse'
        first_id:
          type: string
          description: The identifier of the first chat completion in the data array.
        last_id:
          type: string
          description: The identifier of the last chat completion in the data array.
        has_more:
          type: boolean
          description: Indicates whether there are more Chat Completions available.
      required:
      - object
      - data
      - first_id
      - last_id
      - has_more
      x-oaiMeta:
        name: The chat completion list object
        group: chat
        example: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\"\
          : \"chat.completion\",\n      \"id\": \"chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2\"\
          ,\n      \"model\": \"gpt-4o-2024-08-06\",\n      \"created\": 1738960610,\n\
          \      \"request_id\": \"req_ded8ab984ec4bf840f37566c1011c417\",\n     \
          \ \"tool_choice\": null,\n      \"usage\": {\n        \"total_tokens\":\
          \ 31,\n        \"completion_tokens\": 18,\n        \"prompt_tokens\": 13\n\
          \      },\n      \"seed\": 4944116822809979520,\n      \"top_p\": 1.0,\n\
          \      \"temperature\": 1.0,\n      \"presence_penalty\": 0.0,\n      \"\
          frequency_penalty\": 0.0,\n      \"system_fingerprint\": \"fp_50cad350e4\"\
          ,\n      \"input_user\": null,\n      \"service_tier\": \"default\",\n \
          \     \"tools\": null,\n      \"metadata\": {},\n      \"choices\": [\n\
          \        {\n          \"index\": 0,\n          \"message\": {\n        \
          \    \"content\": \"Mind of circuits hum,  \\nLearning patterns in silence—\
          \  \\nFuture's quiet spark.\",\n            \"role\": \"assistant\",\n \
          \           \"tool_calls\": null,\n            \"function_call\": null\n\
          \          },\n          \"finish_reason\": \"stop\",\n          \"logprobs\"\
          : null\n        }\n      ],\n      \"response_format\": null\n    }\n  ],\n\
          \  \"first_id\": \"chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2\",\n  \"last_id\"\
          : \"chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2\",\n  \"has_more\": false\n}\n"
    ChatCompletionMessageList:
      type: object
      title: ChatCompletionMessageList
      description: 'An object representing a list of chat completion messages.

        '
      properties:
        object:
          type: string
          enum:
          - list
          default: list
          description: 'The type of this object. It is always set to "list".

            '
          x-stainless-const: true
        data:
          type: array
          description: 'An array of chat completion message objects.

            '
          items:
            allOf:
            - $ref: '#/components/schemas/ChatCompletionResponseMessage'
            - type: object
              required:
              - id
              properties:
                id:
                  type: string
                  description: The identifier of the chat message.
        first_id:
          type: string
          description: The identifier of the first chat message in the data array.
        last_id:
          type: string
          description: The identifier of the last chat message in the data array.
        has_more:
          type: boolean
          description: Indicates whether there are more chat messages available.
      required:
      - object
      - data
      - first_id
      - last_id
      - has_more
      x-oaiMeta:
        name: The chat completion message list object
        group: chat
        example: "{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"id\":\
          \ \"chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2-0\",\n      \"role\": \"user\"\
          ,\n      \"content\": \"write a haiku about ai\",\n      \"name\": null,\n\
          \      \"content_parts\": null\n    }\n  ],\n  \"first_id\": \"chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2-0\"\
          ,\n  \"last_id\": \"chatcmpl-AyPNinnUqUDYo9SAdA52NobMflmj2-0\",\n  \"has_more\"\
          : false\n}\n"
    ChatCompletionMessageToolCall:
      type: object
      properties:
        id:
          type: string
          description: The ID of the tool call.
        type:
          type: string
          enum:
          - function
          description: The type of the tool. Currently, only `function` is supported.
          x-stainless-const: true
        function:
          type: object
          description: The function that the model called.
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: The arguments to call the function with, as generated by
                the model in JSON format. Note that the model does not always generate
                valid JSON, and may hallucinate parameters not defined by your function
                schema. Validate the arguments in your code before calling your function.
          required:
          - name
          - arguments
      required:
      - id
      - type
      - function
    ChatCompletionMessageToolCallChunk:
      type: object
      properties:
        index:
          type: integer
        id:
          type: string
          description: The ID of the tool call.
        type:
          type: string
          enum:
          - function
          description: The type of the tool. Currently, only `function` is supported.
          x-stainless-const: true
        function:
          type: object
          properties:
            name:
              type: string
              description: The name of the function to call.
            arguments:
              type: string
              description: The arguments to call the function with, as generated by
                the model in JSON format. Note that the model does not always generate
                valid JSON, and may hallucinate parameters not defined by your function
                schema. Validate the arguments in your code before calling your function.
      required:
      - index
    ChatCompletionMessageToolCalls:
      type: array
      description: The tool calls generated by the model, such as function calls.
      items:
        $ref: '#/components/schemas/ChatCompletionMessageToolCall'
    ChatCompletionNamedToolChoice:
      type: object
      description: Specifies a tool the model should use. Use to force the model to
        call a specific function.
      properties:
        type:
          type: string
          enum:
          - function
          description: The type of the tool. Currently, only `function` is supported.
          x-stainless-const: true
        function:
          type: object
          properties:
            name:
              type: string
              description: The name of the function to call.
          required:
          - name
      required:
      - type
      - function
    ChatCompletionRequestAssistantMessage:
      type: object
      title: Assistant message
      description: 'Messages sent by the model in response to user messages.

        '
      properties:
        content:
          nullable: true
          oneOf:
          - type: string
            description: The contents of the assistant message.
            title: Text content
          - type: array
            description: An array of content parts with a defined type. Can be one
              or more of type `text`, or exactly one of type `refusal`.
            title: Array of content parts
            items:
              $ref: '#/components/schemas/ChatCompletionRequestAssistantMessageContentPart'
            minItems: 1
          description: 'The contents of the assistant message. Required unless `tool_calls`
            or `function_call` is specified.

            '
        refusal:
          nullable: true
          type: string
          description: The refusal message by the assistant.
        role:
          type: string
          enum:
          - assistant
          description: The role of the messages author, in this case `assistant`.
          x-stainless-const: true
        name:
          type: string
          description: An optional name for the participant. Provides the model information
            to differentiate between participants of the same role.
        audio:
          type: object
          nullable: true
          description: "Data about a previous audio response from the model. \n[Learn\
            \ more](/docs/guides/audio).\n"
          required:
          - id
          properties:
            id:
              type: string
              description: 'Unique identifier for a previous audio response from the
                model.

                '
        tool_calls:
          $ref: '#/components/schemas/ChatCompletionMessageToolCalls'
        function_call:
          type: object
          deprecated: true
          description: Deprecated and replaced by `tool_calls`. The name and arguments
            of a function that should be called, as generated by the model.
          nullable: true
          properties:
            arguments:
              type: string
              description: The arguments to call the function with, as generated by
                the model in JSON format. Note that the model does not always generate
                valid JSON, and may hallucinate parameters not defined by your function
                schema. Validate the arguments in your code before calling your function.
            name:
              type: string
              description: The name of the function to call.
          required:
          - arguments
          - name
      required:
      - role
    ChatCompletionRequestAssistantMessageContentPart:
      oneOf:
      - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
      - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartRefusal'
    ChatCompletionRequestDeveloperMessage:
      type: object
      title: Developer message
      description: 'Developer-provided instructions that the model should follow,
        regardless of

        messages sent by the user. With o1 models and newer, `developer` messages

        replace the previous `system` messages.

        '
      properties:
        content:
          description: The contents of the developer message.
          oneOf:
          - type: string
            description: The contents of the developer message.
            title: Text content
          - type: array
            description: An array of content parts with a defined type. For developer
              messages, only type `text` is supported.
            title: Array of content parts
            items:
              $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
            minItems: 1
        role:
          type: string
          enum:
          - developer
          description: The role of the messages author, in this case `developer`.
          x-stainless-const: true
        name:
          type: string
          description: An optional name for the participant. Provides the model information
            to differentiate between participants of the same role.
      required:
      - content
      - role
    ChatCompletionRequestFunctionMessage:
      type: object
      title: Function message
      deprecated: true
      properties:
        role:
          type: string
          enum:
          - function
          description: The role of the messages author, in this case `function`.
          x-stainless-const: true
        content:
          nullable: true
          type: string
          description: The contents of the function message.
        name:
          type: string
          description: The name of the function to call.
      required:
      - role
      - content
      - name
    ChatCompletionRequestMessage:
      oneOf:
      - $ref: '#/components/schemas/ChatCompletionRequestDeveloperMessage'
      - $ref: '#/components/schemas/ChatCompletionRequestSystemMessage'
      - $ref: '#/components/schemas/ChatCompletionRequestUserMessage'
      - $ref: '#/components/schemas/ChatCompletionRequestAssistantMessage'
      - $ref: '#/components/schemas/ChatCompletionRequestToolMessage'
      - $ref: '#/components/schemas/ChatCompletionRequestFunctionMessage'
    ChatCompletionRequestMessageContentPartAudio:
      type: object
      title: Audio content part
      description: 'Learn about [audio inputs](/docs/guides/audio).

        '
      properties:
        type:
          type: string
          enum:
          - input_audio
          description: The type of the content part. Always `input_audio`.
          x-stainless-const: true
        input_audio:
          type: object
          properties:
            data:
              type: string
              description: Base64 encoded audio data.
            format:
              type: string
              enum:
              - wav
              - mp3
              description: 'The format of the encoded audio data. Currently supports
                "wav" and "mp3".

                '
          required:
          - data
          - format
      required:
      - type
      - input_audio
    ChatCompletionRequestMessageContentPartFile:
      type: object
      title: File content part
      description: 'Learn about [file inputs](/docs/guides/text) for text generation.

        '
      properties:
        type:
          type: string
          enum:
          - file
          description: The type of the content part. Always `file`.
          x-stainless-const: true
        file:
          type: object
          properties:
            filename:
              type: string
              description: "The name of the file, used when passing the file to the\
                \ model as a \nstring.\n"
            file_data:
              type: string
              description: "The base64 encoded file data, used when passing the file\
                \ to the model \nas a string.\n"
            file_id:
              type: string
              description: 'The ID of an uploaded file to use as input.

                '
      required:
      - type
      - file
    ChatCompletionRequestMessageContentPartImage:
      type: object
      title: Image content part
      description: 'Learn about [image inputs](/docs/guides/vision).

        '
      properties:
        type:
          type: string
          enum:
          - image_url
          description: The type of the content part.
          x-stainless-const: true
        image_url:
          type: object
          properties:
            url:
              type: string
              description: Either a URL of the image or the base64 encoded image data.
              format: uri
            detail:
              type: string
              description: Specifies the detail level of the image. Learn more in
                the [Vision guide](/docs/guides/vision#low-or-high-fidelity-image-understanding).
              enum:
              - auto
              - low
              - high
              default: auto
          required:
          - url
      required:
      - type
      - image_url
    ChatCompletionRequestMessageContentPartRefusal:
      type: object
      title: Refusal content part
      properties:
        type:
          type: string
          enum:
          - refusal
          description: The type of the content part.
          x-stainless-const: true
        refusal:
          type: string
          description: The refusal message generated by the model.
      required:
      - type
      - refusal
    ChatCompletionRequestMessageContentPartText:
      type: object
      title: Text content part
      description: 'Learn about [text inputs](/docs/guides/text-generation).

        '
      properties:
        type:
          type: string
          enum:
          - text
          description: The type of the content part.
          x-stainless-const: true
        text:
          type: string
          description: The text content.
      required:
      - type
      - text
    ChatCompletionRequestSystemMessage:
      type: object
      title: System message
      description: 'Developer-provided instructions that the model should follow,
        regardless of

        messages sent by the user. With o1 models and newer, use `developer` messages

        for this purpose instead.

        '
      properties:
        content:
          description: The contents of the system message.
          oneOf:
          - type: string
            description: The contents of the system message.
            title: Text content
          - type: array
            description: An array of content parts with a defined type. For system
              messages, only type `text` is supported.
            title: Array of content parts
            items:
              $ref: '#/components/schemas/ChatCompletionRequestSystemMessageContentPart'
            minItems: 1
        role:
          type: string
          enum:
          - system
          description: The role of the messages author, in this case `system`.
          x-stainless-const: true
        name:
          type: string
          description: An optional name for the participant. Provides the model information
            to differentiate between participants of the same role.
      required:
      - content
      - role
    ChatCompletionRequestSystemMessageContentPart:
      oneOf:
      - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
    ChatCompletionRequestToolMessage:
      type: object
      title: Tool message
      properties:
        role:
          type: string
          enum:
          - tool
          description: The role of the messages author, in this case `tool`.
          x-stainless-const: true
        content:
          oneOf:
          - type: string
            description: The contents of the tool message.
            title: Text content
          - type: array
            description: An array of content parts with a defined type. For tool messages,
              only type `text` is supported.
            title: Array of content parts
            items:
              $ref: '#/components/schemas/ChatCompletionRequestToolMessageContentPart'
            minItems: 1
          description: The contents of the tool message.
        tool_call_id:
          type: string
          description: Tool call that this message is responding to.
      required:
      - role
      - content
      - tool_call_id
    ChatCompletionRequestToolMessageContentPart:
      oneOf:
      - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
    ChatCompletionRequestUserMessage:
      type: object
      title: User message
      description: 'Messages sent by an end user, containing prompts or additional
        context

        information.

        '
      properties:
        content:
          description: 'The contents of the user message.

            '
          oneOf:
          - type: string
            description: The text contents of the message.
            title: Text content
          - type: array
            description: An array of content parts with a defined type. Supported
              options differ based on the [model](/docs/models) being used to generate
              the response. Can contain text, image, or audio inputs.
            title: Array of content parts
            items:
              $ref: '#/components/schemas/ChatCompletionRequestUserMessageContentPart'
            minItems: 1
        role:
          type: string
          enum:
          - user
          description: The role of the messages author, in this case `user`.
          x-stainless-const: true
        name:
          type: string
          description: An optional name for the participant. Provides the model information
            to differentiate between participants of the same role.
      required:
      - content
      - role
    ChatCompletionRequestUserMessageContentPart:
      oneOf:
      - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
      - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartImage'
      - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartAudio'
      - $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartFile'
    ChatCompletionResponseMessage:
      type: object
      description: A chat completion message generated by the model.
      properties:
        content:
          type: string
          description: The contents of the message.
          nullable: true
        refusal:
          type: string
          description: The refusal message generated by the model.
          nullable: true
        tool_calls:
          $ref: '#/components/schemas/ChatCompletionMessageToolCalls'
        annotations:
          type: array
          description: 'Annotations for the message, when applicable, as when using
            the

            [web search tool](/docs/guides/tools-web-search?api-mode=chat).

            '
          items:
            type: object
            description: 'A URL citation when using web search.

              '
            required:
            - type
            - url_citation
            properties:
              type:
                type: string
                description: The type of the URL citation. Always `url_citation`.
                enum:
                - url_citation
                x-stainless-const: true
              url_citation:
                type: object
                description: A URL citation when using web search.
                required:
                - end_index
                - start_index
                - url
                - title
                properties:
                  end_index:
                    type: integer
                    description: The index of the last character of the URL citation
                      in the message.
                  start_index:
                    type: integer
                    description: The index of the first character of the URL citation
                      in the message.
                  url:
                    type: string
                    description: The URL of the web resource.
                  title:
                    type: string
                    description: The title of the web resource.
        role:
          type: string
          enum:
          - assistant
          description: The role of the author of this message.
          x-stainless-const: true
        function_call:
          type: object
          deprecated: true
          description: Deprecated and replaced by `tool_calls`. The name and arguments
            of a function that should be called, as generated by the model.
          properties:
            arguments:
              type: string
              description: The arguments to call the function with, as generated by
                the model in JSON format. Note that the model does not always generate
                valid JSON, and may hallucinate parameters not defined by your function
                schema. Validate the arguments in your code before calling your function.
            name:
              type: string
              description: The name of the function to call.
          required:
          - name
          - arguments
        audio:
          type: object
          nullable: true
          description: 'If the audio output modality is requested, this object contains
            data

            about the audio response from the model. [Learn more](/docs/guides/audio).

            '
          required:
          - id
          - expires_at
          - data
          - transcript
          properties:
            id:
              type: string
              description: Unique identifier for this audio response.
            expires_at:
              type: integer
              description: 'The Unix timestamp (in seconds) for when this audio response
                will

                no longer be accessible on the server for use in multi-turn

                conversations.

                '
            data:
              type: string
              description: 'Base64 encoded audio bytes generated by the model, in
                the format

                specified in the request.

                '
            transcript:
              type: string
              description: Transcript of the audio generated by the model.
      required:
      - role
      - content
      - refusal
    ChatCompletionStreamOptions:
      description: 'Options for streaming response. Only set this when you set `stream:
        true`.

        '
      type: object
      nullable: true
      default: null
      properties:
        include_usage:
          type: boolean
          description: "If set, an additional chunk will be streamed before the `data:\
            \ [DONE]`\nmessage. The `usage` field on this chunk shows the token usage\
            \ statistics\nfor the entire request, and the `choices` field will always\
            \ be an empty\narray. \n\nAll other chunks will also include a `usage`\
            \ field, but with a null\nvalue. **NOTE:** If the stream is interrupted,\
            \ you may not receive the\nfinal usage chunk which contains the total\
            \ token usage for the request.\n"
    ChatCompletionStreamResponseDelta:
      type: object
      description: A chat completion delta generated by streamed model responses.
      properties:
        content:
          type: string
          description: The contents of the chunk message.
          nullable: true
        function_call:
          deprecated: true
          type: object
          description: Deprecated and replaced by `tool_calls`. The name and arguments
            of a function that should be called, as generated by the model.
          properties:
            arguments:
              type: string
              description: The arguments to call the function with, as generated by
                the model in JSON format. Note that the model does not always generate
                valid JSON, and may hallucinate parameters not defined by your function
                schema. Validate the arguments in your code before calling your function.
            name:
              type: string
              description: The name of the function to call.
        tool_calls:
          type: array
          items:
            $ref: '#/components/schemas/ChatCompletionMessageToolCallChunk'
        role:
          type: string
          enum:
          - developer
          - system
          - user
          - assistant
          - tool
          description: The role of the author of this message.
        refusal:
          type: string
          description: The refusal message generated by the model.
          nullable: true
    ChatCompletionTokenLogprob:
      type: object
      properties:
        token:
          description: The token.
          type: string
        logprob:
          description: The log probability of this token, if it is within the top
            20 most likely tokens. Otherwise, the value `-9999.0` is used to signify
            that the token is very unlikely.
          type: number
        bytes:
          description: A list of integers representing the UTF-8 bytes representation
            of the token. Useful in instances where characters are represented by
            multiple tokens and their byte representations must be combined to generate
            the correct text representation. Can be `null` if there is no bytes representation
            for the token.
          type: array
          items:
            type: integer
          nullable: true
        top_logprobs:
          description: List of the most likely tokens and their log probability, at
            this token position. In rare cases, there may be fewer than the number
            of requested `top_logprobs` returned.
          type: array
          items:
            type: object
            properties:
              token:
                description: The token.
                type: string
              logprob:
                description: The log probability of this token, if it is within the
                  top 20 most likely tokens. Otherwise, the value `-9999.0` is used
                  to signify that the token is very unlikely.
                type: number
              bytes:
                description: A list of integers representing the UTF-8 bytes representation
                  of the token. Useful in instances where characters are represented
                  by multiple tokens and their byte representations must be combined
                  to generate the correct text representation. Can be `null` if there
                  is no bytes representation for the token.
                type: array
                items:
                  type: integer
                nullable: true
            required:
            - token
            - logprob
            - bytes
      required:
      - token
      - logprob
      - bytes
      - top_logprobs
    ChatCompletionTool:
      type: object
      properties:
        type:
          type: string
          enum:
          - function
          description: The type of the tool. Currently, only `function` is supported.
          x-stainless-const: true
        function:
          $ref: '#/components/schemas/FunctionObject'
      required:
      - type
      - function
    ChatCompletionToolChoiceOption:
      description: 'Controls which (if any) tool is called by the model.

        `none` means the model will not call any tool and instead generates a message.

        `auto` means the model can pick between generating a message or calling one
        or more tools.

        `required` means the model must call one or more tools.

        Specifying a particular tool via `{"type": "function", "function": {"name":
        "my_function"}}` forces the model to call that tool.


        `none` is the default when no tools are present. `auto` is the default if
        tools are present.

        '
      oneOf:
      - type: string
        description: '`none` means the model will not call any tool and instead generates
          a message. `auto` means the model can pick between generating a message
          or calling one or more tools. `required` means the model must call one or
          more tools.

          '
        enum:
        - none
        - auto
        - required
      - $ref: '#/components/schemas/ChatCompletionNamedToolChoice'
    CompletionUsage:
      type: object
      description: Usage statistics for the completion request.
      properties:
        completion_tokens:
          type: integer
          default: 0
          description: Number of tokens in the generated completion.
        prompt_tokens:
          type: integer
          default: 0
          description: Number of tokens in the prompt.
        total_tokens:
          type: integer
          default: 0
          description: Total number of tokens used in the request (prompt + completion).
        completion_tokens_details:
          type: object
          description: Breakdown of tokens used in a completion.
          properties:
            accepted_prediction_tokens:
              type: integer
              default: 0
              description: 'When using Predicted Outputs, the number of tokens in
                the

                prediction that appeared in the completion.

                '
            audio_tokens:
              type: integer
              default: 0
              description: Audio input tokens generated by the model.
            reasoning_tokens:
              type: integer
              default: 0
              description: Tokens generated by the model for reasoning.
            rejected_prediction_tokens:
              type: integer
              default: 0
              description: 'When using Predicted Outputs, the number of tokens in
                the

                prediction that did not appear in the completion. However, like

                reasoning tokens, these tokens are still counted in the total

                completion tokens for purposes of billing, output, and context window

                limits.

                '
        prompt_tokens_details:
          type: object
          description: Breakdown of tokens used in the prompt.
          properties:
            audio_tokens:
              type: integer
              default: 0
              description: Audio input tokens present in the prompt.
            cached_tokens:
              type: integer
              default: 0
              description: Cached tokens present in the prompt.
      required:
      - prompt_tokens
      - completion_tokens
      - total_tokens
    CreateChatCompletionRequest:
      allOf:
      - $ref: '#/components/schemas/CreateModelResponseProperties'
      - type: object
        properties:
          messages:
            description: 'A list of messages comprising the conversation so far. Depending
              on the

              [model](/docs/models) you use, different message types (modalities)
              are

              supported, like [text](/docs/guides/text-generation),

              [images](/docs/guides/vision), and [audio](/docs/guides/audio).

              '
            type: array
            minItems: 1
            items:
              $ref: '#/components/schemas/ChatCompletionRequestMessage'
          model:
            description: 'Model ID used to generate the response, like `gpt-4o` or
              `o3`. OpenAI

              offers a wide range of models with different capabilities, performance

              characteristics, and price points. Refer to the [model guide](/docs/models)

              to browse and compare available models.

              '
            $ref: '#/components/schemas/ModelIdsShared'
          modalities:
            $ref: '#/components/schemas/ResponseModalities'
          reasoning_effort:
            $ref: '#/components/schemas/ReasoningEffort'
          max_completion_tokens:
            description: 'An upper bound for the number of tokens that can be generated
              for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).

              '
            type: integer
            nullable: true
          frequency_penalty:
            type: number
            default: 0
            minimum: -2
            maximum: 2
            nullable: true
            description: 'Number between -2.0 and 2.0. Positive values penalize new
              tokens based on

              their existing frequency in the text so far, decreasing the model''s

              likelihood to repeat the same line verbatim.

              '
          presence_penalty:
            type: number
            default: 0
            minimum: -2
            maximum: 2
            nullable: true
            description: 'Number between -2.0 and 2.0. Positive values penalize new
              tokens based on

              whether they appear in the text so far, increasing the model''s likelihood

              to talk about new topics.

              '
          web_search_options:
            type: object
            title: Web search
            description: 'This tool searches the web for relevant results to use in
              a response.

              Learn more about the [web search tool](/docs/guides/tools-web-search?api-mode=chat).

              '
            properties:
              user_location:
                type: object
                nullable: true
                required:
                - type
                - approximate
                description: 'Approximate location parameters for the search.

                  '
                properties:
                  type:
                    type: string
                    description: 'The type of location approximation. Always `approximate`.

                      '
                    enum:
                    - approximate
                    x-stainless-const: true
                  approximate:
                    $ref: '#/components/schemas/WebSearchLocation'
              search_context_size:
                $ref: '#/components/schemas/WebSearchContextSize'
          top_logprobs:
            description: 'An integer between 0 and 20 specifying the number of most
              likely tokens to

              return at each token position, each with an associated log probability.

              `logprobs` must be set to `true` if this parameter is used.

              '
            type: integer
            minimum: 0
            maximum: 20
            nullable: true
          response_format:
            description: 'An object specifying the format that the model must output.


              Setting to `{ "type": "json_schema", "json_schema": {...} }` enables

              Structured Outputs which ensures the model will match your supplied
              JSON

              schema. Learn more in the [Structured Outputs

              guide](/docs/guides/structured-outputs).


              Setting to `{ "type": "json_object" }` enables the older JSON mode,
              which

              ensures the message the model generates is valid JSON. Using `json_schema`

              is preferred for models that support it.

              '
            oneOf:
            - $ref: '#/components/schemas/ResponseFormatText'
            - $ref: '#/components/schemas/ResponseFormatJsonSchema'
            - $ref: '#/components/schemas/ResponseFormatJsonObject'
          audio:
            type: object
            nullable: true
            description: 'Parameters for audio output. Required when audio output
              is requested with

              `modalities: ["audio"]`. [Learn more](/docs/guides/audio).

              '
            required:
            - voice
            - format
            properties:
              voice:
                $ref: '#/components/schemas/VoiceIdsShared'
                description: "The voice the model uses to respond. Supported voices\
                  \ are \n`alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`,\
                  \ `onyx`, `sage`, and `shimmer`.\n"
              format:
                type: string
                enum:
                - wav
                - aac
                - mp3
                - flac
                - opus
                - pcm16
                description: 'Specifies the output audio format. Must be one of `wav`,
                  `mp3`, `flac`,

                  `opus`, or `pcm16`.

                  '
          store:
            type: boolean
            default: false
            nullable: true
            description: "Whether or not to store the output of this chat completion\
              \ request for \nuse in our [model distillation](/docs/guides/distillation)\
              \ or\n[evals](/docs/guides/evals) products.\n"
          stream:
            description: 'If set to true, the model response data will be streamed
              to the client

              as it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).

              See the [Streaming section below](/docs/api-reference/chat/streaming)

              for more information, along with the [streaming responses](/docs/guides/streaming-responses)

              guide for more information on how to handle the streaming events.

              '
            type: boolean
            nullable: true
            default: false
          stop:
            $ref: '#/components/schemas/StopConfiguration'
          logit_bias:
            type: object
            x-oaiTypeLabel: map
            default: null
            nullable: true
            additionalProperties:
              type: integer
            description: 'Modify the likelihood of specified tokens appearing in the
              completion.


              Accepts a JSON object that maps tokens (specified by their token ID
              in the

              tokenizer) to an associated bias value from -100 to 100. Mathematically,

              the bias is added to the logits generated by the model prior to sampling.

              The exact effect will vary per model, but values between -1 and 1 should

              decrease or increase likelihood of selection; values like -100 or 100

              should result in a ban or exclusive selection of the relevant token.

              '
          logprobs:
            description: 'Whether to return log probabilities of the output tokens
              or not. If true,

              returns the log probabilities of each output token returned in the

              `content` of `message`.

              '
            type: boolean
            default: false
            nullable: true
          max_tokens:
            description: 'The maximum number of [tokens](/tokenizer) that can be generated
              in the

              chat completion. This value can be used to control

              [costs](https://openai.com/api/pricing/) for text generated via API.


              This value is now deprecated in favor of `max_completion_tokens`, and
              is

              not compatible with [o-series models](/docs/guides/reasoning).

              '
            type: integer
            nullable: true
            deprecated: true
          n:
            type: integer
            minimum: 1
            maximum: 128
            default: 1
            example: 1
            nullable: true
            description: How many chat completion choices to generate for each input
              message. Note that you will be charged based on the number of generated
              tokens across all of the choices. Keep `n` as `1` to minimize costs.
          prediction:
            nullable: true
            description: 'Configuration for a [Predicted Output](/docs/guides/predicted-outputs),

              which can greatly improve response times when large parts of the model

              response are known ahead of time. This is most common when you are

              regenerating a file with only minor changes to most of the content.

              '
            oneOf:
            - $ref: '#/components/schemas/PredictionContent'
          seed:
            type: integer
            minimum: -9223372036854776000
            maximum: 9223372036854776000
            nullable: true
            description: 'This feature is in Beta.

              If specified, our system will make a best effort to sample deterministically,
              such that repeated requests with the same `seed` and parameters should
              return the same result.

              Determinism is not guaranteed, and you should refer to the `system_fingerprint`
              response parameter to monitor changes in the backend.

              '
            x-oaiMeta:
              beta: true
          stream_options:
            $ref: '#/components/schemas/ChatCompletionStreamOptions'
          tools:
            type: array
            description: 'A list of tools the model may call. Currently, only functions
              are supported as a tool. Use this to provide a list of functions the
              model may generate JSON inputs for. A max of 128 functions are supported.

              '
            items:
              $ref: '#/components/schemas/ChatCompletionTool'
          tool_choice:
            $ref: '#/components/schemas/ChatCompletionToolChoiceOption'
          parallel_tool_calls:
            $ref: '#/components/schemas/ParallelToolCalls'
          function_call:
            deprecated: true
            description: 'Deprecated in favor of `tool_choice`.


              Controls which (if any) function is called by the model.


              `none` means the model will not call a function and instead generates
              a

              message.


              `auto` means the model can pick between generating a message or calling
              a

              function.


              Specifying a particular function via `{"name": "my_function"}` forces
              the

              model to call that function.


              `none` is the default when no functions are present. `auto` is the default

              if functions are present.

              '
            oneOf:
            - type: string
              description: '`none` means the model will not call a function and instead
                generates a message. `auto` means the model can pick between generating
                a message or calling a function.

                '
              enum:
              - none
              - auto
            - $ref: '#/components/schemas/ChatCompletionFunctionCallOption'
          functions:
            deprecated: true
            description: 'Deprecated in favor of `tools`.


              A list of functions the model may generate JSON inputs for.

              '
            type: array
            minItems: 1
            maxItems: 128
            items:
              $ref: '#/components/schemas/ChatCompletionFunctions'
        required:
        - model
        - messages
    CreateChatCompletionResponse:
      type: object
      description: Represents a chat completion response returned by model, based
        on the provided input.
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion.
        choices:
          type: array
          description: A list of chat completion choices. Can be more than one if
            `n` is greater than 1.
          items:
            type: object
            required:
            - finish_reason
            - index
            - message
            - logprobs
            properties:
              finish_reason:
                type: string
                description: 'The reason the model stopped generating tokens. This
                  will be `stop` if the model hit a natural stop point or a provided
                  stop sequence,

                  `length` if the maximum number of tokens specified in the request
                  was reached,

                  `content_filter` if content was omitted due to a flag from our content
                  filters,

                  `tool_calls` if the model called a tool, or `function_call` (deprecated)
                  if the model called a function.

                  '
                enum:
                - stop
                - length
                - tool_calls
                - content_filter
                - function_call
              index:
                type: integer
                description: The index of the choice in the list of choices.
              message:
                $ref: '#/components/schemas/ChatCompletionResponseMessage'
              logprobs:
                description: Log probability information for the choice.
                type: object
                nullable: true
                properties:
                  content:
                    description: A list of message content tokens with log probability
                      information.
                    type: array
                    items:
                      $ref: '#/components/schemas/ChatCompletionTokenLogprob'
                    nullable: true
                  refusal:
                    description: A list of message refusal tokens with log probability
                      information.
                    type: array
                    items:
                      $ref: '#/components/schemas/ChatCompletionTokenLogprob'
                    nullable: true
                required:
                - content
                - refusal
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the chat completion
            was created.
        model:
          type: string
          description: The model used for the chat completion.
        service_tier:
          $ref: '#/components/schemas/ServiceTier'
        system_fingerprint:
          type: string
          description: 'This fingerprint represents the backend configuration that
            the model runs with.


            Can be used in conjunction with the `seed` request parameter to understand
            when backend changes have been made that might impact determinism.

            '
        object:
          type: string
          description: The object type, which is always `chat.completion`.
          enum:
          - chat.completion
          x-stainless-const: true
        usage:
          $ref: '#/components/schemas/CompletionUsage'
      required:
      - choices
      - created
      - id
      - model
      - object
      x-oaiMeta:
        name: The chat completion object
        group: chat
        example: "{\n  \"id\": \"chatcmpl-B9MHDbslfkBeAs8l4bebGdFOJ6PeG\",\n  \"object\"\
          : \"chat.completion\",\n  \"created\": 1741570283,\n  \"model\": \"gpt-4o-2024-08-06\"\
          ,\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n\
          \        \"role\": \"assistant\",\n        \"content\": \"The image shows\
          \ a wooden boardwalk path running through a lush green field or meadow.\
          \ The sky is bright blue with some scattered clouds, giving the scene a\
          \ serene and peaceful atmosphere. Trees and shrubs are visible in the background.\"\
          ,\n        \"refusal\": null,\n        \"annotations\": []\n      },\n \
          \     \"logprobs\": null,\n      \"finish_reason\": \"stop\"\n    }\n  ],\n\
          \  \"usage\": {\n    \"prompt_tokens\": 1117,\n    \"completion_tokens\"\
          : 46,\n    \"total_tokens\": 1163,\n    \"prompt_tokens_details\": {\n \
          \     \"cached_tokens\": 0,\n      \"audio_tokens\": 0\n    },\n    \"completion_tokens_details\"\
          : {\n      \"reasoning_tokens\": 0,\n      \"audio_tokens\": 0,\n      \"\
          accepted_prediction_tokens\": 0,\n      \"rejected_prediction_tokens\":\
          \ 0\n    }\n  },\n  \"service_tier\": \"default\",\n  \"system_fingerprint\"\
          : \"fp_fc9f1d7035\"\n}\n"
    CreateChatCompletionStreamResponse:
      type: object
      description: "Represents a streamed chunk of a chat completion response returned\n\
        by the model, based on the provided input. \n[Learn more](/docs/guides/streaming-responses).\n"
      properties:
        id:
          type: string
          description: A unique identifier for the chat completion. Each chunk has
            the same ID.
        choices:
          type: array
          description: 'A list of chat completion choices. Can contain more than one
            elements if `n` is greater than 1. Can also be empty for the

            last chunk if you set `stream_options: {"include_usage": true}`.

            '
          items:
            type: object
            required:
            - delta
            - finish_reason
            - index
            properties:
              delta:
                $ref: '#/components/schemas/ChatCompletionStreamResponseDelta'
              logprobs:
                description: Log probability information for the choice.
                type: object
                nullable: true
                properties:
                  content:
                    description: A list of message content tokens with log probability
                      information.
                    type: array
                    items:
                      $ref: '#/components/schemas/ChatCompletionTokenLogprob'
                    nullable: true
                  refusal:
                    description: A list of message refusal tokens with log probability
                      information.
                    type: array
                    items:
                      $ref: '#/components/schemas/ChatCompletionTokenLogprob'
                    nullable: true
                required:
                - content
                - refusal
              finish_reason:
                type: string
                description: 'The reason the model stopped generating tokens. This
                  will be `stop` if the model hit a natural stop point or a provided
                  stop sequence,

                  `length` if the maximum number of tokens specified in the request
                  was reached,

                  `content_filter` if content was omitted due to a flag from our content
                  filters,

                  `tool_calls` if the model called a tool, or `function_call` (deprecated)
                  if the model called a function.

                  '
                enum:
                - stop
                - length
                - tool_calls
                - content_filter
                - function_call
                nullable: true
              index:
                type: integer
                description: The index of the choice in the list of choices.
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the chat completion
            was created. Each chunk has the same timestamp.
        model:
          type: string
          description: The model to generate the completion.
        service_tier:
          $ref: '#/components/schemas/ServiceTier'
        system_fingerprint:
          type: string
          description: 'This fingerprint represents the backend configuration that
            the model runs with.

            Can be used in conjunction with the `seed` request parameter to understand
            when backend changes have been made that might impact determinism.

            '
        object:
          type: string
          description: The object type, which is always `chat.completion.chunk`.
          enum:
          - chat.completion.chunk
          x-stainless-const: true
        usage:
          $ref: '#/components/schemas/CompletionUsage'
          nullable: true
          description: 'An optional field that will only be present when you set

            `stream_options: {"include_usage": true}` in your request. When present,
            it

            contains a null value **except for the last chunk** which contains the

            token usage statistics for the entire request.


            **NOTE:** If the stream is interrupted or cancelled, you may not

            receive the final usage chunk which contains the total token usage for

            the request.

            '
      required:
      - choices
      - created
      - id
      - model
      - object
      x-oaiMeta:
        name: The chat completion chunk object
        group: chat
        example: '{"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
          "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}


          {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
          "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"content":"Hello"},"logprobs":null,"finish_reason":null}]}


          ....


          {"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini",
          "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{},"logprobs":null,"finish_reason":"stop"}]}

          '
    CreateCompletionRequest:
      type: object
      properties:
        model:
          description: 'ID of the model to use. You can use the [List models](/docs/api-reference/models/list)
            API to see all of your available models, or see our [Model overview](/docs/models)
            for descriptions of them.

            '
          anyOf:
          - type: string
          - type: string
            enum:
            - gpt-3.5-turbo-instruct
            - davinci-002
            - babbage-002
          x-oaiTypeLabel: string
        prompt:
          description: 'The prompt(s) to generate completions for, encoded as a string,
            array of strings, array of tokens, or array of token arrays.


            Note that <|endoftext|> is the document separator that the model sees
            during training, so if a prompt is not specified the model will generate
            as if from the beginning of a new document.

            '
          default: <|endoftext|>
          nullable: true
          oneOf:
          - type: string
            default: ''
            example: This is a test.
          - type: array
            items:
              type: string
              default: ''
              example: This is a test.
          - type: array
            minItems: 1
            items:
              type: integer
            example: '[1212, 318, 257, 1332, 13]'
          - type: array
            minItems: 1
            items:
              type: array
              minItems: 1
              items:
                type: integer
            example: '[[1212, 318, 257, 1332, 13]]'
        best_of:
          type: integer
          default: 1
          minimum: 0
          maximum: 20
          nullable: true
          description: 'Generates `best_of` completions server-side and returns the
            "best" (the one with the highest log probability per token). Results cannot
            be streamed.


            When used with `n`, `best_of` controls the number of candidate completions
            and `n` specifies how many to return – `best_of` must be greater than
            `n`.


            **Note:** Because this parameter generates many completions, it can quickly
            consume your token quota. Use carefully and ensure that you have reasonable
            settings for `max_tokens` and `stop`.

            '
        echo:
          type: boolean
          default: false
          nullable: true
          description: 'Echo back the prompt in addition to the completion

            '
        frequency_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: 'Number between -2.0 and 2.0. Positive values penalize new
            tokens based on their existing frequency in the text so far, decreasing
            the model''s likelihood to repeat the same line verbatim.


            [See more information about frequency and presence penalties.](/docs/guides/text-generation)

            '
        logit_bias:
          type: object
          x-oaiTypeLabel: map
          default: null
          nullable: true
          additionalProperties:
            type: integer
          description: 'Modify the likelihood of specified tokens appearing in the
            completion.


            Accepts a JSON object that maps tokens (specified by their token ID in
            the GPT tokenizer) to an associated bias value from -100 to 100. You can
            use this [tokenizer tool](/tokenizer?view=bpe) to convert text to token
            IDs. Mathematically, the bias is added to the logits generated by the
            model prior to sampling. The exact effect will vary per model, but values
            between -1 and 1 should decrease or increase likelihood of selection;
            values like -100 or 100 should result in a ban or exclusive selection
            of the relevant token.


            As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|>
            token from being generated.

            '
        logprobs:
          type: integer
          minimum: 0
          maximum: 5
          default: null
          nullable: true
          description: 'Include the log probabilities on the `logprobs` most likely
            output tokens, as well the chosen tokens. For example, if `logprobs` is
            5, the API will return a list of the 5 most likely tokens. The API will
            always return the `logprob` of the sampled token, so there may be up to
            `logprobs+1` elements in the response.


            The maximum value for `logprobs` is 5.

            '
        max_tokens:
          type: integer
          minimum: 0
          default: 16
          example: 16
          nullable: true
          description: 'The maximum number of [tokens](/tokenizer) that can be generated
            in the completion.


            The token count of your prompt plus `max_tokens` cannot exceed the model''s
            context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)
            for counting tokens.

            '
        n:
          type: integer
          minimum: 1
          maximum: 128
          default: 1
          example: 1
          nullable: true
          description: 'How many completions to generate for each prompt.


            **Note:** Because this parameter generates many completions, it can quickly
            consume your token quota. Use carefully and ensure that you have reasonable
            settings for `max_tokens` and `stop`.

            '
        presence_penalty:
          type: number
          default: 0
          minimum: -2
          maximum: 2
          nullable: true
          description: 'Number between -2.0 and 2.0. Positive values penalize new
            tokens based on whether they appear in the text so far, increasing the
            model''s likelihood to talk about new topics.


            [See more information about frequency and presence penalties.](/docs/guides/text-generation)

            '
        seed:
          type: integer
          format: int64
          nullable: true
          description: 'If specified, our system will make a best effort to sample
            deterministically, such that repeated requests with the same `seed` and
            parameters should return the same result.


            Determinism is not guaranteed, and you should refer to the `system_fingerprint`
            response parameter to monitor changes in the backend.

            '
        stop:
          $ref: '#/components/schemas/StopConfiguration'
        stream:
          description: 'Whether to stream back partial progress. If set, tokens will
            be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)
            as they become available, with the stream terminated by a `data: [DONE]`
            message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).

            '
          type: boolean
          nullable: true
          default: false
        stream_options:
          $ref: '#/components/schemas/ChatCompletionStreamOptions'
        suffix:
          description: 'The suffix that comes after a completion of inserted text.


            This parameter is only supported for `gpt-3.5-turbo-instruct`.

            '
          default: null
          nullable: true
          type: string
          example: test.
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
          description: 'What sampling temperature to use, between 0 and 2. Higher
            values like 0.8 will make the output more random, while lower values like
            0.2 will make it more focused and deterministic.


            We generally recommend altering this or `top_p` but not both.

            '
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: 'An alternative to sampling with temperature, called nucleus
            sampling, where the model considers the results of the tokens with top_p
            probability mass. So 0.1 means only the tokens comprising the top 10%
            probability mass are considered.


            We generally recommend altering this or `temperature` but not both.

            '
        user:
          type: string
          example: user-1234
          description: 'A unique identifier representing your end-user, which can
            help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).

            '
      required:
      - model
      - prompt
    CreateCompletionResponse:
      type: object
      description: 'Represents a completion response from the API. Note: both the
        streamed and non-streamed response objects share the same shape (unlike the
        chat endpoint).

        '
      properties:
        id:
          type: string
          description: A unique identifier for the completion.
        choices:
          type: array
          description: The list of completion choices the model generated for the
            input prompt.
          items:
            type: object
            required:
            - finish_reason
            - index
            - logprobs
            - text
            properties:
              finish_reason:
                type: string
                description: 'The reason the model stopped generating tokens. This
                  will be `stop` if the model hit a natural stop point or a provided
                  stop sequence,

                  `length` if the maximum number of tokens specified in the request
                  was reached,

                  or `content_filter` if content was omitted due to a flag from our
                  content filters.

                  '
                enum:
                - stop
                - length
                - content_filter
              index:
                type: integer
              logprobs:
                type: object
                nullable: true
                properties:
                  text_offset:
                    type: array
                    items:
                      type: integer
                  token_logprobs:
                    type: array
                    items:
                      type: number
                  tokens:
                    type: array
                    items:
                      type: string
                  top_logprobs:
                    type: array
                    items:
                      type: object
                      additionalProperties:
                        type: number
              text:
                type: string
        created:
          type: integer
          description: The Unix timestamp (in seconds) of when the completion was
            created.
        model:
          type: string
          description: The model used for completion.
        system_fingerprint:
          type: string
          description: 'This fingerprint represents the backend configuration that
            the model runs with.


            Can be used in conjunction with the `seed` request parameter to understand
            when backend changes have been made that might impact determinism.

            '
        object:
          type: string
          description: The object type, which is always "text_completion"
          enum:
          - text_completion
          x-stainless-const: true
        usage:
          $ref: '#/components/schemas/CompletionUsage'
      required:
      - id
      - object
      - created
      - model
      - choices
      x-oaiMeta:
        name: The completion object
        legacy: true
        example: "{\n  \"id\": \"cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7\",\n  \"object\": \"\
          text_completion\",\n  \"created\": 1589478378,\n  \"model\": \"gpt-4-turbo\"\
          ,\n  \"choices\": [\n    {\n      \"text\": \"\\n\\nThis is indeed a test\"\
          ,\n      \"index\": 0,\n      \"logprobs\": null,\n      \"finish_reason\"\
          : \"length\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 5,\n \
          \   \"completion_tokens\": 7,\n    \"total_tokens\": 12\n  }\n}\n"
    CreateModelResponseProperties:
      allOf:
      - $ref: '#/components/schemas/ModelResponseProperties'
    DeleteModelResponse:
      type: object
      properties:
        id:
          type: string
        deleted:
          type: boolean
        object:
          type: string
      required:
      - id
      - object
      - deleted
    FunctionObject:
      type: object
      properties:
        description:
          type: string
          description: A description of what the function does, used by the model
            to choose when and how to call the function.
        name:
          type: string
          description: The name of the function to be called. Must be a-z, A-Z, 0-9,
            or contain underscores and dashes, with a maximum length of 64.
        parameters:
          $ref: '#/components/schemas/FunctionParameters'
        strict:
          type: boolean
          nullable: true
          default: false
          description: Whether to enable strict schema adherence when generating the
            function call. If set to true, the model will follow the exact schema
            defined in the `parameters` field. Only a subset of JSON Schema is supported
            when `strict` is `true`. Learn more about Structured Outputs in the [function
            calling guide](docs/guides/function-calling).
      required:
      - name
    FunctionParameters:
      type: object
      description: "The parameters the functions accepts, described as a JSON Schema\
        \ object. See the [guide](/docs/guides/function-calling) for examples, and\
        \ the [JSON Schema reference](https://json-schema.org/understanding-json-schema/)\
        \ for documentation about the format. \n\nOmitting `parameters` defines a\
        \ function with an empty parameter list."
      additionalProperties: true
    ListModelsResponse:
      type: object
      properties:
        object:
          type: string
          enum:
          - list
          x-stainless-const: true
        data:
          type: array
          items:
            $ref: '#/components/schemas/Model'
      required:
      - object
      - data
    Metadata:
      type: object
      description: "Set of 16 key-value pairs that can be attached to an object. This\
        \ can be\nuseful for storing additional information about the object in a\
        \ structured\nformat, and querying for objects via API or the dashboard. \n\
        \nKeys are strings with a maximum length of 64 characters. Values are strings\n\
        with a maximum length of 512 characters.\n"
      additionalProperties:
        type: string
      x-oaiTypeLabel: map
      nullable: true
    Model:
      title: Model
      description: Describes an OpenAI model offering that can be used with the API.
      properties:
        id:
          type: string
          description: The model identifier, which can be referenced in the API endpoints.
        created:
          type: integer
          description: The Unix timestamp (in seconds) when the model was created.
        object:
          type: string
          description: The object type, which is always "model".
          enum:
          - model
          x-stainless-const: true
        owned_by:
          type: string
          description: The organization that owns the model.
      required:
      - id
      - object
      - created
      - owned_by
      x-oaiMeta:
        name: The model object
        example: "{\n  \"id\": \"VAR_chat_model_id\",\n  \"object\": \"model\",\n\
          \  \"created\": 1686935002,\n  \"owned_by\": \"openai\"\n}\n"
    ModelIdsShared:
      example: gpt-4o
      anyOf:
      - type: string
      - type: string
        enum:
        - gpt-4.1
        - gpt-4.1-mini
        - gpt-4.1-nano
        - gpt-4.1-2025-04-14
        - gpt-4.1-mini-2025-04-14
        - gpt-4.1-nano-2025-04-14
        - o4-mini
        - o4-mini-2025-04-16
        - o3
        - o3-2025-04-16
        - o3-mini
        - o3-mini-2025-01-31
        - o1
        - o1-2024-12-17
        - o1-preview
        - o1-preview-2024-09-12
        - o1-mini
        - o1-mini-2024-09-12
        - gpt-4o
        - gpt-4o-2024-11-20
        - gpt-4o-2024-08-06
        - gpt-4o-2024-05-13
        - gpt-4o-audio-preview
        - gpt-4o-audio-preview-2024-10-01
        - gpt-4o-audio-preview-2024-12-17
        - gpt-4o-mini-audio-preview
        - gpt-4o-mini-audio-preview-2024-12-17
        - gpt-4o-search-preview
        - gpt-4o-mini-search-preview
        - gpt-4o-search-preview-2025-03-11
        - gpt-4o-mini-search-preview-2025-03-11
        - chatgpt-4o-latest
        - gpt-4o-mini
        - gpt-4o-mini-2024-07-18
        - gpt-4-turbo
        - gpt-4-turbo-2024-04-09
        - gpt-4-0125-preview
        - gpt-4-turbo-preview
        - gpt-4-1106-preview
        - gpt-4-vision-preview
        - gpt-4
        - gpt-4-0314
        - gpt-4-0613
        - gpt-4-32k
        - gpt-4-32k-0314
        - gpt-4-32k-0613
        - gpt-3.5-turbo
        - gpt-3.5-turbo-16k
        - gpt-3.5-turbo-0301
        - gpt-3.5-turbo-0613
        - gpt-3.5-turbo-1106
        - gpt-3.5-turbo-0125
        - gpt-3.5-turbo-16k-0613
    ModelResponseProperties:
      type: object
      properties:
        metadata:
          $ref: '#/components/schemas/Metadata'
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 1
          example: 1
          nullable: true
          description: 'What sampling temperature to use, between 0 and 2. Higher
            values like 0.8 will make the output more random, while lower values like
            0.2 will make it more focused and deterministic.

            We generally recommend altering this or `top_p` but not both.

            '
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 1
          example: 1
          nullable: true
          description: 'An alternative to sampling with temperature, called nucleus
            sampling,

            where the model considers the results of the tokens with top_p probability

            mass. So 0.1 means only the tokens comprising the top 10% probability
            mass

            are considered.


            We generally recommend altering this or `temperature` but not both.

            '
        user:
          type: string
          example: user-1234
          description: 'A unique identifier representing your end-user, which can
            help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).

            '
        service_tier:
          $ref: '#/components/schemas/ServiceTier'
    ParallelToolCalls:
      description: Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling)
        during tool use.
      type: boolean
      default: true
    PredictionContent:
      type: object
      title: Static Content
      description: 'Static predicted output content, such as the content of a text
        file that is

        being regenerated.

        '
      required:
      - type
      - content
      properties:
        type:
          type: string
          enum:
          - content
          description: 'The type of the predicted content you want to provide. This
            type is

            currently always `content`.

            '
          x-stainless-const: true
        content:
          description: 'The content that should be matched when generating a model
            response.

            If generated tokens would match this content, the entire model response

            can be returned much more quickly.

            '
          oneOf:
          - type: string
            title: Text content
            description: 'The content used for a Predicted Output. This is often the

              text of a file you are regenerating with minor changes.

              '
          - type: array
            description: An array of content parts with a defined type. Supported
              options differ based on the [model](/docs/models) being used to generate
              the response. Can contain text inputs.
            title: Array of content parts
            items:
              $ref: '#/components/schemas/ChatCompletionRequestMessageContentPartText'
            minItems: 1
    ReasoningEffort:
      type: string
      enum:
      - low
      - medium
      - high
      default: medium
      nullable: true
      description: "**o-series models only** \n\nConstrains effort on reasoning for\
        \ \n[reasoning models](https://platform.openai.com/docs/guides/reasoning).\n\
        Currently supported values are `low`, `medium`, and `high`. Reducing\nreasoning\
        \ effort can result in faster responses and fewer tokens used\non reasoning\
        \ in a response.\n"
    ResponseFormatJsonObject:
      type: object
      title: JSON object
      description: 'JSON object response format. An older method of generating JSON
        responses.

        Using `json_schema` is recommended for models that support it. Note that the

        model will not generate JSON without a system or user message instructing
        it

        to do so.

        '
      properties:
        type:
          type: string
          description: The type of response format being defined. Always `json_object`.
          enum:
          - json_object
          x-stainless-const: true
      required:
      - type
    ResponseFormatJsonSchema:
      type: object
      title: JSON schema
      description: 'JSON Schema response format. Used to generate structured JSON
        responses.

        Learn more about [Structured Outputs](/docs/guides/structured-outputs).

        '
      properties:
        type:
          type: string
          description: The type of response format being defined. Always `json_schema`.
          enum:
          - json_schema
          x-stainless-const: true
        json_schema:
          type: object
          title: JSON schema
          description: 'Structured Outputs configuration options, including a JSON
            Schema.

            '
          properties:
            description:
              type: string
              description: 'A description of what the response format is for, used
                by the model to

                determine how to respond in the format.

                '
            name:
              type: string
              description: 'The name of the response format. Must be a-z, A-Z, 0-9,
                or contain

                underscores and dashes, with a maximum length of 64.

                '
            schema:
              $ref: '#/components/schemas/ResponseFormatJsonSchemaSchema'
            strict:
              type: boolean
              nullable: true
              default: false
              description: 'Whether to enable strict schema adherence when generating
                the output.

                If set to true, the model will always follow the exact schema defined

                in the `schema` field. Only a subset of JSON Schema is supported when

                `strict` is `true`. To learn more, read the [Structured Outputs

                guide](/docs/guides/structured-outputs).

                '
          required:
          - name
      required:
      - type
      - json_schema
    ResponseFormatJsonSchemaSchema:
      type: object
      title: JSON schema
      description: 'The schema for the response format, described as a JSON Schema
        object.

        Learn how to build JSON schemas [here](https://json-schema.org/).

        '
      additionalProperties: true
    ResponseFormatText:
      type: object
      title: Text
      description: 'Default response format. Used to generate text responses.

        '
      properties:
        type:
          type: string
          description: The type of response format being defined. Always `text`.
          enum:
          - text
          x-stainless-const: true
      required:
      - type
    ResponseModalities:
      type: array
      nullable: true
      description: "Output types that you would like the model to generate.\nMost\
        \ models are capable of generating text, which is the default:\n\n`[\"text\"\
        ]`\n\nThe `gpt-4o-audio-preview` model can also be used to \n[generate audio](/docs/guides/audio).\
        \ To request that this model generate \nboth text and audio responses, you\
        \ can use:\n\n`[\"text\", \"audio\"]`\n"
      items:
        type: string
        enum:
        - text
        - audio
    ServiceTier:
      type: string
      description: "Specifies the latency tier to use for processing the request.\
        \ This parameter is relevant for customers subscribed to the scale tier service:\n\
        \  - If set to 'auto', and the Project is Scale tier enabled, the system\n\
        \    will utilize scale tier credits until they are exhausted.\n  - If set\
        \ to 'auto', and the Project is not Scale tier enabled, the request will be\
        \ processed using the default service tier with a lower uptime SLA and no\
        \ latency guarentee.\n  - If set to 'default', the request will be processed\
        \ using the default service tier with a lower uptime SLA and no latency guarentee.\n\
        \  - If set to 'flex', the request will be processed with the Flex Processing\
        \ service tier. [Learn more](/docs/guides/flex-processing).\n  - When not\
        \ set, the default behavior is 'auto'.\n\n  When this parameter is set, the\
        \ response body will include the `service_tier` utilized.\n"
      enum:
      - auto
      - default
      - flex
      nullable: true
      default: auto
    StopConfiguration:
      description: 'Not supported with latest reasoning models `o3` and `o4-mini`.


        Up to 4 sequences where the API will stop generating further tokens. The

        returned text will not contain the stop sequence.

        '
      default: null
      nullable: true
      oneOf:
      - type: string
        default: <|endoftext|>
        example: '

          '
        nullable: true
      - type: array
        minItems: 1
        maxItems: 4
        items:
          type: string
          example: '["\n"]'
    VoiceIdsShared:
      example: ash
      anyOf:
      - type: string
      - type: string
        enum:
        - alloy
        - ash
        - ballad
        - coral
        - echo
        - fable
        - onyx
        - nova
        - sage
        - shimmer
        - verse
    WebSearchContextSize:
      type: string
      description: "High level guidance for the amount of context window space to\
        \ use for the \nsearch. One of `low`, `medium`, or `high`. `medium` is the\
        \ default.\n"
      enum:
      - low
      - medium
      - high
      default: medium
    WebSearchLocation:
      type: object
      title: Web search location
      description: Approximate location parameters for the search.
      properties:
        country:
          type: string
          description: "The two-letter \n[ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1)\
            \ of the user,\ne.g. `US`.\n"
        region:
          type: string
          description: 'Free text input for the region of the user, e.g. `California`.

            '
        city:
          type: string
          description: 'Free text input for the city of the user, e.g. `San Francisco`.

            '
        timezone:
          type: string
          description: "The [IANA timezone](https://timeapi.io/documentation/iana-timezones)\
            \ \nof the user, e.g. `America/Los_Angeles`.\n"
  securitySchemes:
    ApiKeyAuth:
      type: http
      scheme: bearer
x-oaiMeta:
  navigationGroups:
  - id: responses
    title: Responses
  - id: chat
    title: Chat Completions
  - id: realtime
    title: Realtime
    beta: true
  - id: endpoints
    title: Platform APIs
  - id: vector_stores
    title: Vector stores
  - id: assistants
    title: Assistants
    beta: true
  - id: administration
    title: Administration
  - id: legacy
    title: Legacy
  groups:
  - id: responses
    title: Responses
    description: 'OpenAI''s most advanced interface for generating model responses.
      Supports

      text and image inputs, and text outputs. Create stateful interactions

      with the model, using the output of previous responses as input. Extend

      the model''s capabilities with built-in tools for file search, web search,

      computer use, and more. Allow the model access to external systems and data

      using function calling.


      Related guides:

      - [Quickstart](/docs/quickstart?api-mode=responses)

      - [Text inputs and outputs](/docs/guides/text?api-mode=responses)

      - [Image inputs](/docs/guides/images?api-mode=responses)

      - [Structured Outputs](/docs/guides/structured-outputs?api-mode=responses)

      - [Function calling](/docs/guides/function-calling?api-mode=responses)

      - [Conversation state](/docs/guides/conversation-state?api-mode=responses)

      - [Extend the models with tools](/docs/guides/tools?api-mode=responses)

      '
    navigationGroup: responses
    sections:
    - type: endpoint
      key: createResponse
      path: create
    - type: endpoint
      key: getResponse
      path: get
    - type: endpoint
      key: deleteResponse
      path: delete
    - type: endpoint
      key: listInputItems
      path: input-items
    - type: object
      key: Response
      path: object
    - type: object
      key: ResponseItemList
      path: list
  - id: responses-streaming
    title: Streaming
    description: 'When you [create a Response](/docs/api-reference/responses/create)
      with

      `stream` set to `true`, the server will emit server-sent events to the

      client as the Response is generated. This section contains the events that

      are emitted by the server.


      [Learn more about streaming responses](/docs/guides/streaming-responses?api-mode=responses).

      '
    navigationGroup: responses
    sections:
    - type: object
      key: ResponseCreatedEvent
      path: <auto>
    - type: object
      key: ResponseInProgressEvent
      path: <auto>
    - type: object
      key: ResponseCompletedEvent
      path: <auto>
    - type: object
      key: ResponseFailedEvent
      path: <auto>
    - type: object
      key: ResponseIncompleteEvent
      path: <auto>
    - type: object
      key: ResponseOutputItemAddedEvent
      path: <auto>
    - type: object
      key: ResponseOutputItemDoneEvent
      path: <auto>
    - type: object
      key: ResponseContentPartAddedEvent
      path: <auto>
    - type: object
      key: ResponseContentPartDoneEvent
      path: <auto>
    - type: object
      key: ResponseTextDeltaEvent
      path: <auto>
    - type: object
      key: ResponseTextAnnotationDeltaEvent
      path: <auto>
    - type: object
      key: ResponseTextDoneEvent
      path: <auto>
    - type: object
      key: ResponseRefusalDeltaEvent
      path: <auto>
    - type: object
      key: ResponseRefusalDoneEvent
      path: <auto>
    - type: object
      key: ResponseFunctionCallArgumentsDeltaEvent
      path: <auto>
    - type: object
      key: ResponseFunctionCallArgumentsDoneEvent
      path: <auto>
    - type: object
      key: ResponseFileSearchCallInProgressEvent
      path: <auto>
    - type: object
      key: ResponseFileSearchCallSearchingEvent
      path: <auto>
    - type: object
      key: ResponseFileSearchCallCompletedEvent
      path: <auto>
    - type: object
      key: ResponseWebSearchCallInProgressEvent
      path: <auto>
    - type: object
      key: ResponseWebSearchCallSearchingEvent
      path: <auto>
    - type: object
      key: ResponseWebSearchCallCompletedEvent
      path: <auto>
    - type: object
      key: ResponseReasoningSummaryPartAddedEvent
      path: <auto>
    - type: object
      key: ResponseReasoningSummaryPartDoneEvent
      path: <auto>
    - type: object
      key: ResponseReasoningSummaryTextDeltaEvent
      path: <auto>
    - type: object
      key: ResponseReasoningSummaryTextDoneEvent
      path: <auto>
    - type: object
      key: ResponseErrorEvent
      path: <auto>
  - id: chat
    title: Chat Completions
    description: 'The Chat Completions API endpoint will generate a model response
      from a

      list of messages comprising a conversation.


      Related guides:

      - [Quickstart](/docs/quickstart?api-mode=chat)

      - [Text inputs and outputs](/docs/guides/text?api-mode=chat)

      - [Image inputs](/docs/guides/images?api-mode=chat)

      - [Audio inputs and outputs](/docs/guides/audio?api-mode=chat)

      - [Structured Outputs](/docs/guides/structured-outputs?api-mode=chat)

      - [Function calling](/docs/guides/function-calling?api-mode=chat)

      - [Conversation state](/docs/guides/conversation-state?api-mode=chat)


      **Starting a new project?** We recommend trying [Responses](/docs/api-reference/responses)

      to take advantage of the latest OpenAI platform features. Compare

      [Chat Completions with Responses](/docs/guides/responses-vs-chat-completions?api-mode=responses).

      '
    navigationGroup: chat
    sections:
    - type: endpoint
      key: createChatCompletion
      path: create
    - type: endpoint
      key: getChatCompletion
      path: get
    - type: endpoint
      key: getChatCompletionMessages
      path: getMessages
    - type: endpoint
      key: listChatCompletions
      path: list
    - type: endpoint
      key: updateChatCompletion
      path: update
    - type: endpoint
      key: deleteChatCompletion
      path: delete
    - type: object
      key: CreateChatCompletionResponse
      path: object
    - type: object
      key: ChatCompletionList
      path: list-object
    - type: object
      key: ChatCompletionMessageList
      path: message-list
  - id: chat-streaming
    title: Streaming
    description: 'Stream Chat Completions in real time. Receive chunks of completions

      returned from the model using server-sent events.

      [Learn more](/docs/guides/streaming-responses?api-mode=chat).

      '
    navigationGroup: chat
    sections:
    - type: object
      key: CreateChatCompletionStreamResponse
      path: streaming
  - id: realtime
    title: Realtime
    beta: true
    description: 'Communicate with a GPT-4o class model in real time using WebRTC
      or

      WebSockets. Supports text and audio inputs and ouputs, along with audio

      transcriptions.

      [Learn more about the Realtime API](/docs/guides/realtime).

      '
    navigationGroup: realtime
  - id: realtime-sessions
    title: Session tokens
    description: 'REST API endpoint to generate ephemeral session tokens for use in
      client-side

      applications.

      '
    navigationGroup: realtime
    sections:
    - type: endpoint
      key: create-realtime-session
      path: create
    - type: endpoint
      key: create-realtime-transcription-session
      path: create-transcription
    - type: object
      key: RealtimeSessionCreateResponse
      path: session_object
    - type: object
      key: RealtimeTranscriptionSessionCreateResponse
      path: transcription_session_object
  - id: realtime-client-events
    title: Client events
    description: 'These are events that the OpenAI Realtime WebSocket server will
      accept from the client.

      '
    navigationGroup: realtime
    sections:
    - type: object
      key: RealtimeClientEventSessionUpdate
      path: <auto>
    - type: object
      key: RealtimeClientEventInputAudioBufferAppend
      path: <auto>
    - type: object
      key: RealtimeClientEventInputAudioBufferCommit
      path: <auto>
    - type: object
      key: RealtimeClientEventInputAudioBufferClear
      path: <auto>
    - type: object
      key: RealtimeClientEventConversationItemCreate
      path: <auto>
    - type: object
      key: RealtimeClientEventConversationItemRetrieve
      path: <auto>
    - type: object
      key: RealtimeClientEventConversationItemTruncate
      path: <auto>
    - type: object
      key: RealtimeClientEventConversationItemDelete
      path: <auto>
    - type: object
      key: RealtimeClientEventResponseCreate
      path: <auto>
    - type: object
      key: RealtimeClientEventResponseCancel
      path: <auto>
    - type: object
      key: RealtimeClientEventTranscriptionSessionUpdate
      path: <auto>
    - type: object
      key: RealtimeClientEventOutputAudioBufferClear
      path: <auto>
  - id: realtime-server-events
    title: Server events
    description: 'These are events emitted from the OpenAI Realtime WebSocket server
      to the client.

      '
    navigationGroup: realtime
    sections:
    - type: object
      key: RealtimeServerEventError
      path: <auto>
    - type: object
      key: RealtimeServerEventSessionCreated
      path: <auto>
    - type: object
      key: RealtimeServerEventSessionUpdated
      path: <auto>
    - type: object
      key: RealtimeServerEventConversationCreated
      path: <auto>
    - type: object
      key: RealtimeServerEventConversationItemCreated
      path: <auto>
    - type: object
      key: RealtimeServerEventConversationItemRetrieved
      path: <auto>
    - type: object
      key: RealtimeServerEventConversationItemInputAudioTranscriptionCompleted
      path: <auto>
    - type: object
      key: RealtimeServerEventConversationItemInputAudioTranscriptionDelta
      path: <auto>
    - type: object
      key: RealtimeServerEventConversationItemInputAudioTranscriptionFailed
      path: <auto>
    - type: object
      key: RealtimeServerEventConversationItemTruncated
      path: <auto>
    - type: object
      key: RealtimeServerEventConversationItemDeleted
      path: <auto>
    - type: object
      key: RealtimeServerEventInputAudioBufferCommitted
      path: <auto>
    - type: object
      key: RealtimeServerEventInputAudioBufferCleared
      path: <auto>
    - type: object
      key: RealtimeServerEventInputAudioBufferSpeechStarted
      path: <auto>
    - type: object
      key: RealtimeServerEventInputAudioBufferSpeechStopped
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseCreated
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseDone
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseOutputItemAdded
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseOutputItemDone
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseContentPartAdded
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseContentPartDone
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseTextDelta
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseTextDone
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseAudioTranscriptDelta
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseAudioTranscriptDone
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseAudioDelta
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseAudioDone
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseFunctionCallArgumentsDelta
      path: <auto>
    - type: object
      key: RealtimeServerEventResponseFunctionCallArgumentsDone
      path: <auto>
    - type: object
      key: RealtimeServerEventTranscriptionSessionUpdated
      path: <auto>
    - type: object
      key: RealtimeServerEventRateLimitsUpdated
      path: <auto>
    - type: object
      key: RealtimeServerEventOutputAudioBufferStarted
      path: <auto>
    - type: object
      key: RealtimeServerEventOutputAudioBufferStopped
      path: <auto>
    - type: object
      key: RealtimeServerEventOutputAudioBufferCleared
      path: <auto>
  - id: audio
    title: Audio
    description: 'Learn how to turn audio into text or text into audio.


      Related guide: [Speech to text](/docs/guides/speech-to-text)

      '
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createSpeech
      path: createSpeech
    - type: endpoint
      key: createTranscription
      path: createTranscription
    - type: endpoint
      key: createTranslation
      path: createTranslation
    - type: object
      key: CreateTranscriptionResponseJson
      path: json-object
    - type: object
      key: CreateTranscriptionResponseVerboseJson
      path: verbose-json-object
    - type: object
      key: TranscriptTextDeltaEvent
      path: transcript-text-delta-event
    - type: object
      key: TranscriptTextDoneEvent
      path: transcript-text-done-event
  - id: images
    title: Images
    description: 'Given a prompt and/or an input image, the model will generate a
      new image.

      Related guide: [Image generation](/docs/guides/images)

      '
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createImage
      path: create
    - type: endpoint
      key: createImageEdit
      path: createEdit
    - type: endpoint
      key: createImageVariation
      path: createVariation
    - type: object
      key: ImagesResponse
      path: object
  - id: embeddings
    title: Embeddings
    description: 'Get a vector representation of a given input that can be easily
      consumed by machine learning models and algorithms.

      Related guide: [Embeddings](/docs/guides/embeddings)

      '
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createEmbedding
      path: create
    - type: object
      key: Embedding
      path: object
  - id: evals
    title: Evals
    description: 'Create, manage, and run evals in the OpenAI platform.

      Related guide: [Evals](/docs/guides/evals)

      '
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createEval
      path: create
    - type: endpoint
      key: getEval
      path: get
    - type: endpoint
      key: updateEval
      path: update
    - type: endpoint
      key: deleteEval
      path: delete
    - type: endpoint
      key: listEvals
      path: list
    - type: endpoint
      key: getEvalRuns
      path: getRuns
    - type: endpoint
      key: getEvalRun
      path: getRun
    - type: endpoint
      key: createEvalRun
      path: createRun
    - type: endpoint
      key: cancelEvalRun
      path: cancelRun
    - type: endpoint
      key: deleteEvalRun
      path: deleteRun
    - type: endpoint
      key: getEvalRunOutputItem
      path: getRunOutputItem
    - type: endpoint
      key: getEvalRunOutputItems
      path: getRunOutputItems
    - type: object
      key: Eval
      path: object
    - type: object
      key: EvalRun
      path: run-object
    - type: object
      key: EvalRunOutputItem
      path: run-output-item-object
  - id: fine-tuning
    title: Fine-tuning
    description: 'Manage fine-tuning jobs to tailor a model to your specific training
      data.

      Related guide: [Fine-tune models](/docs/guides/fine-tuning)

      '
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createFineTuningJob
      path: create
    - type: endpoint
      key: listPaginatedFineTuningJobs
      path: list
    - type: endpoint
      key: listFineTuningEvents
      path: list-events
    - type: endpoint
      key: listFineTuningJobCheckpoints
      path: list-checkpoints
    - type: endpoint
      key: listFineTuningCheckpointPermissions
      path: list-permissions
    - type: endpoint
      key: createFineTuningCheckpointPermission
      path: create-permission
    - type: endpoint
      key: deleteFineTuningCheckpointPermission
      path: delete-permission
    - type: endpoint
      key: retrieveFineTuningJob
      path: retrieve
    - type: endpoint
      key: cancelFineTuningJob
      path: cancel
    - type: object
      key: FineTuneChatRequestInput
      path: chat-input
    - type: object
      key: FineTunePreferenceRequestInput
      path: preference-input
    - type: object
      key: FineTuneCompletionRequestInput
      path: completions-input
    - type: object
      key: FineTuningJob
      path: object
    - type: object
      key: FineTuningJobEvent
      path: event-object
    - type: object
      key: FineTuningJobCheckpoint
      path: checkpoint-object
    - type: object
      key: FineTuningCheckpointPermission
      path: permission-object
  - id: batch
    title: Batch
    description: 'Create large batches of API requests for asynchronous processing.
      The Batch API returns completions within 24 hours for a 50% discount.

      Related guide: [Batch](/docs/guides/batch)

      '
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createBatch
      path: create
    - type: endpoint
      key: retrieveBatch
      path: retrieve
    - type: endpoint
      key: cancelBatch
      path: cancel
    - type: endpoint
      key: listBatches
      path: list
    - type: object
      key: Batch
      path: object
    - type: object
      key: BatchRequestInput
      path: request-input
    - type: object
      key: BatchRequestOutput
      path: request-output
  - id: files
    title: Files
    description: 'Files are used to upload documents that can be used with features
      like [Assistants](/docs/api-reference/assistants), [Fine-tuning](/docs/api-reference/fine-tuning),
      and [Batch API](/docs/guides/batch).

      '
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createFile
      path: create
    - type: endpoint
      key: listFiles
      path: list
    - type: endpoint
      key: retrieveFile
      path: retrieve
    - type: endpoint
      key: deleteFile
      path: delete
    - type: endpoint
      key: downloadFile
      path: retrieve-contents
    - type: object
      key: OpenAIFile
      path: object
  - id: uploads
    title: Uploads
    description: 'Allows you to upload large files in multiple parts.

      '
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createUpload
      path: create
    - type: endpoint
      key: addUploadPart
      path: add-part
    - type: endpoint
      key: completeUpload
      path: complete
    - type: endpoint
      key: cancelUpload
      path: cancel
    - type: object
      key: Upload
      path: object
    - type: object
      key: UploadPart
      path: part-object
  - id: models
    title: Models
    description: 'List and describe the various models available in the API. You can
      refer to the [Models](/docs/models) documentation to understand what models
      are available and the differences between them.

      '
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: listModels
      path: list
    - type: endpoint
      key: retrieveModel
      path: retrieve
    - type: endpoint
      key: deleteModel
      path: delete
    - type: object
      key: Model
      path: object
  - id: moderations
    title: Moderations
    description: 'Given text and/or image inputs, classifies if those inputs are potentially
      harmful across several categories.

      Related guide: [Moderations](/docs/guides/moderation)

      '
    navigationGroup: endpoints
    sections:
    - type: endpoint
      key: createModeration
      path: create
    - type: object
      key: CreateModerationResponse
      path: object
  - id: vector-stores
    title: Vector stores
    description: 'Vector stores power semantic search for the Retrieval API and the
      `file_search` tool in the Responses and Assistants APIs.


      Related guide: [File Search](/docs/assistants/tools/file-search)

      '
    navigationGroup: vector_stores
    sections:
    - type: endpoint
      key: createVectorStore
      path: create
    - type: endpoint
      key: listVectorStores
      path: list
    - type: endpoint
      key: getVectorStore
      path: retrieve
    - type: endpoint
      key: modifyVectorStore
      path: modify
    - type: endpoint
      key: deleteVectorStore
      path: delete
    - type: endpoint
      key: searchVectorStore
      path: search
    - type: object
      key: VectorStoreObject
      path: object
  - id: vector-stores-files
    title: Vector store files
    description: 'Vector store files represent files inside a vector store.


      Related guide: [File Search](/docs/assistants/tools/file-search)

      '
    navigationGroup: vector_stores
    sections:
    - type: endpoint
      key: createVectorStoreFile
      path: createFile
    - type: endpoint
      key: listVectorStoreFiles
      path: listFiles
    - type: endpoint
      key: getVectorStoreFile
      path: getFile
    - type: endpoint
      key: retrieveVectorStoreFileContent
      path: getContent
    - type: endpoint
      key: updateVectorStoreFileAttributes
      path: updateAttributes
    - type: endpoint
      key: deleteVectorStoreFile
      path: deleteFile
    - type: object
      key: VectorStoreFileObject
      path: file-object
  - id: vector-stores-file-batches
    title: Vector store file batches
    description: 'Vector store file batches represent operations to add multiple files
      to a vector store.

      Related guide: [File Search](/docs/assistants/tools/file-search)

      '
    navigationGroup: vector_stores
    sections:
    - type: endpoint
      key: createVectorStoreFileBatch
      path: createBatch
    - type: endpoint
      key: getVectorStoreFileBatch
      path: getBatch
    - type: endpoint
      key: cancelVectorStoreFileBatch
      path: cancelBatch
    - type: endpoint
      key: listFilesInVectorStoreBatch
      path: listBatchFiles
    - type: object
      key: VectorStoreFileBatchObject
      path: batch-object
  - id: assistants
    title: Assistants
    beta: true
    description: 'Build assistants that can call models and use tools to perform tasks.


      [Get started with the Assistants API](/docs/assistants)

      '
    navigationGroup: assistants
    sections:
    - type: endpoint
      key: createAssistant
      path: createAssistant
    - type: endpoint
      key: listAssistants
      path: listAssistants
    - type: endpoint
      key: getAssistant
      path: getAssistant
    - type: endpoint
      key: modifyAssistant
      path: modifyAssistant
    - type: endpoint
      key: deleteAssistant
      path: deleteAssistant
    - type: object
      key: AssistantObject
      path: object
  - id: threads
    title: Threads
    beta: true
    description: 'Create threads that assistants can interact with.


      Related guide: [Assistants](/docs/assistants/overview)

      '
    navigationGroup: assistants
    sections:
    - type: endpoint
      key: createThread
      path: createThread
    - type: endpoint
      key: getThread
      path: getThread
    - type: endpoint
      key: modifyThread
      path: modifyThread
    - type: endpoint
      key: deleteThread
      path: deleteThread
    - type: object
      key: ThreadObject
      path: object
  - id: messages
    title: Messages
    beta: true
    description: 'Create messages within threads


      Related guide: [Assistants](/docs/assistants/overview)

      '
    navigationGroup: assistants
    sections:
    - type: endpoint
      key: createMessage
      path: createMessage
    - type: endpoint
      key: listMessages
      path: listMessages
    - type: endpoint
      key: getMessage
      path: getMessage
    - type: endpoint
      key: modifyMessage
      path: modifyMessage
    - type: endpoint
      key: deleteMessage
      path: deleteMessage
    - type: object
      key: MessageObject
      path: object
  - id: runs
    title: Runs
    beta: true
    description: 'Represents an execution run on a thread.


      Related guide: [Assistants](/docs/assistants/overview)

      '
    navigationGroup: assistants
    sections:
    - type: endpoint
      key: createRun
      path: createRun
    - type: endpoint
      key: createThreadAndRun
      path: createThreadAndRun
    - type: endpoint
      key: listRuns
      path: listRuns
    - type: endpoint
      key: getRun
      path: getRun
    - type: endpoint
      key: modifyRun
      path: modifyRun
    - type: endpoint
      key: submitToolOuputsToRun
      path: submitToolOutputs
    - type: endpoint
      key: cancelRun
      path: cancelRun
    - type: object
      key: RunObject
      path: object
  - id: run-steps
    title: Run steps
    beta: true
    description: 'Represents the steps (model and tool calls) taken during the run.


      Related guide: [Assistants](/docs/assistants/overview)

      '
    navigationGroup: assistants
    sections:
    - type: endpoint
      key: listRunSteps
      path: listRunSteps
    - type: endpoint
      key: getRunStep
      path: getRunStep
    - type: object
      key: RunStepObject
      path: step-object
  - id: assistants-streaming
    title: Streaming
    beta: true
    description: 'Stream the result of executing a Run or resuming a Run after submitting
      tool outputs.

      You can stream events from the [Create Thread and Run](/docs/api-reference/runs/createThreadAndRun),

      [Create Run](/docs/api-reference/runs/createRun), and [Submit Tool Outputs](/docs/api-reference/runs/submitToolOutputs)

      endpoints by passing `"stream": true`. The response will be a [Server-Sent events](https://html.spec.whatwg.org/multipage/server-sent-events.html#server-sent-events)
      stream.

      Our Node and Python SDKs provide helpful utilities to make streaming easy. Reference
      the

      [Assistants API quickstart](/docs/assistants/overview) to learn more.

      '
    navigationGroup: assistants
    sections:
    - type: object
      key: MessageDeltaObject
      path: message-delta-object
    - type: object
      key: RunStepDeltaObject
      path: run-step-delta-object
    - type: object
      key: AssistantStreamEvent
      path: events
  - id: administration
    title: Administration
    description: 'Programmatically manage your organization.

      The Audit Logs endpoint provides a log of all actions taken in the organization
      for security and monitoring purposes.

      To access these endpoints please generate an Admin API Key through the [API
      Platform Organization overview](/organization/admin-keys). Admin API keys cannot
      be used for non-administration endpoints.

      For best practices on setting up your organization, please refer to this [guide](/docs/guides/production-best-practices#setting-up-your-organization)

      '
    navigationGroup: administration
  - id: admin-api-keys
    title: Admin API Keys
    description: 'Admin API keys enable Organization Owners to programmatically manage
      various aspects of their organization, including users, projects, and API keys.
      These keys provide administrative capabilities, such as creating, updating,
      and deleting users; managing projects; and overseeing API key lifecycles.


      Key Features of Admin API Keys:


      - User Management: Invite new users, update roles, and remove users from the
      organization.


      - Project Management: Create, update, archive projects, and manage user assignments
      within projects.


      - API Key Oversight: List, retrieve, and delete API keys associated with projects.


      Only Organization Owners have the authority to create and utilize Admin API
      keys. To manage these keys, Organization Owners can navigate to the Admin Keys
      section of their API Platform dashboard.


      For direct access to the Admin Keys management page, Organization Owners can
      use the following link:


      [https://platform.openai.com/settings/organization/admin-keys](https://platform.openai.com/settings/organization/admin-keys)


      It''s crucial to handle Admin API keys with care due to their elevated permissions.
      Adhering to best practices, such as regular key rotation and assigning appropriate
      permissions, enhances security and ensures proper governance within the organization.

      '
    navigationGroup: administration
    sections:
    - type: endpoint
      key: admin-api-keys-list
      path: list
    - type: endpoint
      key: admin-api-keys-create
      path: create
    - type: endpoint
      key: admin-api-keys-get
      path: listget
    - type: endpoint
      key: admin-api-keys-delete
      path: delete
    - type: object
      key: AdminApiKey
      path: object
  - id: invite
    title: Invites
    description: Invite and manage invitations for an organization.
    navigationGroup: administration
    sections:
    - type: endpoint
      key: list-invites
      path: list
    - type: endpoint
      key: inviteUser
      path: create
    - type: endpoint
      key: retrieve-invite
      path: retrieve
    - type: endpoint
      key: delete-invite
      path: delete
    - type: object
      key: Invite
      path: object
  - id: users
    title: Users
    description: 'Manage users and their role in an organization.

      '
    navigationGroup: administration
    sections:
    - type: endpoint
      key: list-users
      path: list
    - type: endpoint
      key: modify-user
      path: modify
    - type: endpoint
      key: retrieve-user
      path: retrieve
    - type: endpoint
      key: delete-user
      path: delete
    - type: object
      key: User
      path: object
  - id: projects
    title: Projects
    description: 'Manage the projects within an orgnanization includes creation, updating,
      and archiving or projects.

      The Default project cannot be archived.

      '
    navigationGroup: administration
    sections:
    - type: endpoint
      key: list-projects
      path: list
    - type: endpoint
      key: create-project
      path: create
    - type: endpoint
      key: retrieve-project
      path: retrieve
    - type: endpoint
      key: modify-project
      path: modify
    - type: endpoint
      key: archive-project
      path: archive
    - type: object
      key: Project
      path: object
  - id: project-users
    title: Project users
    description: 'Manage users within a project, including adding, updating roles,
      and removing users.

      '
    navigationGroup: administration
    sections:
    - type: endpoint
      key: list-project-users
      path: list
    - type: endpoint
      key: create-project-user
      path: creeate
    - type: endpoint
      key: retrieve-project-user
      path: retrieve
    - type: endpoint
      key: modify-project-user
      path: modify
    - type: endpoint
      key: delete-project-user
      path: delete
    - type: object
      key: ProjectUser
      path: object
  - id: project-service-accounts
    title: Project service accounts
    description: 'Manage service accounts within a project. A service account is a
      bot user that is not associated with a user.

      If a user leaves an organization, their keys and membership in projects will
      no longer work. Service accounts

      do not have this limitation. However, service accounts can also be deleted from
      a project.

      '
    navigationGroup: administration
    sections:
    - type: endpoint
      key: list-project-service-accounts
      path: list
    - type: endpoint
      key: create-project-service-account
      path: create
    - type: endpoint
      key: retrieve-project-service-account
      path: retrieve
    - type: endpoint
      key: delete-project-service-account
      path: delete
    - type: object
      key: ProjectServiceAccount
      path: object
  - id: project-api-keys
    title: Project API keys
    description: 'Manage API keys for a given project. Supports listing and deleting
      keys for users.

      This API does not allow issuing keys for users, as users need to authorize themselves
      to generate keys.

      '
    navigationGroup: administration
    sections:
    - type: endpoint
      key: list-project-api-keys
      path: list
    - type: endpoint
      key: retrieve-project-api-key
      path: retrieve
    - type: endpoint
      key: delete-project-api-key
      path: delete
    - type: object
      key: ProjectApiKey
      path: object
  - id: project-rate-limits
    title: Project rate limits
    description: 'Manage rate limits per model for projects. Rate limits may be configured
      to be equal to or lower than the organization''s rate limits.

      '
    navigationGroup: administration
    sections:
    - type: endpoint
      key: list-project-rate-limits
      path: list
    - type: endpoint
      key: update-project-rate-limits
      path: update
    - type: object
      key: ProjectRateLimit
      path: object
  - id: audit-logs
    title: Audit logs
    description: 'Logs of user actions and configuration changes within this organization.

      To log events, you must activate logging in the [Organization Settings](/settings/organization/general).

      Once activated, for security reasons, logging cannot be deactivated.

      '
    navigationGroup: administration
    sections:
    - type: endpoint
      key: list-audit-logs
      path: list
    - type: object
      key: AuditLog
      path: object
  - id: usage
    title: Usage
    description: 'The **Usage API** provides detailed insights into your activity
      across the OpenAI API. It also includes a separate [Costs endpoint](/docs/api-reference/usage/costs),
      which offers visibility into your spend, breaking down consumption by invoice
      line items and project IDs.


      While the Usage API delivers granular usage data, it may not always reconcile
      perfectly with the Costs due to minor differences in how usage and spend are
      recorded. For financial purposes, we recommend using the [Costs endpoint](/docs/api-reference/usage/costs)
      or the [Costs tab](/settings/organization/usage) in the Usage Dashboard, which
      will reconcile back to your billing invoice.

      '
    navigationGroup: administration
    sections:
    - type: endpoint
      key: usage-completions
      path: completions
    - type: object
      key: UsageCompletionsResult
      path: completions_object
    - type: endpoint
      key: usage-embeddings
      path: embeddings
    - type: object
      key: UsageEmbeddingsResult
      path: embeddings_object
    - type: endpoint
      key: usage-moderations
      path: moderations
    - type: object
      key: UsageModerationsResult
      path: moderations_object
    - type: endpoint
      key: usage-images
      path: images
    - type: object
      key: UsageImagesResult
      path: images_object
    - type: endpoint
      key: usage-audio-speeches
      path: audio_speeches
    - type: object
      key: UsageAudioSpeechesResult
      path: audio_speeches_object
    - type: endpoint
      key: usage-audio-transcriptions
      path: audio_transcriptions
    - type: object
      key: UsageAudioTranscriptionsResult
      path: audio_transcriptions_object
    - type: endpoint
      key: usage-vector-stores
      path: vector_stores
    - type: object
      key: UsageVectorStoresResult
      path: vector_stores_object
    - type: endpoint
      key: usage-code-interpreter-sessions
      path: code_interpreter_sessions
    - type: object
      key: UsageCodeInterpreterSessionsResult
      path: code_interpreter_sessions_object
    - type: endpoint
      key: usage-costs
      path: costs
    - type: object
      key: CostsResult
      path: costs_object
  - id: certificates
    beta: true
    title: Certificates
    description: 'Manage Mutual TLS certificates across your organization and projects.


      [Learn more about Mutual TLS.](https://help.openai.com/en/articles/10876024-openai-mutual-tls-beta-program)

      '
    navigationGroup: administration
    sections:
    - type: endpoint
      key: uploadCertificate
      path: uploadCertificate
    - type: endpoint
      key: getCertificate
      path: getCertificate
    - type: endpoint
      key: modifyCertificate
      path: modifyCertificate
    - type: endpoint
      key: deleteCertificate
      path: deleteCertificate
    - type: endpoint
      key: listOrganizationCertificates
      path: listOrganizationCertificates
    - type: endpoint
      key: listProjectCertificates
      path: listProjectCertificates
    - type: endpoint
      key: activateOrganizationCertificates
      path: activateOrganizationCertificates
    - type: endpoint
      key: deactivateOrganizationCertificates
      path: deactivateOrganizationCertificates
    - type: endpoint
      key: activateProjectCertificates
      path: activateProjectCertificates
    - type: endpoint
      key: deactivateProjectCertificates
      path: deactivateProjectCertificates
    - type: object
      key: Certificate
      path: object
  - id: completions
    title: Completions
    legacy: true
    navigationGroup: legacy
    description: 'Given a prompt, the model will return one or more predicted completions
      along with the probabilities of alternative tokens at each position. Most developer
      should use our [Chat Completions API](/docs/guides/text-generation#text-generation-models)
      to leverage our best and newest models.

      '
    sections:
    - type: endpoint
      key: createCompletion
      path: create
    - type: object
      key: CreateCompletionResponse
      path: object
